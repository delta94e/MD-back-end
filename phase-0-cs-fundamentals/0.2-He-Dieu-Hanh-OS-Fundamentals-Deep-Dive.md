# Hệ Điều Hành (OS Fundamentals) — Deep Dive Từ Kernel Đến Goroutine

> **Tài liệu học dành cho:** Người mới bắt đầu, chuẩn bị trở thành Senior Backend Engineer
> **Chủ đề:** Phase 0.2 — Hệ điều hành (OS Fundamentals)
> **Phương pháp phân tích:** 6 patterns (5 Whys, First Principles, Trade-off Analysis, Mental Mapping, Reverse Engineering, Contextual History)
> **Ngôn ngữ:** Hoàn toàn bằng Tiếng Việt

---

## Mục lục

| #   | Chủ đề                                      | Mô tả                                                                                                                                                |
| --- | ------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
| §1  | Process & Thread — Đơn Vị Thực Thi (10 mục) | Process vs Thread, PCB, Context Switch, Fork/Exec, Thread Models, Goroutines vs Threads, GMP Scheduler, Process States, IPC, Zombie/Orphan Processes |
| §2  | CPU Scheduling — Ai Được Chạy Tiếp? (8 mục) | Scheduling Algorithms, Preemptive vs Cooperative, Priority Inversion, Real-time, CFS Linux, Go Scheduler, Starvation, Multi-queue                    |
| §3  | Memory Management — Bộ Nhớ Ảo (10 mục)      | Virtual Memory, Page Table, TLB, Page Fault, Demand Paging, Copy-on-Write, mmap, malloc/free, GC, Go Memory Allocator                                |
| §4  | File System — Mọi Thứ Là File (8 mục)       | Inode, File Descriptor, VFS, ext4/XFS, Journaling, Buffered I/O, Directory Structure, Links                                                          |
| §5  | I/O & System Calls — Cổng Giao Tiếp (8 mục) | Syscall Mechanism, Blocking/Non-blocking, select/poll/epoll, io_uring, Signals, Everything-is-a-File, /proc & /sys, strace                           |
| §6  | Synchronization — Đồng Bộ Hóa (8 mục)       | Race Condition, Mutex, Semaphore, Deadlock, RWLock, Atomic, sync.Map, Go Channel Patterns                                                            |
| §7  | Deep Analysis Patterns (6 mục)              | 5 Whys, First Principles, Trade-off Analysis, Mental Mapping, Reverse Engineering, Contextual History                                                |
| §8  | Tổng Kết & Câu Hỏi Phỏng Vấn Senior         | Ôn tập & thực hành                                                                                                                                   |

---

## §1. Process & Thread — Đơn Vị Thực Thi

### 1.1 Process là gì? — Chương Trình Đang Sống

#### Bắt đầu từ câu hỏi đơn giản nhất

Khi bạn viết code Go và chạy `go run main.go`, file `.go` trên disk chỉ là **text file** — nó chưa phải process. Process chỉ xuất hiện khi OS **nạp chương trình vào bộ nhớ** và bắt đầu thực thi.

Nhiều người mới học nhầm lẫn hai khái niệm **Program** (chương trình) và **Process** (tiến trình). Để hiểu sâu, hãy bắt đầu bằng một ví dụ đời thực:

> Hãy tưởng tượng bạn có **công thức nấu phở** (recipe) ghi trên giấy. Tờ giấy đó nằm yên trên bàn — nó không nấu phở, không tiêu tốn gas, không dùng nồi. Đó chính là **Program**: một tập hợp instructions được lưu trên disk, hoàn toàn **thụ động (passive)**, không tiêu tốn CPU hay RAM.
>
> Khi bạn **bắt đầu nấu** theo công thức — lấy nồi, đổ nước, bật gas, cho xương vào — lúc đó bạn đang tạo ra một **Process**: phiên bản "đang sống" của công thức, tiêu tốn tài nguyên thật (gas, nước, thời gian). Bạn thậm chí có thể nấu **2 nồi phở cùng lúc** từ cùng 1 công thức — 2 processes từ 1 program!

#### Từ source code đến process — chuyện gì xảy ra?

Khi bạn chạy `go run main.go`, hàng loạt bước diễn ra mà hầu hết developer không hề biết:

**Bước 1 — Compile**: Go compiler (`go tool compile`) biến source code `.go` thành **object file** chứa machine code (mã máy). Khác với Python hay JavaScript (interpreted), Go tạo ra **binary gốc** (native binary) — CPU đọc trực tiếp, không cần VM hay interpreter.

**Bước 2 — Link**: Go linker (`go tool link`) gom tất cả object files + Go runtime + standard library thành **một file binary duy nhất** (ELF format trên Linux, Mach-O trên macOS). File binary này nằm trên disk — vẫn chỉ là **program**, chưa phải process.

**Bước 3 — Exec**: Khi bạn chạy binary, OS kernel thực hiện **system call `execve()`**, bắt đầu chuỗi hành động:

```
╔═══════════════════════════════════════════════════════════════╗
║   TỪ BINARY (DISK) → PROCESS (RAM) — TỪNG BƯỚC            ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  $ ./myserver                                                 ║
║                                                               ║
║  ① Shell gọi fork() → tạo child process (bản sao shell)    ║
║  ② Child gọi execve("./myserver") → kernel tiếp nhận       ║
║  ③ Kernel đọc ELF header của binary:                        ║
║     → Xác định entry point (địa chỉ bắt đầu)             ║
║     → Xác định các segments (code, data, bss)               ║
║  ④ Kernel tạo address space MỚI cho process:               ║
║     → Allocate page table (bảng trang)                       ║
║     → Map code segment vào virtual memory (read-only)        ║
║     → Map data segment vào virtual memory (read-write)       ║
║     → Setup stack (8MB default trên Linux)                   ║
║     → Setup heap (bắt đầu rỗng)                            ║
║  ⑤ Kernel tạo PCB (Process Control Block):                  ║
║     → Gán PID (Process ID) duy nhất                        ║
║     → Set state = READY                                       ║
║     → Mở 3 file descriptors mặc định:                       ║
║       fd 0 = stdin, fd 1 = stdout, fd 2 = stderr            ║
║  ⑥ Kernel đặt Program Counter → entry point                ║
║  ⑦ Scheduler chọn process → CPU bắt đầu thực thi!        ║
║                                                               ║
║  → Từ lúc gõ Enter đến instruction đầu tiên: ~1-5ms       ║
║  → Phần lớn thời gian: đọc binary từ disk (I/O)           ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Lưu ý quan trọng: ở bước ④, kernel **không copy toàn bộ binary vào RAM ngay**! Kernel chỉ **map** (ánh xạ) các segments qua Virtual Memory. Khi CPU thực sự cần đọc một trang code lần đầu tiên, **page fault** xảy ra và kernel mới load trang đó từ disk. Đây gọi là **Demand Paging** — sẽ nói chi tiết ở §3. Kỹ thuật này giúp process khởi động nhanh vì không cần đợi load hết binary.

#### Program vs Process — bảng so sánh chi tiết

```
╔═══════════════════════════════════════════════════════════════╗
║   PROGRAM vs PROCESS                                         ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  PROGRAM (Passive — nằm trên disk):                          ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  /usr/bin/nginx                               │             ║
║  │  → Chỉ là file binary (ELF format)           │             ║
║  │  → Chứa: machine code + data + metadata      │             ║
║  │  → KHÔNG chiếm CPU, KHÔNG có state           │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  PROCESS (Active — đang chạy trong RAM):                     ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  nginx (PID 1234)           nginx (PID 1235) │             ║
║  │  ┌──────────────────┐      ┌────────────────┐│             ║
║  │  │ Code (text)      │      │ Code (shared!) ││             ║
║  │  │ Data (heap+stack)│      │ Data (riêng!)  ││             ║
║  │  │ Open files       │      │ Open files     ││             ║
║  │  │ CPU registers    │      │ CPU registers  ││             ║
║  │  │ PID, PPID        │      │ PID, PPID      ││             ║
║  │  │ Memory map       │      │ Memory map     ││             ║
║  │  └──────────────────┘      └────────────────┘│             ║
║  │                                                │             ║
║  │  → 1 program → nhiều processes!              │             ║
║  │  → Mỗi process: address space RIÊNG          │             ║
║  │  → Process này KHÔNG thể đọc RAM process kia │             ║
║  │    (memory isolation = bảo mật!)              │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Nhìn vào diagram trên, có 2 điểm quan trọng:

**Thứ nhất — Code segment được CHIA SẺ (shared)**: Khi bạn chạy 2 instances nginx, kernel đủ thông minh để chỉ load code (machine instructions) **một lần** vào RAM, rồi cả 2 processes cùng trỏ đến vùng code đó. Vì code là read-only (chỉ đọc), việc share hoàn toàn an toàn. Đây là tối ưu memory rất quan trọng — tưởng tượng 100 nginx workers share 10MB code, tiết kiệm 990MB!

**Thứ hai — Data segment là RIÊNG BIỆT**: Mỗi process có heap, stack, biến toàn cục riêng. Nếu nginx process A đặt `max_connections = 1000`, process B không hề biết — chúng sống trong "thế giới riêng". Đây là **memory isolation** (cách ly bộ nhớ) — nền tảng bảo mật của OS. Nếu không có isolation, 1 process bug có thể ghi đè lên memory của process khác, gây crash toàn hệ thống.

#### Ví dụ thực tế với Go

```go
// main.go — Khi chạy, đây là PROGRAM (file trên disk)
package main

import (
    "fmt"
    "os"
    "time"
)

func main() {
    // Từ đây trở đi, code chạy TRONG 1 PROCESS
    // OS đã cấp PID, memory, file descriptors...

    fmt.Printf("PID của tôi: %d\n", os.Getpid())
    fmt.Printf("PID của parent: %d\n", os.Getppid())

    // Process này đang SỐNG — chiếm RAM, có thể dùng CPU
    // Nó sẽ sống cho đến khi main() return hoặc os.Exit()
    time.Sleep(5 * time.Second)

    // Khi main() return:
    // → Go runtime cleanup (GC, close files)
    // → syscall exit_group(0)
    // → Kernel: giải phóng memory, close file descriptors
    // → Process trở thành ZOMBIE cho đến parent gọi wait()
    // → Cuối cùng: PID được thu hồi
}
```

Chạy 2 lần cùng lúc:

```bash
$ ./myserver &    # Process 1, PID 12345
$ ./myserver &    # Process 2, PID 12346
# CÙNG 1 binary → 2 processes KHÁC NHAU
# PID khác, memory khác, nhưng share code segment!
```

#### PCB — "Hộ chiếu" của process

Mỗi process được OS quản lý qua một cấu trúc gọi là **PCB (Process Control Block)** — đây là "hộ chiếu" của process, chứa mọi thông tin OS cần để quản lý nó.

Hãy hình dung PCB như **hồ sơ nhân viên** trong công ty: nó chứa tên (PID), vị trí hiện tại (state), mức lương (priority), ai là sếp (parent PID), bàn làm việc đang mở những gì (open files), và nếu nhân viên tạm nghỉ thì ghi lại "đang làm đến đâu" (saved registers) để khi quay lại tiếp tục ngay.

```
╔═══════════════════════════════════════════════════════════════╗
║   PCB — PROCESS CONTROL BLOCK                               ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  struct task_struct {  // Linux kernel                        ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  PID:          1234                           │             ║
║  │  State:        RUNNING / SLEEPING / STOPPED   │             ║
║  │  Priority:     nice value (-20 đến +19)       │             ║
║  │  Parent PID:   1200                           │             ║
║  │                                                │             ║
║  │  CPU Context (saved khi bị tạm dừng):        │             ║
║  │  ├── Program Counter (RIP): 0x401020          │             ║
║  │  ├── Stack Pointer (RSP):   0x7fff8000        │             ║
║  │  ├── General Registers:     RAX, RBX, RCX...  │             ║
║  │  └── Flags Register:        EFLAGS            │             ║
║  │                                                │             ║
║  │  Memory Info:                                  │             ║
║  │  ├── Page Table Base:       CR3 register       │             ║
║  │  ├── Code segment:          0x400000-0x401000  │             ║
║  │  ├── Heap:                  0x600000-0x800000  │             ║
║  │  └── Stack:                 0x7fff0000-0x8000  │             ║
║  │                                                │             ║
║  │  I/O Info:                                     │             ║
║  │  ├── File Descriptor Table: [0,1,2,3,4...]    │             ║
║  │  ├── stdin(0), stdout(1), stderr(2)            │             ║
║  │  └── Open sockets, pipes...                    │             ║
║  │                                                │             ║
║  │  Scheduling Info:                              │             ║
║  │  ├── Time slice remaining: 4ms                 │             ║
║  │  ├── Total CPU time used:  150ms               │             ║
║  │  └── Wait channel:         (nếu đang sleep)   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  → Linux kernel: mỗi process = 1 struct task_struct          ║
║  → Size: ~6KB trên Linux x86_64                              ║
║  → Kernel quản lý hàng ngàn task_struct đồng thời           ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Giải thích từng phần trong PCB:

**PID (Process ID)**: Số nguyên duy nhất do kernel gán. Trên Linux, PID 1 luôn là `init`/`systemd` — process đầu tiên sau khi kernel boot. Mọi process khác đều là "con cháu" của PID 1. Trong Docker container, app của bạn thường là PID 1, và điều này có hệ quả quan trọng (phải handle signals đúng cách).

**State**: Process không phải lúc nào cũng chạy! Nó có thể đang `RUNNING` (đang dùng CPU), `SLEEPING` (chờ I/O, ví dụ chờ database trả kết quả), `STOPPED` (bị pause, ví dụ khi debug). Hiểu states giúp debug tại sao server "chậm" dù CPU usage thấp — có thể hàng ngàn processes đang `SLEEPING` chờ disk I/O!

**CPU Context (Registers)**: Đây là phần **quan trọng nhất để hiểu context switch** (xem 1.4). Khi OS cần tạm dừng process A để chạy process B, nó PHẢI lưu toàn bộ registers của A (đang tính toán gì, con trỏ stack ở đâu, kết quả trung gian trong RAX là gì...) vào PCB. Khi A được chạy lại, kernel khôi phục registers — A tiếp tục như thể chưa hề bị dừng!

**Page Table Base (CR3)**: Mỗi process có bảng trang riêng, ánh xạ virtual address → physical address. Khi switch process, CPU phải load CR3 register mới. Đây là nguyên nhân khiến process switch đắt đỏ hơn thread switch (sẽ giải thích ở 1.4).

**File Descriptor Table**: Mỗi process có bảng riêng. Mặc định: `fd 0 = stdin`, `fd 1 = stdout`, `fd 2 = stderr`. Khi bạn mở file hay socket, OS gán `fd 3, 4, 5...`. Go server mở 10K connections = 10K file descriptors! Nếu vượt giới hạn (`ulimit -n`), bạn gặp lỗi kinh điển: _"too many open files"_. Xem §4 để hiểu chi tiết.

#### 5 Whys — Đào sâu: Tại sao cần khái niệm Process?

```
╔═══════════════════════════════════════════════════════════════╗
║   5 WHYS: TẠI SAO CẦN PROCESS?                              ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  WHY 1: Tại sao OS cần khái niệm "process"?                ║
║  → Vì cần chạy NHIỀU chương trình "cùng lúc" trên 1 CPU.  ║
║                                                               ║
║  WHY 2: Tại sao không cho chạy trực tiếp trên CPU?         ║
║  → Vì programs sẽ ghi đè bộ nhớ lẫn nhau, crash cả máy! ║
║  → Cần ISOLATION: mỗi program nghĩ mình có cả máy tính.   ║
║                                                               ║
║  WHY 3: Tại sao cần isolation?                               ║
║  → Bảo mật: web server không được đọc password file        ║
║    của database server.                                       ║
║  → Ổn định: 1 process crash không kéo theo cả hệ thống.   ║
║  → Quản lý: OS biết cái nào chiếm bao nhiêu RAM/CPU.      ║
║                                                               ║
║  WHY 4: Isolation được thực hiện bằng cơ chế gì?            ║
║  → Virtual Memory: mỗi process có address space RIÊNG.      ║
║  → Hardware support: CPU có protection rings (Ring 0/3).     ║
║  → MMU (Memory Management Unit) chặn truy cập trái phép.   ║
║                                                               ║
║  WHY 5: Giới hạn vật lý cuối cùng là gì?                   ║
║  → CPU chỉ có 1 bộ registers → phải save/restore khi      ║
║    switch giữa các processes → CHI PHÍ = context switch!    ║
║  → RAM có giới hạn → không thể chạy vô hạn processes!    ║
║  → Đây là lý do Go tạo ra goroutine: giảm overhead!       ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Thực hành: xem process trên Linux

```bash
# Xem process Go server đang chạy
$ ps aux | grep myserver
user  12345  0.5  1.2  524288  49152  ?  Sl  10:30  0:03  ./myserver

# Giải mã output:
# PID=12345, CPU=0.5%, RAM=1.2% (49MB), State=Sl (Sleeping+threads)

# Xem CHI TIẾT process qua /proc
$ cat /proc/12345/status
Name:   myserver
State:  S (sleeping)          ← Đang chờ I/O (bình thường cho server)
Pid:    12345
PPid:   1                     ← Parent là init/systemd
Threads: 8                    ← 8 OS threads (Go M threads)
VmRSS:  49152 kB              ← RAM thực sự đang dùng
VmSize: 524288 kB              ← Virtual memory (lớn hơn nhiều!)
FDSize: 256                    ← Số file descriptors allocated

# Xem memory map (mỗi vùng nhớ đang dùng làm gì)
$ cat /proc/12345/maps | head -5
00400000-00600000 r-xp  ./myserver        ← Code (read+execute)
00600000-00800000 rw-p  ./myserver        ← Data (read+write)
c000000000-c000400000 rw-p  [heap]        ← Go heap
7fff80000000-7fff80800000 rw-p [stack]    ← Stack
```

Khi bạn nhìn output `ps aux`, bạn thấy mỗi dòng là **1 PCB** mà kernel đang quản lý. Column `State` cho biết process đang ở trạng thái nào — `R` (Running), `S` (Sleeping), `D` (Disk sleep, KHÔNG thể kill!), `Z` (Zombie). Hiểu các states này là kỹ năng debug production quan trọng bậc nhất.

#### Vòng đời 1 Process — Từ sinh ra đến chết đi

Process không tồn tại mãi mãi. Nó có vòng đời rõ ràng, giống như con người: sinh ra, sống, và chết đi.

```
╔═══════════════════════════════════════════════════════════════╗
║   VÒNG ĐỜI CỦA PROCESS                                     ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ① SINH RA (Create):                                         ║
║     $ ./myserver                                              ║
║     → Parent (bash) gọi fork() + exec()                     ║
║     → Kernel tạo PCB, gán PID, allocate address space       ║
║     → Process ở trạng thái READY (chờ CPU)                  ║
║                                                               ║
║  ② SỐNG & LÀM VIỆC (Lifetime):                              ║
║     Process luân chuyển giữa 3 trạng thái:                  ║
║                                                               ║
║     ┌──────────────────────────────────────────┐             ║
║     │                                            │             ║
║     │  READY ──(scheduler pick)──► RUNNING       │             ║
║     │    ▲                           │      │     │             ║
║     │    │                           │      │     │             ║
║     │    │    ┌───(timer)────────────┘      │     │             ║
║     │    │    │                              │     │             ║
║     │    │    │  ┌──(I/O done)───┐          │     │             ║
║     │    │    │  │               │          │     │             ║
║     │    │    └──┤  WAITING  ◄───┘          │     │             ║
║     │    │       │  (sleeping) ◄─(I/O req)──┘     │             ║
║     │    │       │               │                 │             ║
║     │    └───────┘               │                 │             ║
║     │                                              │             ║
║     └──────────────────────────────────────────┘             ║
║                                                               ║
║     RUNNING: CPU đang thực thi instructions của process     ║
║     READY:   Sẵn sàng chạy, đang chờ scheduler chọn        ║
║     WAITING: Chờ event bên ngoài (I/O, timer, signal)       ║
║                                                               ║
║  ③ CHẾT ĐI (Terminate):                                     ║
║     → main() return hoặc os.Exit()                          ║
║     → Kernel giải phóng: memory, file descriptors, locks    ║
║     → Process → ZOMBIE (chờ parent wait())                  ║
║     → Parent gọi wait() → kernel xóa PCB → DONE!          ║
║                                                               ║
║  ④ TIMELINE THỰC TẾ CỦA GO SERVER:                          ║
║                                                               ║
║  t=0ms     fork()+exec()     → READY                        ║
║  t=1ms     scheduler pick    → RUNNING                       ║
║  t=1-5ms   Go runtime init   → RUNNING (setup GC, M, P)    ║
║  t=5ms     main() bắt đầu  → RUNNING                       ║
║  t=6ms     http.ListenAndServe() → bind port                ║
║  t=7ms     accept() chờ conn → WAITING (epoll_wait)         ║
║  ...                                                          ║
║  t=100ms   request đến!     → READY → RUNNING              ║
║  t=101ms   query DB          → WAITING (I/O wait)           ║
║  t=110ms   DB trả kết quả  → READY → RUNNING              ║
║  t=111ms   send response     → RUNNING                       ║
║  t=112ms   accept() tiếp    → WAITING (chờ request mới)    ║
║  ...       (lặp lại mãi mãi cho đến khi Ctrl+C)           ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Nhìn vào timeline, bạn hiểu tại sao server **phần lớn thời gian ở trạng thái WAITING**: nó chờ connections (accept), chờ database (query), chờ client gửi data (read). CPU chỉ thực sự "làm việc" vài microsecond cho mỗi request. Đây là lý do 1 Go server đơn luồng vẫn handle được hàng nghìn concurrent connections — phần lớn goroutines đang WAITING, không chiếm CPU!

#### Go Runtime khởi tạo process — chuyện gì xảy ra trước main()?

Khi binary Go bắt đầu chạy, **main() KHÔNG phải function đầu tiên**! Go runtime phải setup rất nhiều thứ trước:

```
╔═══════════════════════════════════════════════════════════════╗
║   GO PROCESS STARTUP — TRƯỚC KHI main() ĐƯỢC GỌI           ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ELF entry point: _rt0_amd64_linux (assembly)               ║
║  │                                                            ║
║  ├── ① runtime.check()                                       ║
║  │   → Verify CPU features (SSE2, etc.)                      ║
║  │   → Verify go runtime data structures                     ║
║  │                                                            ║
║  ├── ② runtime.args(argc, argv)                              ║
║  │   → Parse command line arguments                          ║
║  │   → Setup os.Args                                         ║
║  │                                                            ║
║  ├── ③ runtime.osinit()                                      ║
║  │   → Detect số CPU cores → set GOMAXPROCS mặc định       ║
║  │   → Setup signal handlers                                 ║
║  │   → Get page size từ kernel                               ║
║  │                                                            ║
║  ├── ④ runtime.schedinit()                    ← QUAN TRỌNG! ║
║  │   → Khởi tạo Go scheduler (GMP model)                   ║
║  │   → Tạo P (processor) theo GOMAXPROCS                    ║
║  │   → Setup memory allocator                                ║
║  │   → Khởi tạo GC (Garbage Collector)                      ║
║  │   → Tạo goroutine 0 (g0) cho scheduling                 ║
║  │                                                            ║
║  ├── ⑤ runtime.newproc(main.main)                            ║
║  │   → Tạo goroutine MỚI để chạy main.main()              ║
║  │   → Goroutine này = goroutine 1 (g1)                     ║
║  │   → Push vào run queue của P0                             ║
║  │                                                            ║
║  ├── ⑥ runtime.mstart()                                     ║
║  │   → Khởi động M0 (OS thread đầu tiên)                   ║
║  │   → M0 bind với P0                                        ║
║  │   → Bắt đầu schedule loop                                ║
║  │                                                            ║
║  ├── ⑦ schedule() → pick g1 → execute()                     ║
║  │   → Scheduler chọn goroutine 1 (main.main)              ║
║  │   → Context switch đến g1                                ║
║  │                                                            ║
║  └── ⑧ main.main() ← BÂY GIỜ mới gọi code của bạn!      ║
║      → package init() functions chạy trước                  ║
║      → Toàn bộ import chain init() chạy trước              ║
║      → Cuối cùng: main.main()                               ║
║                                                               ║
║  Tổng thời gian startup: ~1-5ms (rất nhanh so với JVM 1-5s!)║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// Kiểm tra Go runtime đã setup gì
package main

import (
    "fmt"
    "runtime"
)

func init() {
    // init() chạy TRƯỚC main()!
    fmt.Println("init() chạy đầu tiên")
}

func main() {
    // Lúc này Go runtime đã sẵn sàng hoàn toàn
    fmt.Printf("GOMAXPROCS = %d\n", runtime.GOMAXPROCS(0))  // = số CPU cores
    fmt.Printf("Num Goroutine = %d\n", runtime.NumGoroutine()) // Ít nhất 1
    fmt.Printf("Num CPU = %d\n", runtime.NumCPU())
    fmt.Printf("GOARCH = %s\n", runtime.GOARCH)  // amd64, arm64
    fmt.Printf("GOOS = %s\n", runtime.GOOS)      // linux, darwin
}
```

#### Processes giao tiếp với nhau — IPC Preview

Process sống trong "phòng cách ly" (isolated address space). Vậy làm sao 2 processes giao tiếp? Đây là bài toán **IPC (Inter-Process Communication)** — rất quan trọng cho backend architecture:

```
╔═══════════════════════════════════════════════════════════════╗
║   IPC — CÁCH PROCESSES GIAO TIẾP                            ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Process A                    Process B                       ║
║  ┌──────────┐                ┌──────────┐                    ║
║  │ RAM riêng │                │ RAM riêng │                    ║
║  │ ████████ │   ← KHÔNG      │ ████████ │                    ║
║  │          │   thể đọc      │          │                    ║
║  │          │   trực tiếp! → │          │                    ║
║  └────┬─────┘                └─────┬────┘                    ║
║       │                            │                          ║
║       │     CẦN "CẦU NỐI":        │                          ║
║       │                            │                          ║
║  ① PIPE: A | B                     │                          ║
║     A ──stdout──► pipe ──stdin──► B                          ║
║     → Đơn giản, 1 chiều                                     ║
║     → VD: `cat log | grep error`                             ║
║                                                               ║
║  ② SOCKET: TCP/Unix Domain Socket                            ║
║     A ←──socket──► B                                          ║
║     → 2 chiều, có thể cross-machine                         ║
║     → VD: Go server ←→ PostgreSQL                           ║
║     → VD: Microservices gRPC calls                          ║
║                                                               ║
║  ③ SHARED MEMORY: shmget()/mmap()                            ║
║     A ──► [shared region] ◄── B                              ║
║     → NHANH NHẤT! Không copy data                           ║
║     → Nhưng cần synchronization (semaphore)                 ║
║     → VD: Nginx ←→ Workers, PostgreSQL ←→ backends         ║
║                                                               ║
║  ④ SIGNAL: kill(pid, SIGTERM)                                ║
║     A ──signal──► B                                           ║
║     → Notification đơn giản (không có data)                 ║
║     → VD: Ctrl+C = SIGINT, docker stop = SIGTERM            ║
║     → VD: Go: signal.Notify(ch, syscall.SIGTERM)            ║
║                                                               ║
║  ⑤ MESSAGE QUEUE / FILE:                                     ║
║     A ──write──► [queue/file] ──read──► B                    ║
║     → Asynchronous, có persistence                          ║
║     → VD: Redis Pub/Sub, Kafka, RabbitMQ                    ║
║                                                               ║
║  Backend relevance:                                           ║
║  → Go server ←→ DB: Socket (TCP)                            ║
║  → Go server ←→ Redis: Socket (TCP hoặc Unix)              ║
║  → Go server ←→ Nginx: Socket (Unix domain)                ║
║  → Docker: container A ←→ container B: Network (veth pair) ║
║  → Kubernetes: Pod A ←→ Pod B: Service (virtual IP)        ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Debug Process trong Production — 4 Scenario thường gặp

```
╔═══════════════════════════════════════════════════════════════╗
║   PRODUCTION DEBUGGING — PROCESS ISSUES                      ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  SCENARIO 1: "Server CPU 100%!"                              ║
║  ┌────────────────────────────────────────────┐              ║
║  │ $ top -p $(pidof myserver)                  │              ║
║  │ PID   %CPU  %MEM  STAT  COMMAND             │              ║
║  │ 1234  99.8  5.0   R     myserver            │              ║
║  │                    ↑                         │              ║
║  │              R = RUNNING (không sleep!)      │              ║
║  │                                              │              ║
║  │ Debug:                                       │              ║
║  │ $ curl localhost:6060/debug/pprof/profile    │              ║
║  │ → pprof CPU profile → tìm hot function     │              ║
║  │                                              │              ║
║  │ Nguyên nhân thường gặp:                     │              ║
║  │ → Infinite loop, busy wait, GC liên tục    │              ║
║  └────────────────────────────────────────────┘              ║
║                                                               ║
║  SCENARIO 2: "Server RAM tang liên tục!"                     ║
║  ┌────────────────────────────────────────────┐              ║
║  │ $ watch -n1 'cat /proc/$(pidof myserver)/status \        │
║  │   | grep VmRSS'                              │              ║
║  │ VmRSS: 100000 kB → 200000 kB → 500000 kB  │              ║
║  │                                              │              ║
║  │ Debug:                                       │              ║
║  │ $ curl localhost:6060/debug/pprof/heap       │              ║
║  │ → pprof heap profile → tìm allocation leak │              ║
║  │                                              │              ║
║  │ Nguyên nhân: goroutine leak, slice append   │              ║
║  │ không release, global cache không có TTL    │              ║
║  └────────────────────────────────────────────┘              ║
║                                                               ║
║  SCENARIO 3: "Too many open files!"                          ║
║  ┌────────────────────────────────────────────┐              ║
║  │ $ ls /proc/$(pidof myserver)/fd | wc -l     │              ║
║  │ 1021    ← Gần limit!                       │              ║
║  │                                              │              ║
║  │ $ ulimit -n                                  │              ║
║  │ 1024    ← Limit hiện tại!                  │              ║
║  │                                              │              ║
║  │ Fix: ulimit -n 65535                         │              ║
║  │ Fix: Đóng connections sau khi dùng!         │              ║
║  │ Go: defer resp.Body.Close()                 │              ║
║  │ Go: defer db.Close()                         │              ║
║  └────────────────────────────────────────────┘              ║
║                                                               ║
║  SCENARIO 4: "Server không respond nhưng CPU=0%"            ║
║  ┌────────────────────────────────────────────┐              ║
║  │ $ cat /proc/$(pidof myserver)/status        │              ║
║  │ State: D (disk sleep)                        │              ║
║  │        ↑                                     │              ║
║  │  D = Uninterruptible sleep (KHÔNG kill được!)│              ║
║  │  → Chờ I/O response từ hardware            │              ║
║  │  → NFS mount treo, disk hỏng               │              ║
║  │  → DMESG: xem kernel log                    │              ║
║  │                                              │              ║
║  │ Fix: Kiểm tra disk, NFS, hoặc reboot!      │              ║
║  └────────────────────────────────────────────┘              ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### ELF Binary Format — File binary chứa gì bên trong?

Khi Go compiler tạo binary, output là file **ELF (Executable and Linkable Format)** trên Linux (hoặc Mach-O trên macOS). Hiểu ELF giúp bạn debug crashes, đọc core dumps, và hiểu tại sao binary Go lớn (~10MB cho "Hello World").

```
╔═══════════════════════════════════════════════════════════════╗
║   ELF FILE — CẤU TRÚC BÊN TRONG BINARY                     ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  $ file ./myserver                                            ║
║  ./myserver: ELF 64-bit LSB executable, x86-64, version 1    ║
║              (SYSV), statically linked, Go, not stripped      ║
║                                                               ║
║  ┌────────────────────────────────────────────┐              ║
║  │           ELF HEADER (64 bytes)             │              ║
║  │  Magic: 7f 45 4c 46 (0x7f "ELF")           │              ║
║  │  Class: 64-bit                               │              ║
║  │  Type:  EXEC (executable)                    │              ║
║  │  Machine: x86-64                              │              ║
║  │  Entry Point: 0x465e00 ← _rt0_amd64_linux  │              ║
║  │  → Đây là instruction ĐẦU TIÊN CPU chạy!  │              ║
║  ├────────────────────────────────────────────┤              ║
║  │        PROGRAM HEADERS (cho loader)         │              ║
║  │  Segment 1: LOAD → map vào 0x400000 (R-X)  │              ║
║  │  Segment 2: LOAD → map vào 0x600000 (RW-)  │              ║
║  │  → Kernel dùng headers này để mmap() binary│              ║
║  ├────────────────────────────────────────────┤              ║
║  │        SECTIONS (cho developer/debugger)     │              ║
║  │                                              │              ║
║  │  .text     Machine code (instructions)      │              ║
║  │            → Hàm main(), handler(), etc.    │              ║
║  │            → READ + EXECUTE, không WRITE    │              ║
║  │                                              │              ║
║  │  .rodata   Read-only data                    │              ║
║  │            → String literals: "Hello"       │              ║
║  │            → const values                    │              ║
║  │                                              │              ║
║  │  .data     Initialized global variables     │              ║
║  │            → var maxConn = 100              │              ║
║  │            → READ + WRITE                    │              ║
║  │                                              │              ║
║  │  .bss      Uninitialized globals (zero!)    │              ║
║  │            → var counter int (= 0)          │              ║
║  │            → KHÔNG chiếm space trong file!  │              ║
║  │            → Kernel fill zeros khi load     │              ║
║  │                                              │              ║
║  │  .symtab   Symbol table (tên functions)     │              ║
║  │            → Debug info: main.main ở đâu   │              ║
║  │            → go tool nm ./myserver          │              ║
║  │                                              │              ║
║  │  .gopclntab  Go-specific: line number table │              ║
║  │            → Map instruction → source line  │              ║
║  │            → Stack trace in panic cần cái này│              ║
║  │            → Chiếm ~30% binary size!        │              ║
║  └────────────────────────────────────────────┘              ║
║                                                               ║
║  Tại sao Go binary lớn (~10MB cho Hello World)?             ║
║  → Go link tĩnh (static link): toàn bộ runtime + stdlib    ║
║    đều nằm TRONG binary (không cần .so/.dll bên ngoài)      ║
║  → .gopclntab (line tables): ~30% size                       ║
║  → .symtab (symbol table): ~15% size                         ║
║  → Ưu điểm: COPY 1 FILE = deploy xong! (Docker scratch)    ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```bash
# Đọc ELF header
$ readelf -h ./myserver
  Entry point address: 0x465e00   ← _rt0_amd64_linux

# Xem sections
$ readelf -S ./myserver | head -20
  [Nr] Name          Type      Address          Size
  [ 1] .text         PROGBITS  0000000000401000 001a3000  ← 1.7MB code!
  [ 2] .rodata       PROGBITS  00000000005a4000 00082000  ← 520KB strings
  [ 7] .gopclntab    PROGBITS  0000000000680000 00120000  ← 1.2MB line tables!

# Xem symbols (tên functions)
$ go tool nm ./myserver | grep main
  465e00 T main.main         ← T = Text (code) section
  466000 T main.handler
  466200 T main.init

# Giảm binary size (production)
$ go build -ldflags="-s -w" -o myserver .
# -s: bỏ symbol table (~15% nhỏ hơn)
# -w: bỏ DWARF debug info (~10% nhỏ hơn)
# Kết quả: ~6MB thay vì ~10MB
```

#### Virtual Memory — Mỗi process "nghĩ" mình sở hữu toàn bộ RAM

Đây là 1 trong những abstraction quan trọng nhất của OS. Mỗi process **tin rằng** mình có toàn bộ address space (trên 64-bit: 256TB!), nhưng thực tế RAM chỉ có 16GB. Làm sao?

```
╔═══════════════════════════════════════════════════════════════╗
║   VIRTUAL MEMORY — ẢO GIÁC BỘ NHỚ                          ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Vấn đề: Máy có 16GB RAM, nhưng chạy 100 processes,        ║
║  mỗi cái "nghĩ" mình có 256TB. Làm sao?                    ║
║                                                               ║
║  Analogy: Nhà hàng có 20 bàn, nhưng nhận 100 booking.       ║
║  → Không phải 100 khách đến CÙNG LÚC!                      ║
║  → Quản lý xếp luân phiên: ai đến thì có bàn              ║
║  → Nếu đầy: "xin chờ ngoài" (swap to disk)                ║
║                                                               ║
║  CÁCH HOẠT ĐỘNG:                                             ║
║                                                               ║
║  Process A             MMU (Hardware)        Physical RAM     ║
║  ┌──────────┐         ┌─────────────┐       ┌──────────┐    ║
║  │VA 0x1000 │──────►  │ Page Table   │──────►│PA 0x5000 │    ║
║  │VA 0x2000 │──────►  │ VA→PA lookup │──────►│PA 0x8000 │    ║
║  │VA 0x3000 │──────►  │             │──────►│PA 0x2000 │    ║
║  └──────────┘         └─────────────┘       └──────────┘    ║
║                                                               ║
║  Process B             MMU                   Physical RAM     ║
║  ┌──────────┐         ┌─────────────┐       ┌──────────┐    ║
║  │VA 0x1000 │──────►  │ Page Table   │──────►│PA 0xA000 │    ║
║  │VA 0x2000 │──────►  │ (KHÁC!)      │──────►│PA 0xC000 │    ║
║  │VA 0x3000 │──────►  │             │──────►│PA 0xF000 │    ║
║  └──────────┘         └─────────────┘       └──────────┘    ║
║                                                               ║
║  ĐIỂM THEN CHỐT:                                             ║
║  → A và B đều dùng VA 0x1000, nhưng map đến PA KHÁC NHAU! ║
║  → A KHÔNG THỂ truy cập PA 0xA000 (của B)                  ║
║  → MMU (chip hardware trên CPU) kiểm tra MỌI lần truy cập ║
║  → Truy cập sai → PAGE FAULT → kernel kill process!       ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  4-LEVEL PAGE TABLE (x86-64):                                ║
║                                                               ║
║  Virtual Address (48 bits used of 64):                        ║
║  ┌───────┬───────┬───────┬───────┬────────────┐             ║
║  │ PML4  │ PDPT  │  PD   │  PT   │   Offset   │             ║
║  │ 9 bits│ 9 bits│ 9 bits│ 9 bits│  12 bits   │             ║
║  └───┬───┴───┬───┴───┬───┴───┬───┴──────┬─────┘             ║
║      │       │       │       │          │                     ║
║      ▼       ▼       ▼       ▼          │                     ║
║  ┌──────┐┌──────┐┌──────┐┌──────┐      │                     ║
║  │512   ││512   ││512   ││512   │      │                     ║
║  │entries││entries││entries││entries│      │                     ║
║  └──┬───┘└──┬───┘└──┬───┘└──┬───┘      │                     ║
║     └───────┴───────┴───────┘           │                     ║
║            = Physical Page Address + Offset = PA             ║
║                                                               ║
║  → 4 lần dereference memory = CHẬM!                        ║
║  → TLB (Translation Lookaside Buffer) cache kết quả        ║
║  → TLB hit: ~1 cycle, TLB miss: ~100 cycles!               ║
║  → Context switch flush TLB → đó là lý do switch đắt!     ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  DEMAND PAGING — Load "lười":                                ║
║  ┌────────────────────────────────────────────┐              ║
║  │ Binary 50MB trên disk                       │              ║
║  │ → Kernel KHÔNG load hết 50MB vào RAM!      │              ║
║  │ → Chỉ MAP: "page 0x401 = offset 0x1000"   │              ║
║  │ → Khi CPU đọc page 0x401 lần đầu:        │              ║
║  │   1. PAGE FAULT! (page chưa có trong RAM)  │              ║
║  │   2. Kernel đọc 4KB từ disk → RAM         │              ║
║  │   3. Update page table: VA→PA              │              ║
║  │   4. CPU retry instruction → thành công!   │              ║
║  │ → Lần sau đọc page 0x401: TLB hit, 1 cycle│              ║
║  │                                              │              ║
║  │ Ưu điểm:                                    │              ║
║  │ → Process start NHANH! (không load hết)    │              ║
║  │ → RAM chỉ chứa pages THỰC SỰ DÙNG        │              ║
║  │ → Go binary 50MB nhưng chỉ dùng 5MB RAM   │              ║
║  └────────────────────────────────────────────┘              ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// Xem virtual memory stats trong Go
package main

import (
    "fmt"
    "runtime"
    "unsafe"
)

func main() {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)

    fmt.Printf("Heap Alloc:    %d MB\n", m.HeapAlloc/1024/1024)
    fmt.Printf("Heap Sys:      %d MB\n", m.HeapSys/1024/1024)   // Virtual mem reserved
    fmt.Printf("Stack Sys:     %d MB\n", m.StackSys/1024/1024)
    fmt.Printf("Total Sys:     %d MB\n", m.Sys/1024/1024)       // Tổng virtual memory

    // In địa chỉ virtual address
    x := 42
    fmt.Printf("Stack var addr: %p\n", &x)       // 0xc0000b4008 (stack)
    y := new(int)
    fmt.Printf("Heap var addr:  %p\n", y)         // 0xc0000b4010 (heap)
    fmt.Printf("Func addr:      %p\n", main)      // 0x465e00 (text segment)
    fmt.Printf("Pointer size:   %d bytes\n", unsafe.Sizeof(&x)) // 8 bytes (64-bit)
}
```

#### CFS Scheduler — OS chọn process nào chạy tiếp?

Khi có 100 processes READY nhưng chỉ 4 CPU cores, OS cần quyết định: **chạy cái nào trước?** Linux dùng **CFS (Completely Fair Scheduler)** — chia CPU "công bằng" cho mọi process.

```
╔═══════════════════════════════════════════════════════════════╗
║   CFS SCHEDULER — CHIA CPU CÔNG BẰNG                        ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Nguyên tắc: Process nào DÙNG ÍT CPU NHẤT → được chạy TRƯỚC║
║                                                               ║
║  CFS dùng khái niệm "vruntime" (virtual runtime):           ║
║  → Mỗi process có vruntime = thời gian CPU ĐÃ DÙNG        ║
║  → Process có vruntime THẤP NHẤT = được chọn chạy tiếp     ║
║  → Kết quả: mọi process dùng CPU gần bằng nhau!           ║
║                                                               ║
║  CẤU TRÚC DỮ LIỆU: Red-Black Tree                          ║
║                                                               ║
║              ┌────────────┐                                   ║
║              │ vruntime=50│ (root)                             ║
║              └─────┬──────┘                                   ║
║            ┌───────┴───────┐                                  ║
║       ┌────┴────┐    ┌─────┴────┐                             ║
║       │ vrt=30  │    │ vrt=70   │                             ║
║       └────┬────┘    └─────┬────┘                             ║
║       ┌────┴────┐    ┌─────┴────┐                             ║
║       │ vrt=20 ★│    │ vrt=60   │                             ║
║       └─────────┘    └──────────┘                             ║
║            ↑                                                   ║
║       LEFTMOST = vruntime thấp nhất                          ║
║       → Chọn process này chạy tiếp! (O(1) lookup)           ║
║       → Insert/remove: O(log n)                              ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  NICE VALUE → ẢNH HƯỞNG đến tốc độ tăng vruntime:          ║
║                                                               ║
║  nice = -20 (cao nhất): vruntime tăng CHẬM → được chạy NHIỀU║
║  nice =   0 (mặc định): vruntime tăng bình thường           ║
║  nice = +19 (thấp nhất): vruntime tăng NHANH → ít được chạy║
║                                                               ║
║  Ví dụ: 2 processes, 1 CPU core                             ║
║  ┌──────────────────────────────────────────────┐            ║
║  │  Process A (nice=0):  chạy 50% CPU time      │            ║
║  │  Process B (nice=0):  chạy 50% CPU time      │            ║
║  │                                                │            ║
║  │  Process A (nice=-10): chạy 75% CPU time!    │            ║
║  │  Process B (nice=0):   chạy 25% CPU time     │            ║
║  │                                                │            ║
║  │  $ renice -10 -p $(pidof myserver)  ← ưu tiên│            ║
║  │  $ nice -n -10 ./myserver           ← chạy mới│            ║
║  └──────────────────────────────────────────────┘            ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  TIME SLICE — Mỗi lần chạy bao lâu?                        ║
║                                                               ║
║  CFS KHÔNG dùng fixed time slice!                            ║
║  → Target latency: 6ms (mặc định)                           ║
║  → Nếu 6 processes READY: mỗi cái chạy ~1ms rồi switch    ║
║  → Nếu 60 processes: mỗi cái ~0.1ms (nhưng min = 0.75ms)  ║
║                                                               ║
║  → Nhiều processes = switch nhiều hơn = overhead TĂNG!      ║
║  → Đây là lý do Go dùng ÍT OS threads (GOMAXPROCS)        ║
║    và nhiều goroutines thay vì nhiều threads!                ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  GO & CFS:                                                    ║
║  ┌──────────────────────────────────────────────┐            ║
║  │  GOMAXPROCS = 4 → Go tạo 4 OS threads (M)   │            ║
║  │  CFS quản lý 4 threads này, KHÔNG biết       │            ║
║  │  goroutines!                                   │            ║
║  │                                                │            ║
║  │  Go scheduler (user-space) quản lý goroutines │            ║
║  │  → CFS lo phân chia CPU cho threads           │            ║
║  │  → Go scheduler lo phân chia threads cho      │            ║
║  │    goroutines                                   │            ║
║  │  → 2 tầng scheduling → overhead thấp!        │            ║
║  │                                                │            ║
║  │  Tip: GOMAXPROCS = số CPU cores               │            ║
║  │  → Mỗi M thread chạy trên 1 core             │            ║
║  │  → CFS gần như KHÔNG cần switch M threads!   │            ║
║  │  → Goroutine switch chỉ trong user-space     │            ║
║  └──────────────────────────────────────────────┘            ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```bash
# Xem scheduling info
$ chrt -p $(pidof myserver)
pid 1234's current scheduling policy: SCHED_OTHER  ← CFS!
pid 1234's current scheduling priority: 0

# Xem nice value
$ ps -o pid,ni,comm -p $(pidof myserver)
  PID  NI COMMAND
 1234   0 myserver    ← nice=0 (mặc định)

# Monitor scheduling realtime
$ perf sched record sleep 5    # Thu thập 5 giây
$ perf sched latency           # Xem scheduling latency
# Task              |  Runtime  |  Switches | Avg delay
# myserver:1234     | 250.000ms |      1200 |  0.015ms
#                     ↑ tổng CPU   ↑ switches  ↑ trung bình chờ
```

#### Signal Handling — Process nhận "thông báo" từ bên ngoài

**Signal** là cơ chế để kernel hoặc process khác **gửi thông báo** đến 1 process. Đây là nền tảng của graceful shutdown, reload config, và debug.

```
╔═══════════════════════════════════════════════════════════════╗
║   SIGNALS — THÔNG BÁO CHO PROCESS                           ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Analogy: Signal giống CHUÔNG CỬA.                          ║
║  → Bạn đang nấu ăn (RUNNING), chuông kêu.                  ║
║  → Bạn có 3 lựa chọn:                                       ║
║    1. MỞ CỬA: xử lý signal (custom handler)                ║
║    2. LỜ ĐI: ignore signal (SIG_IGN)                       ║
║    3. HÀNH ĐỘNG MẶC ĐỊNH: tùy loại chuông                  ║
║  → Nhưng có 1 signal KHÔNG THỂ lờ: SIGKILL (cảnh sát phá  ║
║    cửa — bạn PHẢI dừng lại, không có lựa chọn!)           ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  SIGNALS QUAN TRỌNG CHO BACKEND:                             ║
║                                                               ║
║  ┌──────────┬────────┬──────────────────────────────────┐    ║
║  │ Signal    │ Số     │ Mô tả                            │    ║
║  ├──────────┼────────┼──────────────────────────────────┤    ║
║  │ SIGTERM  │ 15     │ "Xin hãy dừng lại" (polite)     │    ║
║  │          │        │ → docker stop, kill PID          │    ║
║  │          │        │ → CÓ THỂ catch → graceful shutdown│   ║
║  │          │        │ → Đóng connections, flush data   │    ║
║  ├──────────┼────────┼──────────────────────────────────┤    ║
║  │ SIGKILL  │ 9      │ "CHẾT NGAY!" (không thương lượng)│    ║
║  │          │        │ → kill -9 PID                     │    ║
║  │          │        │ → KHÔNG THỂ catch! Kernel kill!  │    ║
║  │          │        │ → Data có thể mất, FD không close│    ║
║  ├──────────┼────────┼──────────────────────────────────┤    ║
║  │ SIGINT   │ 2      │ Ctrl+C trong terminal            │    ║
║  │          │        │ → CÓ THỂ catch                   │    ║
║  │          │        │ → Thường = SIGTERM behavior      │    ║
║  ├──────────┼────────┼──────────────────────────────────┤    ║
║  │ SIGHUP   │ 1      │ "Terminal đóng" / Reload config  │    ║
║  │          │        │ → nginx -s reload = SIGHUP       │    ║
║  │          │        │ → Reload config không restart!   │    ║
║  ├──────────┼────────┼──────────────────────────────────┤    ║
║  │ SIGUSR1  │ 10     │ User-defined signal              │    ║
║  │          │        │ → Go: dump all goroutine stacks  │    ║
║  │          │        │ → kill -USR1 PID → debug info   │    ║
║  ├──────────┼────────┼──────────────────────────────────┤    ║
║  │ SIGCHLD  │ 17     │ "Child process đã exit"          │    ║
║  │          │        │ → Parent nhận để reap zombie     │    ║
║  └──────────┴────────┴──────────────────────────────────┘    ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  DOCKER STOP FLOW:                                           ║
║  ┌────────────────────────────────────────────┐              ║
║  │ $ docker stop mycontainer                    │              ║
║  │                                              │              ║
║  │ ① Docker gửi SIGTERM đến PID 1 trong container│             ║
║  │ ② App nhận SIGTERM → bắt đầu graceful shutdown│             ║
║  │ ③ Chờ tối đa 10 giây (--stop-timeout)       │              ║
║  │ ④ Nếu chưa exit → Docker gửi SIGKILL!      │              ║
║  │ ⑤ Process bị kill NGAY LẬP TỨC             │              ║
║  │                                              │              ║
║  │ → Nếu app KHÔNG handle SIGTERM:            │              ║
║  │   → Chờ 10s rồi bị SIGKILL = data loss!   │              ║
║  └────────────────────────────────────────────┘              ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Go graceful shutdown pattern — **phỏng vấn hay hỏi!**

```go
package main

import (
    "context"
    "fmt"
    "net/http"
    "os"
    "os/signal"
    "syscall"
    "time"
)

func main() {
    // Setup HTTP server
    srv := &http.Server{Addr: ":8080"}

    // ① Tạo channel nhận signals
    sigCh := make(chan os.Signal, 1)
    signal.Notify(sigCh,
        syscall.SIGTERM, // docker stop, kill PID
        syscall.SIGINT,  // Ctrl+C
    )

    // ② Goroutine chờ signal
    go func() {
        sig := <-sigCh // Block cho đến khi nhận signal
        fmt.Printf("\n🛑 Nhận signal: %v\n", sig)

        // ③ Graceful shutdown: chờ requests đang xử lý hoàn tất
        ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
        defer cancel()

        fmt.Println("⏳ Đang graceful shutdown...")
        fmt.Println("   → Không nhận request mới")
        fmt.Println("   → Chờ requests đang xử lý xong (max 30s)")

        if err := srv.Shutdown(ctx); err != nil {
            // Hết 30s mà vẫn có requests → force close
            fmt.Printf("❌ Forced shutdown: %v\n", err)
        }
        fmt.Println("✅ Server shutdown sạch sẽ!")
    }()

    // ④ Start server (block cho đến khi Shutdown() được gọi)
    fmt.Println("🚀 Server running on :8080")
    if err := srv.ListenAndServe(); err != http.ErrServerClosed {
        fmt.Printf("❌ Server error: %v\n", err)
    }
}

// Flow khi Ctrl+C:
// 1. User nhấn Ctrl+C → kernel gửi SIGINT
// 2. Go runtime nhận signal → ghi vào sigCh
// 3. Goroutine đọc sigCh → gọi srv.Shutdown()
// 4. Shutdown(): listener.Close() (không nhận mới)
//    → chờ active requests hoàn tất
//    → timeout 30s nếu request lâu quá
// 5. srv.ListenAndServe() return ErrServerClosed
// 6. main() return → process exit(0) → sạch sẽ!
```

```bash
# Test graceful shutdown
$ ./myserver &                    # Chạy nền
$ curl localhost:8080/slow &      # Request mất 5s
$ kill -TERM $(pidof myserver)    # Gửi SIGTERM

# Output:
# 🛑 Nhận signal: terminated
# ⏳ Đang graceful shutdown...
#    → Không nhận request mới
#    → Chờ requests đang xử lý xong (max 30s)
# (5 giây sau, /slow request xong)
# ✅ Server shutdown sạch sẽ!

# Go: gửi SIGUSR1 để dump goroutine stacks (debug!)
$ kill -USR1 $(pidof myserver)
# → Go runtime in TẤT CẢ goroutine stack traces
# → Cực kỳ hữu ích để debug goroutine leak!
```

### 1.2 Process Memory Layout — Bản Đồ Bộ Nhớ

#### Tại sao cần hiểu Memory Layout?

Mỗi process có **address space riêng** — từ góc nhìn của process, nó nghĩ mình sở hữu toàn bộ bộ nhớ. Đây là ảo giác do **Virtual Memory** tạo ra (sẽ nói chi tiết ở §3).

Nhưng tại sao backend engineer cần quan tâm? Vì **90% production bugs liên quan đến memory**: memory leak (heap tăng mãi), stack overflow (đệ quy quá sâu), goroutine leak (goroutines tích tụ), cache bloat (cache chiếm hết RAM). Hiểu memory layout = biết data nằm ở đâu = debug nhanh hơn 10×.

> **Hình dung Memory Layout như một tòa nhà cao tầng:**
>
> - **Tầng hầm (Low address)**: phòng máy chủ, nơi chứa "bản thiết kế" (code) — ai cũng xem được nhưng KHÔNG được sửa
> - **Tầng 1-2**: kho lưu trữ cố định (Data, BSS) — đồ đạc đặt sẵn từ khi xây nhà
> - **Tầng 3 trở lên (Heap)**: phòng thuê linh hoạt — ai cần thì thuê, xài xong trả lại, có thể mở rộng lên tầng cao hơn
> - **Tầng trên cùng (Stack)**: phòng họp tạm — mở cuộc họp (gọi function) thì tạo phòng, họp xong phòng biến mất
> - **Penthouse (Kernel Space)**: chỉ ban quản lý (kernel) mới được vào!

```
╔═══════════════════════════════════════════════════════════════╗
║   PROCESS MEMORY LAYOUT (Linux x86-64)                       ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  High Address (0x7FFF_FFFF_FFFF)                              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │            KERNEL SPACE                       │             ║
║  │        (Process không được truy cập!)        │             ║
║  ├──────────────────────────────────────────────┤             ║
║  │            STACK  ↓                           │             ║
║  │  → Local variables, function call frames     │             ║
║  │  → Grow downward (từ cao xuống thấp)        │             ║
║  │  → Mỗi thread có stack riêng!               │             ║
║  │  → Default: 8MB (Linux), 1MB (Go goroutine   │             ║
║  │    bắt đầu 2KB, grow tự động!)              │             ║
║  │                                                │             ║
║  │         ↓ ↓ ↓  (grows down)                  │             ║
║  │  ┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈  │             ║
║  │         Unmapped region (guard page)         │             ║
║  │  ┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈┈  │             ║
║  │         ↑ ↑ ↑  (grows up)                    │             ║
║  │                                                │             ║
║  │            HEAP  ↑                            │             ║
║  │  → Dynamic allocation (malloc/new)            │             ║
║  │  → Go: runtime manages via mspan/mcache      │             ║
║  │  → Grow upward (từ thấp lên cao)            │             ║
║  ├──────────────────────────────────────────────┤             ║
║  │            BSS  (Uninitialized data)          │             ║
║  │  → var count int  // = 0, OS cấp zero pages │             ║
║  ├──────────────────────────────────────────────┤             ║
║  │            DATA (Initialized data)            │             ║
║  │  → var name = "hello" // giá trị cụ thể    │             ║
║  ├──────────────────────────────────────────────┤             ║
║  │            TEXT (Code)                         │             ║
║  │  → Machine instructions (read-only!)          │             ║
║  │  → Shared giữa các process chạy cùng binary │             ║
║  └──────────────────────────────────────────────┘             ║
║  Low Address (0x0000_0000_0000)                               ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Giải thích từng vùng nhớ — từ dưới lên trên

**① TEXT Segment (Code) — "Bản thiết kế"**

Đây là nơi chứa **machine instructions** — mã máy mà CPU thực thi trực tiếp. Khi Go compiler biên dịch `main.go`, kết quả là chuỗi bytes mà CPU hiểu (ví dụ `0x48 0x89 0xe5` = `mov rbp, rsp` trên x86-64).

Đặc điểm quan trọng:

- **Read-only**: CPU chỉ ĐỌC code, không BAO GIỜ ghi vào đây. Nếu process cố ghi vào TEXT segment → `SIGSEGV` (segmentation fault) → process crash!
- **Shared**: 100 nginx workers chạy cùng binary → chỉ có 1 bản TEXT trong RAM. Kernel map virtual address của mỗi process đến cùng physical pages. Tiết kiệm cực kỳ!
- **Kích thước cố định**: không grow/shrink khi chạy.

```go
// Toàn bộ code compiled của hàm này nằm trong TEXT segment:
func handleRequest(w http.ResponseWriter, r *http.Request) {
    // machine instructions cho hàm này: ~200 bytes trong TEXT
    w.Write([]byte("Hello"))
}
```

**② DATA Segment — "Đồ đạc đã sắp sẵn"**

Chứa **biến toàn cục đã được khởi tạo với giá trị cụ thể**. Giá trị này được ghi trong binary file, nên khi process start, kernel copy trực tiếp từ binary vào RAM.

```go
// Các biến này nằm trong DATA segment:
var appName = "myserver"        // string đã có giá trị
var maxRetries = 3              // int đã có giá trị
var defaultTimeout = 30.0       // float64 đã có giá trị

// Nằm trong binary file → binary lớn hơn nếu có nhiều initialized globals
```

**③ BSS Segment — "Phòng trống đã dọn sạch"**

Chứa **biến toàn cục CHƯA khởi tạo** (hoặc khởi tạo = 0). Tên BSS từ "Block Started by Symbol" (term cổ từ assembly).

Điểm hay: BSS **KHÔNG chiếm không gian trong binary file**! Binary chỉ ghi "cần 1000 bytes BSS", khi process start, kernel cấp pages đã fill zero. Đây là lý do `var arr [1000000]int` không làm binary tăng 8MB!

```go
// Các biến này nằm trong BSS segment:
var count int                   // = 0 (zero value)
var buffer [4096]byte           // = all zeros
var isReady bool                // = false

// BSS không tăng kích thước binary!
// Kernel cấp "zero pages" khi process start
```

**④ HEAP — "Phòng thuê linh hoạt"**

Đây là vùng nhớ **quan trọng nhất** cho backend engineer. Heap là nơi chứa tất cả data được **allocate động** trong runtime — struct, slice, map, string, channel, interface... bất cứ thứ gì tồn tại lâu hơn function call tạo ra nó.

Heap grow **từ dưới lên** (low → high address), ngược chiều với Stack. Vùng giữa Heap và Stack là **unmapped region** — nếu 2 bên grow chạm nhau thì process hết memory!

```go
func processOrder(orderID string) *Order {
    // order được allocate trên HEAP vì:
    // 1. Trả về pointer → data phải tồn tại sau khi function return
    // 2. Go escape analysis phát hiện biến "escape" khỏi stack
    order := &Order{
        ID:     orderID,        // string trên heap
        Items:  make([]Item, 0), // slice trên heap
        Total:  0,
    }
    return order  // pointer trỏ đến heap memory
}

// Sau khi processOrder() return:
// - Stack frame bị hủy (local variables biến mất)
// - Nhưng Order{} vẫn sống trên HEAP
// - Sống cho đến khi GC phát hiện không ai trỏ đến nữa → thu hồi
```

**Tại sao heap chậm hơn stack?**

| Tiêu chí    | Stack                                         | Heap                                    |
| ----------- | --------------------------------------------- | --------------------------------------- |
| Allocate    | ~1ns (chỉ di chuyển stack pointer)            | ~25-100ns (tìm free block, bookkeeping) |
| Free        | Tự động khi function return                   | GC phải scan toàn bộ heap (pause!)      |
| Cache       | Cực tốt (data liền kề, sequential)            | Kém (data phân tán, random access)      |
| Thread-safe | Không cần lock (mỗi goroutine có stack riêng) | Cần synchronization (GC + allocator)    |

**⑤ STACK — "Phòng họp tạm"**

Stack là vùng nhớ **tự động, nhanh, tuần tự**. Mỗi khi gọi function, OS "đẩy" stack frame mới lên đỉnh stack; khi function return, frame bị "bật" ra. Như chồng đĩa trong nhà hàng — đĩa nào đặt lên cuối cùng sẽ được lấy ra trước (LIFO — Last In, First Out).

```go
func main() {                    // Stack frame cho main()
    result := calculate(10, 20)   // Push frame cho calculate()
    fmt.Println(result)           // calculate() return → Pop frame
}

func calculate(a, b int) int {    // Stack frame: a=10, b=20
    sum := a + b                  // sum=30 (trên stack)
    return sum                    // return → frame bị pop
}
```

```
╔═══════════════════════════════════════════════════════════════╗
║   STACK FRAMES — KHI GỌI calculate(10, 20)                  ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Trước khi gọi:       Trong calculate():    Sau return:      ║
║                                                               ║
║  ┌──────────────┐   ┌──────────────┐   ┌──────────────┐     ║
║  │              │   │ sum = 30     │   │              │     ║
║  │              │   │ b = 20       │   │              │     ║
║  │              │   │ a = 10       │   │              │     ║
║  │              │   │ return addr  │   │              │     ║
║  │              │   ├──────────────┤   │              │     ║
║  │ result = ?   │   │ result = ?   │   │ result = 30  │     ║
║  │ main() frame │   │ main() frame │   │ main() frame │     ║
║  └──────────────┘   └──────────────┘   └──────────────┘     ║
║       SP ↑               SP ↑               SP ↑             ║
║  (stack pointer)    (di chuyển LÊN)   (quay lại CHỖ CŨ)    ║
║                                                               ║
║  Quan sát:                                                    ║
║  → Allocate: chỉ cần di chuyển SP → ~1 nanosecond!        ║
║  → Free: di chuyển SP lại → ~1 nanosecond!                 ║
║  → KHÔNG cần GC, KHÔNG cần malloc, KHÔNG cần free!          ║
║  → Đây là lý do stack nhanh hơn heap 10-100×               ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

**Go stack đặc biệt ở chỗ nào?**

Linux thread stack: cố định 8MB, không thể grow → quá nhỏ thì stack overflow, quá lớn thì lãng phí.

Go goroutine stack: bắt đầu chỉ **2KB**, và **tự động grow** khi cần! Go runtime chèn stack-check vào mỗi function call — nếu stack sắp hết, runtime sẽ allocate stack mới lớn gấp đôi, copy toàn bộ data sang, update tất cả pointers. Goroutine stack cũng có thể **shrink** khi GC chạy — nếu stack đang dùng ít hơn 1/4 capacity, runtime thu nhỏ lại.

```
╔═══════════════════════════════════════════════════════════════╗
║   GO GOROUTINE STACK GROWTH                                  ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Ban đầu:   Cần thêm:    Sau grow:                          ║
║  ┌──────┐   ┌──────┐     ┌────────────┐                     ║
║  │ 2KB  │   │ 2KB  │     │            │                     ║
║  │ đầy! │   │ đầy! │──►  │   4KB      │ (copy + update ptrs)║
║  └──────┘   └──────┘     │            │                     ║
║                           └────────────┘                     ║
║                                                               ║
║  Tiếp tục grow: 4KB → 8KB → 16KB → ... → 1GB (max)        ║
║  Shrink khi GC: nếu dùng < 1/4 → thu nhỏ!                 ║
║                                                               ║
║  → 1M goroutines × 2KB = 2GB (ban đầu)                     ║
║  → 1M Linux threads × 8MB = 8TB (KHÔNG KHẢ THI!)           ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

**⑥ KERNEL SPACE — "Penthouse chỉ dành cho ban quản lý"**

Phần trên cùng của address space được dành riêng cho kernel. Process ở user mode (Ring 3) **tuyệt đối không được truy cập** vùng này. Nếu process cố đọc/ghi vào kernel space → `SIGSEGV` → crash!

Tuy nhiên, kernel map code & data của mình vào address space của MỌI process. Khi process gọi syscall (ví dụ `read()`), CPU chuyển từ Ring 3 → Ring 0, và lúc đó kernel code trong phần này được phép truy cập. Kết thúc syscall, CPU quay lại Ring 3 — kernel space lại bị "khóa".

#### Go Escape Analysis — Biến nằm ở Stack hay Heap?

Đây là **kiến thức then chốt** cho Go performance. Go compiler tự quyết định biến nằm trên stack (nhanh) hay heap (chậm hơn, cần GC) qua **escape analysis**.

```go
// ❌ ESCAPE TO HEAP — biến "sống" lâu hơn function
func createUser(name string) *User {
    u := User{Name: name}  // u ESCAPE to heap! (trả về pointer)
    return &u
}

// ✅ STAY ON STACK — biến chết khi function return
func processUser(name string) string {
    u := User{Name: name}  // u stays on STACK (không escape)
    return u.Name           // trả về copy, không phải pointer
}

// ❌ ESCAPE — interface{} gây escape
func log(msg interface{}) {  // msg escape to heap!
    fmt.Println(msg)
}

// ❌ ESCAPE — closure capture
func counter() func() int {
    count := 0                // count escape to heap!
    return func() int {       // closure giữ reference đến count
        count++
        return count
    }
}
```

Kiểm tra escape analysis:

```bash
$ go build -gcflags="-m" main.go
./main.go:5:2: moved to heap: u        ← ESCAPE! Heap allocation
./main.go:10:2: u does not escape      ← Stack! Nhanh hơn!
./main.go:15:6: msg escapes to heap    ← Interface gây escape!
./main.go:20:2: moved to heap: count   ← Closure capture!
```

**Quy tắc ngón tay cho Go**:

- Return pointer (`&x`) → escape to heap
- Assign vào interface → escape to heap
- Gửi qua channel → escape to heap
- Closure capture biến → biến escape to heap
- Slice/map append có thể gây grow → escape to heap

#### Debug Memory — Công cụ thực tế

```bash
# 1. Xem memory layout thật của Go process
$ cat /proc/$(pidof myserver)/maps
# Thấy rõ vùng [heap], [stack], code segments

# 2. Go pprof — phân tích heap
$ go tool pprof http://localhost:6060/debug/pprof/heap
(pprof) top 10           # Top 10 allocation sites
(pprof) list processOrder # Xem dòng code nào allocate nhiều

# 3. Go pprof — tìm memory leak
$ go tool pprof -diff_base=before.prof after.prof
# So sánh 2 snapshots → thấy gì tăng = leak!

# 4. Runtime stats
import "runtime"
var m runtime.MemStats
runtime.ReadMemStats(&m)
fmt.Printf("Heap: %d MB\n", m.HeapAlloc/1024/1024)    // Heap đang dùng
fmt.Printf("Stack: %d MB\n", m.StackInuse/1024/1024)   // Stack đang dùng
fmt.Printf("Goroutines: %d\n", runtime.NumGoroutine())  // Số goroutines
```

#### Backend Relevance — Tại sao quan trọng?

```
╔═══════════════════════════════════════════════════════════════╗
║   MEMORY LAYOUT & PRODUCTION BUGS                            ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  BUG 1: Memory leak (Heap tăng mãi):                        ║
║  → Goroutine leak: goroutine chờ channel không bao giờ close║
║  → Slice leak: sub-slice giữ reference đến array gốc lớn  ║
║  → Map không bao giờ delete keys cũ                        ║
║  → Debug: pprof heap → tìm allocation site                 ║
║                                                               ║
║  BUG 2: Goroutine stack tăng quá lớn:                       ║
║  → Đệ quy vô hạn: mỗi call = 1 frame trên stack          ║
║  → Go cho phép stack grow đến 1GB → rồi panic!            ║
║  → Debug: pprof goroutine → xem goroutine profile          ║
║                                                               ║
║  BUG 3: OOM Kill (Out Of Memory):                            ║
║  → Heap vượt quá cgroup limit (Docker --memory=512m)       ║
║  → Linux OOM Killer chọn process có "badness" cao nhất    ║
║  → Fix: GOMEMLIMIT=400MiB (Go 1.19+, set < cgroup limit)  ║
║                                                               ║
║  BUG 4: High GC pressure:                                    ║
║  → Quá nhiều allocations trên heap → GC chạy liên tục     ║
║  → Fix: escape analysis, sync.Pool, pre-allocate slices    ║
║  → Fix: giảm GOGC (mặc định 100)                          ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Go Heap Allocator — Runtime quản lý Heap thế nào?

Go **KHÔNG gọi `malloc()` trực tiếp** như C. Go runtime có **heap allocator riêng**, tối ưu cho goroutines — đây là lý do `new()` và `make()` trong Go cực nhanh:

```
╔═══════════════════════════════════════════════════════════════╗
║   GO HEAP ALLOCATOR — 3 TẦNG                                ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Analogy: Như hệ thống kho hàng 3 cấp                       ║
║  → mcache  = Túi xách riêng (không cần xin ai)             ║
║  → mcentral = Kho tầng (chung cho cả phòng)               ║
║  → mheap    = Kho lớn (chung cho cả công ty)              ║
║                                                               ║
║  ┌─────────────────────────────────────────────────────┐     ║
║  │  Goroutine muốn allocate 48 bytes                    │     ║
║  │                                                       │     ║
║  │  ① TÌM TRONG mcache (per-P, KHÔNG cần lock!)       │     ║
║  │     ┌──────────────────────────────────────┐         │     ║
║  │     │ P0's mcache:                          │         │     ║
║  │     │   size class 48B: [span1][span2]...  │         │     ║
║  │     │   size class 64B: [span3]...          │         │     ║
║  │     │   size class 128B: [span4]...         │         │     ║
║  │     │   ... (67 size classes tổng cộng)    │         │     ║
║  │     └──────────────────────────────────────┘         │     ║
║  │     → Tìm free slot trong span của size class 48B   │     ║
║  │     → Nếu CÓ → trả về pointer, XONG! (~25ns)     │     ║
║  │     → KHÔNG cần lock vì mcache riêng cho mỗi P!   │     ║
║  │                                                       │     ║
║  │  ② NẾU mcache HẾT → xin từ mcentral (cần lock)   │     ║
║  │     ┌──────────────────────────────────────┐         │     ║
║  │     │ mcentral (shared, per size class):    │         │     ║
║  │     │   → List các mspans có slots trống   │         │     ║
║  │     │   → Lock → lấy 1 span → Unlock      │         │     ║
║  │     │   → Đưa span vào mcache của P0      │         │     ║
║  │     └──────────────────────────────────────┘         │     ║
║  │                                                       │     ║
║  │  ③ NẾU mcentral HẾT → xin từ mheap (global lock) │     ║
║  │     ┌──────────────────────────────────────┐         │     ║
║  │     │ mheap (global, 1 per process):        │         │     ║
║  │     │   → Quản lý toàn bộ virtual memory   │         │     ║
║  │     │   → Chia thành "arenas" (64MB mỗi cái)│        │     ║
║  │     │   → mmap() từ OS nếu cần thêm RAM   │         │     ║
║  │     └──────────────────────────────────────┘         │     ║
║  │                                                       │     ║
║  └─────────────────────────────────────────────────────┘     ║
║                                                               ║
║  SIZE CLASSES (67 classes, 8B đến 32KB):                    ║
║  ┌──────────┬─────────────────────────────────────┐         ║
║  │ Request   │ Actual Allocation (size class)      │         ║
║  ├──────────┼─────────────────────────────────────┤         ║
║  │ 1-8B     │ 8 bytes (class 1)                   │         ║
║  │ 9-16B    │ 16 bytes (class 2)                  │         ║
║  │ 17-32B   │ 32 bytes (class 3)                  │         ║
║  │ 33-48B   │ 48 bytes (class 4)  ← ví dụ trên  │         ║
║  │ 49-64B   │ 64 bytes (class 5)                  │         ║
║  │ ...       │ ...                                  │         ║
║  │ >32KB    │ Direct from mheap (large object)    │         ║
║  └──────────┴─────────────────────────────────────┘         ║
║                                                               ║
║  → Cần 33 bytes? Allocate 48 bytes → 15 bytes wasted       ║
║  → Trade-off: waste ~15% RAM để allocate NHANH hơn 10×    ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// Xem Go allocator stats
package main

import (
    "fmt"
    "runtime"
)

func main() {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)

    fmt.Printf("=== GO HEAP ALLOCATOR STATS ===\n")
    fmt.Printf("HeapAlloc:    %d MB (heap đang dùng)\n", m.HeapAlloc/1024/1024)
    fmt.Printf("HeapSys:      %d MB (heap reserved từ OS)\n", m.HeapSys/1024/1024)
    fmt.Printf("HeapIdle:     %d MB (heap free, chưa trả OS)\n", m.HeapIdle/1024/1024)
    fmt.Printf("HeapReleased: %d MB (đã trả lại OS)\n", m.HeapReleased/1024/1024)
    fmt.Printf("HeapObjects:  %d (số objects trên heap)\n", m.HeapObjects)
    fmt.Printf("MSpanInuse:   %d MB (spans đang dùng)\n", m.MSpanInuse/1024/1024)
    fmt.Printf("MCacheInuse:  %d KB (mcache đang dùng)\n", m.MCacheInuse/1024)
    fmt.Printf("Mallocs:      %d (tổng allocations)\n", m.Mallocs)
    fmt.Printf("Frees:        %d (tổng frees)\n", m.Frees)
    fmt.Printf("Live Objects: %d\n", m.Mallocs-m.Frees)

    // Tip: HeapIdle cao = runtime giữ memory "dự trữ"
    // → runtime.debug.FreeOSMemory() để force trả OS
    // → Nhưng ĐỪNG gọi thường xuyên! Sẽ phải mmap() lại
}
```

#### Memory-Mapped Files (mmap) — Map file vào address space

**mmap** là system call cho phép **map file trên disk vào virtual address space** — đọc file như đọc array trong RAM! Đây là cách kernel load binary, cách Go allocator xin memory, và cách database đọc data files:

```
╔═══════════════════════════════════════════════════════════════╗
║   MMAP — FILE TRONG BỘ NHỚ                                  ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Cách đọc file TRUYỀN THỐNG:                                ║
║  ┌─────────────────────────────────────────────┐             ║
║  │ User: read(fd, buf, 4096)                     │             ║
║  │   1. Syscall → enter kernel                   │             ║
║  │   2. Kernel đọc disk → kernel buffer         │             ║
║  │   3. Copy kernel buffer → user buffer        │             ║
║  │   4. Return to user                            │             ║
║  │   → 2 lần copy data! (disk→kernel→user)     │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
║  Cách dùng MMAP:                                             ║
║  ┌─────────────────────────────────────────────┐             ║
║  │ User: ptr = mmap(fd, 4096)                    │             ║
║  │   1. Kernel map file pages vào address space  │             ║
║  │   2. User đọc *ptr → page fault →            │             ║
║  │      kernel load page từ disk                  │             ║
║  │   3. Sau lần đầu: đọc trực tiếp từ RAM!     │             ║
║  │   → 0 lần copy! (zero-copy I/O)             │             ║
║  │   → File = array trong memory                 │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
║  AI DÙNG MMAP?                                               ║
║  ┌────────────────────┬────────────────────────────┐         ║
║  │ Ai                  │ Dùng mmap để làm gì      │         ║
║  ├────────────────────┼────────────────────────────┤         ║
║  │ Linux kernel       │ Load .text segment từ ELF │         ║
║  │ Go runtime         │ Xin memory cho heap (arena)│         ║
║  │ SQLite             │ Đọc database file           │         ║
║  │ MongoDB (WiredTiger)│ Memory-mapped storage      │         ║
║  │ Kafka              │ Log segment files           │         ║
║  │ Shared libraries   │ Load .so files              │         ║
║  │ Docker             │ Overlay filesystem layers  │         ║
║  └────────────────────┴────────────────────────────┘         ║
║                                                               ║
║  Go runtime xin memory:                                      ║
║  → mheap gọi mmap(64MB) để tạo arena mới                  ║
║  → Kernel chỉ MAP, không allocate RAM thật!                ║
║  → Khi goroutine dùng → page fault → RAM thật cấp         ║
║  → Đây là lý do HeapSys >> HeapAlloc!                      ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// Go mmap example — đọc file lớn hiệu quả
package main

import (
    "fmt"
    "os"
    "syscall"
)

func readWithMmap(filename string) ([]byte, error) {
    f, err := os.Open(filename)
    if err != nil {
        return nil, err
    }
    defer f.Close()

    info, _ := f.Stat()
    size := int(info.Size())

    // mmap: map TOÀN BỘ file vào memory
    data, err := syscall.Mmap(
        int(f.Fd()),     // file descriptor
        0,               // offset from start
        size,            // length
        syscall.PROT_READ, // read-only
        syscall.MAP_SHARED, // shared mapping
    )
    if err != nil {
        return nil, err
    }

    // data bây giờ = []byte trỏ vào file!
    // Đọc data[1000] → kernel load page chứa byte 1000 từ disk
    // Đọc lại data[1000] → đọc từ RAM (cached), ~200ns!
    fmt.Printf("First 10 bytes: %s\n", data[:10])

    // PHẢI unmap khi xong!
    defer syscall.Munmap(data)
    return data, nil
}

// Khi nào dùng mmap vs read():
// mmap: file lớn (>MB), random access, read-only → database, log analysis
// read(): file nhỏ, sequential, cần copy → config files, request body
```

#### Memory Alignment & Padding — CPU đọc RAM theo "khối"

CPU **KHÔNG đọc RAM từng byte**. CPU đọc theo **word size** (8 bytes trên 64-bit). Nếu data không aligned (nằm lệch ranh giới word), CPU phải đọc 2 lần! Vì vậy compiler **thêm padding** (bytes trống) vào struct để data aligned:

```
╔═══════════════════════════════════════════════════════════════╗
║   MEMORY ALIGNMENT — TẠI SAO STRUCT SIZE LẠ?               ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  struct { bool; int64; bool } — bạn nghĩ = 1+8+1 = 10 bytes?║
║  Thực tế: 24 bytes! Tại sao?                                ║
║                                                               ║
║  ❌ SẮP XẾP TỒI (24 bytes):                                ║
║  ┌─────┬───────┬─────────────────┬─────┬───────┐            ║
║  │bool │padding│     int64       │bool │padding│            ║
║  │ 1B  │  7B   │      8B         │ 1B  │  7B   │            ║
║  └─────┴───────┴─────────────────┴─────┴───────┘            ║
║  Offset: 0       1-7      8-15          16     17-23         ║
║  → 14 bytes padding = 58% lãng phí!                        ║
║                                                               ║
║  ✅ SẮP XẾP TỐT (16 bytes):                                ║
║  struct { int64; bool; bool } ← sắp xếp LỚN → NHỎ        ║
║  ┌─────────────────┬─────┬─────┬───────┐                    ║
║  │     int64       │bool │bool │padding│                    ║
║  │      8B         │ 1B  │ 1B  │  6B   │                    ║
║  └─────────────────┴─────┴─────┴───────┘                    ║
║  → 6 bytes padding = 37% lãng phí                           ║
║  → TIẾT KIỆM 8 bytes per struct!                           ║
║                                                               ║
║  QUY TẮC: Fields cùng alignment group liền kề nhau         ║
║  → 8-byte fields trước (int64, float64, pointer)            ║
║  → 4-byte fields (int32, float32)                            ║
║  → 2-byte fields (int16)                                     ║
║  → 1-byte fields cuối (bool, byte)                          ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
package main

import (
    "fmt"
    "unsafe"
)

// ❌ Tồi: 24 bytes (padding 14 bytes)
type BadStruct struct {
    a bool    // 1 byte + 7 padding
    b int64   // 8 bytes
    c bool    // 1 byte + 7 padding
}

// ✅ Tốt: 16 bytes (padding 6 bytes)
type GoodStruct struct {
    b int64   // 8 bytes
    a bool    // 1 byte
    c bool    // 1 byte + 6 padding
}

func main() {
    fmt.Printf("BadStruct:  %d bytes\n", unsafe.Sizeof(BadStruct{}))   // 24
    fmt.Printf("GoodStruct: %d bytes\n", unsafe.Sizeof(GoodStruct{}))  // 16
    // 1M objects: Bad = 24MB, Good = 16MB → tiết kiệm 33%!

    // Xem alignment requirement
    fmt.Printf("bool align:    %d\n", unsafe.Alignof(bool(false)))  // 1
    fmt.Printf("int32 align:   %d\n", unsafe.Alignof(int32(0)))     // 4
    fmt.Printf("int64 align:   %d\n", unsafe.Alignof(int64(0)))     // 8
    fmt.Printf("string align:  %d\n", unsafe.Alignof(""))           // 8
    fmt.Printf("pointer align: %d\n", unsafe.Alignof(new(int)))     // 8

    // Xem offset chi tiết
    var bad BadStruct
    fmt.Printf("\nBadStruct field offsets:\n")
    fmt.Printf("  a (bool):  offset=%d\n", unsafe.Offsetof(bad.a))  // 0
    fmt.Printf("  b (int64): offset=%d\n", unsafe.Offsetof(bad.b))  // 8 (7 bytes padding!)
    fmt.Printf("  c (bool):  offset=%d\n", unsafe.Offsetof(bad.c))  // 16
}

// Tool: go vet ./... sẽ CẢNH BÁO struct alignment issues
// Tool: fieldalignment (go install golang.org/x/tools/go/analysis/passes/fieldalignment/cmd/fieldalignment)
// $ fieldalignment -fix ./...
// → Tự động sắp xếp lại struct fields!
```

#### Go Garbage Collector — Ai dọn rác trên Heap?

Heap allocation dễ, nhưng **ai free?** C/C++ = manual (`free()`), dễ leak hoặc double-free. Go dùng **Garbage Collector (GC)** — tự động tìm và thu hồi memory không còn ai reference:

```
╔═══════════════════════════════════════════════════════════════╗
║   GO GC — TRI-COLOR MARK-AND-SWEEP                          ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Tri-color algorithm:                                        ║
║  → TRẮNG: chưa scan (có thể là rác)                        ║
║  → XÁM: đang scan (đã thấy nhưng chưa scan children)      ║
║  → ĐEN: scan xong (reachable, KHÔNG phải rác)             ║
║                                                               ║
║  ① Ban đầu: tất cả objects = TRẮNG                         ║
║  ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐                            ║
║  │ ○ │ │ ○ │ │ ○ │ │ ○ │ │ ○ │  all WHITE                 ║
║  └───┘ └───┘ └───┘ └───┘ └───┘                            ║
║                                                               ║
║  ② Mark roots (stack vars, globals) = XÁM                  ║
║  ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐                            ║
║  │ ◐ │ │ ○ │ │ ◐ │ │ ○ │ │ ○ │  roots → GRAY             ║
║  └─┬─┘ └───┘ └─┬─┘ └───┘ └───┘                            ║
║    │ ref         │ ref                                       ║
║    ▼             ▼                                           ║
║  ┌───┐         ┌───┐                                        ║
║  │ ○ │         │ ○ │  children still WHITE                 ║
║  └───┘         └───┘                                        ║
║                                                               ║
║  ③ Scan XÁM: tô children thành XÁM, tô bản thân ĐEN    ║
║  ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐                            ║
║  │ ● │ │ ○ │ │ ● │ │ ○ │ │ ○ │  scanned → BLACK          ║
║  └─┬─┘ └───┘ └─┬─┘ └───┘ └───┘                            ║
║    │             │                                           ║
║    ▼             ▼                                           ║
║  ┌───┐         ┌───┐                                        ║
║  │ ◐ │         │ ◐ │  children → GRAY                     ║
║  └───┘         └───┘                                        ║
║                                                               ║
║  ④ Lặp lại cho đến khi KHÔNG CÒN XÁM                     ║
║  ⑤ Sweep: tất cả objects CÒN TRẮNG = RÁC → thu hồi!     ║
║  ┌───┐ ┌───┐ ┌───┐ ┌───┐ ┌───┐                            ║
║  │ ● │ │ ✗ │ │ ● │ │ ✗ │ │ ✗ │  WHITE = freed!           ║
║  └───┘ └───┘ └───┘ └───┘ └───┘                            ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  GO GC PHASES (concurrent!):                                 ║
║                                                               ║
║   Goroutines:  ████████████████████████████████████████       ║
║   GC:                                                         ║
║   ┌──────┐ ┌────────────────┐ ┌──────┐ ┌────────┐           ║
║   │ STW  │ │  Concurrent    │ │ STW  │ │Concurr.│           ║
║   │ Mark │ │  Mark          │ │ Mark │ │ Sweep  │           ║
║   │Setup │ │  (scan heap)   │ │Term. │ │        │           ║
║   │~10μs │ │  (goroutines   │ │~10μs │ │(free   │           ║
║   │      │ │   VẪN CHẠY!)  │ │      │ │ pages) │           ║
║   └──────┘ └────────────────┘ └──────┘ └────────┘           ║
║                                                               ║
║   STW = Stop-The-World (tất cả goroutines dừng)            ║
║   → Go 1.8+: STW chỉ ~10-100μs! (vs Java: ~100ms-10s)    ║
║   → Concurrent mark: GC scan song song với goroutines      ║
║   → Write barrier: tracking mutations during concurrent mark║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// GC tuning — production settings
package main

import (
    "runtime"
    "runtime/debug"
)

func main() {
    // GOGC: target heap growth ratio (default = 100)
    // → GC chạy khi heap tăng 100% so với lần GC trước
    // → GOGC=50: GC chạy thường hơn (heap tăng 50% → GC)
    //   → dùng ÍT RAM hơn, nhưng CPU cho GC NHIỀU hơn
    // → GOGC=200: GC chạy ít hơn (heap tăng 200% → GC)
    //   → dùng NHIỀU RAM hơn, nhưng CPU cho GC ÍT hơn
    // → GOGC=off: TẮT GC hoàn toàn! (nguy hiểm)
    debug.SetGCPercent(100) // default

    // GOMEMLIMIT (Go 1.19+): hard memory limit
    // → Khi heap gần limit → GC chạy aggressive hơn
    // → Tránh OOM Kill trong Docker!
    debug.SetMemoryLimit(400 * 1024 * 1024) // 400MB

    // Docker best practice:
    // $ docker run --memory=512m -e GOMEMLIMIT=430MiB myserver
    // → Container limit: 512MB
    // → Go limit: 430MB (để 80MB cho stack, OS overhead)
    // → Khi heap gần 430MB: GC chạy liên tục thay vì OOM!

    // Force GC
    runtime.GC() // Trigger GC manually (debug only!)
}
```

```bash
# Xem GC hoạt động realtime
$ GODEBUG=gctrace=1 ./myserver 2>&1 | head -10

# Output giải thích:
# gc 1 @0.012s 3%: 0.021+1.2+0.009 ms clock, 0.17+0.8/1.0/0+0.074 ms cpu, 4->5->2 MB, 5 MB goal, 8 P
#
# gc 1         → Lần GC thứ 1
# @0.012s      → 12ms sau khi process start
# 3%           → 3% CPU time dành cho GC
# 0.021+1.2+0.009 ms clock:
#   0.021ms    → STW mark setup (TẤT CẢ goroutines dừng)
#   1.2ms      → Concurrent mark (goroutines VẪN CHẠY)
#   0.009ms    → STW mark termination
# 4->5->2 MB:
#   4MB        → Heap trước GC
#   5MB        → Heap peak during GC
#   2MB        → Heap sau GC (thu hồi 3MB!)
# 5 MB goal   → Target heap size trước khi GC chạy lại
# 8 P         → 8 GOMAXPROCS

# Nếu thấy GC chạy liên tục (>10% CPU) → allocating quá nhiều!
# Fix: sync.Pool, pre-allocate, reduce escape to heap
```

#### /proc/PID/maps — Xem memory map thật

Trên Linux, `/proc/PID/maps` cho bạn thấy **chính xác** mỗi vùng memory của process:

```bash
$ cat /proc/$(pidof myserver)/maps

# Output (annotated):
# Address range          Perms  Offset   Dev   Inode  Pathname
00400000-00640000 r-xp   00000000 fd:01 1234   /usr/bin/myserver
#  ↑ TEXT segment           ↑ r-x = read + execute (code!)
#  ├── 0x400000-0x640000 = 2.25MB of compiled Go code
#  └── 'p' = private mapping

00640000-006f0000 r--p   00240000 fd:01 1234   /usr/bin/myserver
#  ↑ RODATA segment        ↑ r-- = read only (string literals)

006f0000-00730000 rw-p   002f0000 fd:01 1234   /usr/bin/myserver
#  ↑ DATA + BSS segment    ↑ rw- = read + write (global vars)

c000000000-c000400000 rw-p  00000000 00:00 0
#  ↑ GO HEAP                ↑ rw- = read + write
#  ├── 0xc000000000 = Go heap bắt đầu ở đây
#  ├── Go runtime mmap() vùng này
#  └── Go pointers luôn bắt đầu bằng 0xc... (dễ nhận!)

7f2a00000000-7f2a00800000 rw-p 00000000 00:00 0
#  ↑ THREAD STACK           ↑ rw-
#  └── 8MB stack cho 1 OS thread

7ffd12300000-7ffd12400000 rw-p 00000000 00:00 0  [stack]
#  ↑ MAIN THREAD STACK      ↑ [stack] tag

7ffd12500000-7ffd12502000 r--p 00000000 00:00 0  [vvar]
7ffd12502000-7ffd12504000 r-xp 00000000 00:00 0  [vdso]
#  ↑ VDSO = Virtual Dynamic Shared Object
#  └── Kernel functions callable from userspace WITHOUT syscall!
#      → gettimeofday(), clock_gettime() = gần như free!
```

```go
// Đọc memory maps từ Go
package main

import (
    "fmt"
    "os"
)

func main() {
    pid := os.Getpid()
    data, _ := os.ReadFile(fmt.Sprintf("/proc/%d/maps", pid))
    fmt.Println(string(data))

    // Đọc memory summary
    status, _ := os.ReadFile(fmt.Sprintf("/proc/%d/status", pid))
    fmt.Println(string(status))
    // VmSize: tổng virtual memory
    // VmRSS:  resident set size (RAM thực dùng!)
    // VmData: heap size
    // VmStk:  stack size
    // Threads: số OS threads
}
```

#### 5 Whys: Tại sao Memory Layout quan trọng?

```
╔═══════════════════════════════════════════════════════════════╗
║   5 WHYS: TẠI SAO PHẢI HIỂU MEMORY LAYOUT?                 ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  WHY 1: Tại sao phải biết data nằm ở đâu?                  ║
║  → Vì stack vs heap quyết định PERFORMANCE!                 ║
║  → Stack: ~1ns allocate, KHÔNG cần GC                       ║
║  → Heap: ~25-100ns allocate, CẦN GC (pause!)               ║
║                                                               ║
║  WHY 2: Tại sao heap chậm hơn stack?                        ║
║  → Heap phải tìm free block (allocator overhead)            ║
║  → Heap data phân tán → cache miss nhiều                   ║
║  → GC phải scan toàn bộ heap → STW pauses                 ║
║                                                               ║
║  WHY 3: Tại sao Go cần escape analysis?                      ║
║  → Để compiler tự quyết stack vs heap                       ║
║  → Programmer KHÔNG cần malloc/free (an toàn hơn C)        ║
║  → Nhưng phải HIỂU để tối ưu (gcflags="-m")              ║
║                                                               ║
║  WHY 4: Tại sao memory layout ảnh hưởng production?        ║
║  → Memory leak = heap tăng mãi → OOM Kill                  ║
║  → Goroutine leak = stack tích tụ → RAM exhaustion         ║
║  → Struct padding = waste 30% RAM (fix = sắp xếp fields)  ║
║  → GC pressure = high latency (p99 tăng!)                  ║
║                                                               ║
║  WHY 5: Senior engineer cần biết gì thêm?                   ║
║  → mmap: zero-copy I/O, database internals                  ║
║  → Go allocator: mcache→mcentral→mheap (debug allocations) ║
║  → /proc/PID/maps: diagnose memory issues in production    ║
║  → GOMEMLIMIT: prevent OOM in Docker/K8s                   ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 1.3 Thread là gì? — Lightweight Process

#### Bài toán gốc: Tại sao cần Thread?

Hãy tưởng tượng bạn viết một web server. Mỗi request cần: đọc HTTP header, query database, xử lý logic, gửi response. Nếu dùng **1 process, 1 luồng xử lý**, thì khi request A đang chờ database (I/O wait — CPU idle!), request B, C, D... phải **xếp hàng chờ**, dù CPU hoàn toàn rảnh. Server của bạn xử lý 1 request/time — thảm họa performance!

Giải pháp 1: **Multi-process** (Apache cổ điển). Mỗi request = fork 1 process mới. Hoạt động được, nhưng:

- fork() tốn ~100μs + copy memory (dù có CoW)
- Mỗi process chiếm ~10MB riêng
- Giao tiếp giữa processes phải qua IPC (pipe, socket) — phức tạp & chậm
- 10K requests = 10K processes = 100GB RAM → **không khả thi!**

Giải pháp 2: **Multi-thread** (Tomcat, Java). Mỗi request = 1 thread trong CÙNG 1 process. Thread **chia sẻ bộ nhớ** với nhau → giao tiếp trực tiếp qua shared variables, tạo nhanh hơn, tốn ít RAM hơn. Đây là lý do thread được gọi là **"lightweight process"**.

> **Analogy chi tiết**: Nếu process là **một văn phòng** (có không gian riêng, bàn ghế, tài liệu, máy in), thì thread là **nhân viên** trong văn phòng đó. Các nhân viên:
>
> - **Chia sẻ**: tài liệu (heap), máy in (file descriptors), quy trình (code), bàn họp (globals)
> - **Riêng biệt**: bàn làm việc (stack), sổ ghi chép cá nhân (registers), nhiệm vụ hiện tại (program counter)
>
> Nếu 2 nhân viên cùng sửa 1 tài liệu cùng lúc mà không phối hợp → tài liệu **hỏng** (race condition!). Đây là cái giá phải trả cho việc chia sẻ.

#### Thread chia sẻ gì, sở hữu riêng gì?

Đây là điểm **then chốt** để hiểu mọi vấn đề concurrency:

```
╔═══════════════════════════════════════════════════════════════╗
║   THREAD: SHARED vs PRIVATE                                  ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ┌─────────────────────────────────────────────────┐         ║
║  │  SHARED giữa tất cả threads (SỞ HỮU CHUNG):   │         ║
║  │                                                   │         ║
║  │  ✓ Code (TEXT segment)   — cùng chạy 1 binary   │         ║
║  │  ✓ Heap                  — cùng malloc/new       │         ║
║  │  ✓ Global variables      — cùng đọc/ghi         │         ║
║  │  ✓ File Descriptor Table — cùng open files       │         ║
║  │  ✓ Signal handlers       — cùng xử lý signals   │         ║
║  │  ✓ PID                   — cùng 1 Process ID     │         ║
║  │  ✓ Address space         — cùng page table       │         ║
║  │  ✓ Working directory     — cùng thư mục         │         ║
║  │                                                   │         ║
║  │  → Ưu điểm: giao tiếp NHANH, trực tiếp!       │         ║
║  │  → Nhược: phải ĐỒNG BỘ khi ghi shared data!   │         ║
║  └─────────────────────────────────────────────────┘         ║
║                                                               ║
║  ┌─────────────────────────────────────────────────┐         ║
║  │  PRIVATE cho mỗi thread (SỞ HỮU RIÊNG):       │         ║
║  │                                                   │         ║
║  │  ✗ Stack               — mỗi thread stack riêng │         ║
║  │  ✗ Registers           — CPU state riêng         │         ║
║  │  ✗ Program Counter     — đang chạy dòng nào    │         ║
║  │  ✗ Thread ID (TID)     — ID riêng biệt          │         ║
║  │  ✗ Stack Pointer       — đỉnh stack riêng       │         ║
║  │  ✗ errno (trên Linux)  — error code riêng       │         ║
║  │  ✗ Signal mask         — block signals riêng     │         ║
║  │                                                   │         ║
║  │  → Mỗi thread = 1 "luồng thực thi" độc lập    │         ║
║  │  → Chạy function khác nhau, tốc độ khác nhau   │         ║
║  └─────────────────────────────────────────────────┘         ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Trực quan: Process vs Thread trong bộ nhớ

```
╔═══════════════════════════════════════════════════════════════╗
║   PROCESS vs THREAD — MEMORY VIEW                            ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  PROCESS A (PID 100)         PROCESS B (PID 200)             ║
║  ┌───────────────────┐       ┌───────────────────┐           ║
║  │ Code   ████████   │       │ Code   ████████   │           ║
║  │ Heap   ████████   │       │ Heap   ████████   │           ║
║  │ Files  [0,1,2,3]  │       │ Files  [0,1,2,5]  │           ║
║  │ Stack  ████████   │       │ Stack  ████████   │           ║
║  └───────────────────┘       └───────────────────┘           ║
║  ↑ Hoàn toàn cách ly ↑       ↑ Hoàn toàn cách ly ↑         ║
║                                                               ║
║  PROCESS C (PID 300) — Multi-threaded:                       ║
║  ┌─────────────────────────────────────────────┐             ║
║  │ Code    ████████████  ← SHARED (read-only)  │             ║
║  │ Heap    ████████████  ← SHARED (read-write) │             ║
║  │ Files   [0,1,2,3,4]  ← SHARED               │             ║
║  │ Globals ████████████  ← SHARED               │             ║
║  │                                               │             ║
║  │ ┌─────────┐ ┌─────────┐ ┌─────────┐         │             ║
║  │ │Thread 1 │ │Thread 2 │ │Thread 3 │         │             ║
║  │ │Stack    │ │Stack    │ │Stack    │         │             ║
║  │ │Registers│ │Registers│ │Registers│         │             ║
║  │ │TID: 301 │ │TID: 302 │ │TID: 303 │         │             ║
║  │ └─────────┘ └─────────┘ └─────────┘         │             ║
║  │  ↑ Riêng ↑   ↑ Riêng ↑   ↑ Riêng ↑          │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
║  SO SÁNH CHI TIẾT:                                           ║
║  ┌──────────────────┬──────────────┬──────────────┐          ║
║  │ Tiêu chí          │ Process      │ Thread       │          ║
║  ├──────────────────┼──────────────┼──────────────┤          ║
║  │ Address space    │ Riêng biệt  │ Chia sẻ      │          ║
║  │ Tạo mới (fork)  │ ~100μs       │ ~10μs        │          ║
║  │ Context switch   │ ~1-5μs       │ ~0.5-1μs     │          ║
║  │ Memory overhead  │ Nhiều (copy) │ Ít (chỉ stack)│         ║
║  │ Crash isolation  │ Process khác  │ CẢ process   │          ║
║  │                  │ không ảnh    │ chết theo!   │          ║
║  │ Communication    │ IPC (pipe,    │ Shared memory│          ║
║  │                  │ socket, shm) │ (trực tiếp)  │          ║
║  │ Stack size       │ 8MB default  │ 8MB default  │          ║
║  └──────────────────┴──────────────┴──────────────┘          ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Linux không phân biệt Thread và Process!

Đây là kiến thức hay mà ít người biết. Trên Linux kernel, **thread và process dùng cùng cấu trúc `task_struct`**. Không có "struct thread" riêng! Sự khác biệt duy nhất nằm ở **cờ (flags)** khi tạo:

```c
// Tạo process mới (fork) — Linux kernel:
clone(SIGCHLD, 0);
// → Child có address space RIÊNG (copy page table)
// → Child có file descriptor table RIÊNG

// Tạo thread mới (pthread_create) — Linux kernel:
clone(CLONE_VM | CLONE_FS | CLONE_FILES | CLONE_SIGHAND | CLONE_THREAD, 0);
// CLONE_VM:      SHARE address space (same page table!)
// CLONE_FS:      SHARE filesystem info (cwd, root)
// CLONE_FILES:   SHARE file descriptor table
// CLONE_SIGHAND: SHARE signal handlers
// CLONE_THREAD:  SHARE PID (cùng thread group)
```

Insight: Thread trên Linux chỉ là **process mà share nhiều thứ hơn**. Đây là triết lý thiết kế rất đẹp của Linux — thay vì tạo abstraction mới, Linux tái sử dụng process model và thêm flags cho phép share từng resource. Linus Torvalds gọi threads là _"processes that share the address space"_.

Hệ quả thực tế:

- `top` trên Linux hiển thị threads như processes riêng (dùng `top -H` để thấy)
- `/proc/PID/task/` liệt kê tất cả threads của process
- `ps -eLf` hiện threads cùng processes
- `htop` tô màu threads khác processes

```bash
# Go process với GOMAXPROCS=4 → tạo 4+ OS threads
$ ls /proc/$(pidof myserver)/task/
12345/  12346/  12347/  12348/  12349/
# 5 threads (main + 4 M threads + 1 sysmon)

# Mỗi thread có stack riêng
$ cat /proc/12345/task/12346/maps | grep stack
7f8c00000000-7f8c00800000 rw-p [stack:12346]
```

#### Ví dụ thực tế: Web server multi-threaded

```go
// Java-style: 1 thread per request (Tomcat model)
// Mỗi request handler chạy trên 1 OS thread riêng

// Khi 1000 requests đến cùng lúc:
// → 1000 OS threads được tạo/reuse từ thread pool
// → Mỗi thread: 1MB stack = 1GB RAM CHỈ CHO STACKS!
// → 1000 threads context switch = scheduler overhead!
// → Mỗi thread chờ DB query = thread blocked, CPU idle

// Go: 1 goroutine per request (net/http default)
func main() {
    http.HandleFunc("/api/users", func(w http.ResponseWriter, r *http.Request) {
        // Mỗi request = 1 goroutine (KHÔNG phải 1 OS thread!)
        // Goroutine: 2KB stack, ~0.3μs tạo mới

        users, err := db.Query("SELECT * FROM users")
        // Khi chờ DB: goroutine PARK (không block OS thread!)
        // Go scheduler: M thread pick goroutine khác → CPU không idle!

        json.NewEncoder(w).Encode(users)
    })
    http.ListenAndServe(":8080", nil)
}

// 1000 requests = 1000 goroutines
// = 2MB RAM cho stacks (vs 1GB với threads!)
// = 4-8 OS threads xử lý tất cả (GOMAXPROCS)
```

#### Cái giá của shared memory: Race Condition preview

Thread chia sẻ heap = thread có thể đọc/ghi **cùng 1 biến**. Điều này vừa là sức mạnh (giao tiếp nhanh), vừa là **nguồn gốc của mọi concurrent bugs**:

```go
// ❌ BUG: Race condition — 2 goroutines ghi cùng 1 biến
var counter int

func main() {
    for i := 0; i < 1000; i++ {
        go func() {
            counter++  // READ counter → ADD 1 → WRITE counter
            // Nếu 2 goroutines cùng READ "5", cùng WRITE "6"
            // → counter chỉ tăng 1 thay vì 2!
        }()
    }
    time.Sleep(time.Second)
    fmt.Println(counter) // Mong đợi: 1000, Thực tế: ~950-990!
}

// ✅ FIX 1: Mutex (xem §6)
var mu sync.Mutex
mu.Lock()
counter++
mu.Unlock()

// ✅ FIX 2: Atomic (xem §6)
var counter int64
atomic.AddInt64(&counter, 1)

// ✅ FIX 3: Channel — "Go way" (xem §6)
// "Don't communicate by sharing memory,
//  share memory by communicating."
```

```bash
# Detect race condition:
$ go run -race main.go
==================
WARNING: DATA RACE
Write at 0x00c0000b4010 by goroutine 7:
  main.main.func1()
      main.go:10 +0x38
Previous write at 0x00c0000b4010 by goroutine 6:
  main.main.func1()
      main.go:10 +0x38
==================
```

#### 5 Whys: Tại sao Thread tồn tại?

```
╔═══════════════════════════════════════════════════════════════╗
║   5 WHYS: TẠI SAO CẦN THREAD?                               ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  WHY 1: Tại sao cần thread?                                  ║
║  → Vì muốn làm nhiều việc cùng lúc trong 1 process.        ║
║                                                               ║
║  WHY 2: Tại sao không dùng multi-process?                    ║
║  → Vì process quá nặng: fork chậm, memory lớn, IPC phức tạp║
║                                                               ║
║  WHY 3: Tại sao thread nhẹ hơn process?                     ║
║  → Vì thread share address space → không cần copy memory!  ║
║  → Context switch rẻ hơn: không flush TLB!                 ║
║                                                               ║
║  WHY 4: Tại sao share memory lại là vấn đề?                ║
║  → Vì 2 threads ghi cùng data = UNDEFINED BEHAVIOR!        ║
║  → CPU instruction KHÔNG atomic (read-modify-write)         ║
║  → CPU cache coherency: mỗi core có cache riêng           ║
║                                                               ║
║  WHY 5: Giới hạn cuối cùng?                                  ║
║  → Thread vẫn là OS resource: mỗi thread = 1MB+ stack      ║
║  → Context switch qua kernel: ~1μs (chậm cho I/O-heavy)    ║
║  → Go giải quyết: goroutine = user-space thread, 2KB stack ║
║  → Trade-off: Go runtime phức tạp hơn, debug khó hơn       ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Thread Models: User-space vs Kernel-space

```
╔═══════════════════════════════════════════════════════════════╗
║   THREAD MODELS                                              ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  1:1 Model (Linux NPTL, Java):                               ║
║  ┌─────────────────────────────────────────────┐             ║
║  │  User Thread 1 ←→ Kernel Thread 1          │             ║
║  │  User Thread 2 ←→ Kernel Thread 2          │             ║
║  │  User Thread 3 ←→ Kernel Thread 3          │             ║
║  │                                               │             ║
║  │  ✓ Đơn giản, kernel quản lý scheduling     │             ║
║  │  ✓ True parallelism on multi-core           │             ║
║  │  ✗ Mỗi thread = syscall create → chậm      │             ║
║  │  ✗ Context switch qua kernel → overhead     │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
║  M:N Model (Go runtime, Erlang):                             ║
║  ┌─────────────────────────────────────────────┐             ║
║  │  Goroutine 1 ─┐                              │             ║
║  │  Goroutine 2 ─┼→ OS Thread 1 (M0)           │             ║
║  │  Goroutine 3 ─┘                              │             ║
║  │  Goroutine 4 ─┐                              │             ║
║  │  Goroutine 5 ─┼→ OS Thread 2 (M1)           │             ║
║  │  Goroutine 6 ─┘                              │             ║
║  │                                               │             ║
║  │  ✓ M goroutines trên N OS threads            │             ║
║  │  ✓ Goroutine switch: ~200ns (userspace!)     │             ║
║  │  ✓ 2KB stack, grow dynamic                   │             ║
║  │  ✓ 1M goroutines trên 4 threads = hoàn toàn OK!  │       ║
║  │  ✗ Complexity: Go runtime phải tự schedule   │             ║
║  │  ✗ Blocking syscall: phải detach P từ M     │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Thread Stack Deep Dive — Mỗi thread = 1 stack riêng

Thread stack là phần "riêng" quan trọng nhất — hiểu nó giúp debug **stack overflow**, **goroutine stack growth**, và **tại sao thread tốn RAM hơn goroutine**:

```
╔═══════════════════════════════════════════════════════════════╗
║   THREAD STACK — CHI TIẾT                                    ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  OS Thread Stack (Linux/Java/C):                             ║
║  ┌───────────────────────────────────┐                       ║
║  │         Guard Page (4KB)           │ ← READ-ONLY          ║
║  │  → Truy cập = SIGSEGV → crash!  │ ← Phát hiện overflow  ║
║  ├───────────────────────────────────┤ ← Stack Top (high)    ║
║  │                                     │                       ║
║  │         STACK (8MB mặc định)       │  ← FIXED SIZE!       ║
║  │                                     │                       ║
║  │  ┌───────────┐  ← Frame N (main)  │                       ║
║  │  │ local vars │                     │                       ║
║  │  │ return addr│                     │                       ║
║  │  ├───────────┤  ← Frame N-1       │                       ║
║  │  │ params     │    (handler)        │                       ║
║  │  │ local vars │                     │                       ║
║  │  │ return addr│                     │                       ║
║  │  ├───────────┤  ← Frame N-2       │                       ║
║  │  │ ...         │    (helper func)   │                       ║
║  │  │             │                     │                       ║
║  │  │             │  Stack grows DOWN ↓│                       ║
║  │  │             │                     │                       ║
║  │  │  (unused)   │ ← 90% thường      │                       ║
║  │  │             │    không dùng!     │                       ║
║  │  │             │                     │                       ║
║  ├───────────────────────────────────┤ ← Stack Bottom (low)  ║
║  │         Guard Page (4KB)           │ ← Phát hiện overflow  ║
║  └───────────────────────────────────┘                       ║
║                                                               ║
║  VẤN ĐỀ: 8MB cố định → lãng phí!                          ║
║  → 1000 threads = 1000 × 8MB = 8GB CHỈ CHO STACKS!       ║
║  → Phần lớn chỉ dùng vài KB → 99% bị lãng phí           ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  Goroutine Stack (Go):                                       ║
║  ┌───────────────────────────────────┐                       ║
║  │                                     │                       ║
║  │   Ban đầu: chỉ 2KB!              │                       ║
║  │   ┌───────────┐                    │                       ║
║  │   │ main()     │ ← 2KB đủ cho    │                       ║
║  │   │ frame      │   hầu hết cases  │                       ║
║  │   └───────────┘                    │                       ║
║  │                                     │                       ║
║  │   Khi cần thêm: GROW tự động!    │                       ║
║  │   ┌───────────┐                    │                       ║
║  │   │ main()     │ ← Allocate stack │                       ║
║  │   │ handler()  │   mới GẤP ĐÔI   │                       ║
║  │   │ helper()   │   (2KB→4KB→8KB)  │                       ║
║  │   │ ...         │                    │                       ║
║  │   └───────────┘                    │                       ║
║  │                                     │                       ║
║  │   Copy TẤT CẢ frames sang stack   │                       ║
║  │   mới, update TẤT CẢ pointers!   │                       ║
║  │   → Đây là "stack copying"        │                       ║
║  │   → Max grow: 1GB (runtime limit) │                       ║
║  │                                     │                       ║
║  │   1M goroutines × 2KB = 2GB!     │                       ║
║  │   (vs 1M threads × 8MB = 8TB!)   │                       ║
║  │                                     │                       ║
║  └───────────────────────────────────┘                       ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// Stack overflow demo — goroutine stack tự grow
func infiniteRecursion(depth int) {
    var buf [1024]byte // 1KB mỗi frame
    _ = buf
    if depth%10000 == 0 {
        fmt.Printf("Depth: %d, stack ~%d KB\n", depth, depth)
    }
    infiniteRecursion(depth + 1) // Stack grows...
}

// Khoảng depth=1,000,000 (dùng ~1GB stack) → panic:
// runtime: goroutine stack exceeds 1000000000-byte limit
// runtime: sp: 0xc0200e0370, stack: [0xc0100e0000, 0xc0500e0000]
// fatal error: stack overflow

// Thay đổi max stack size:
// import "runtime/debug"
// debug.SetMaxStack(2 * 1024 * 1024 * 1024) // 2GB
```

```bash
# Kiểm tra stack size mặc định của OS thread
$ ulimit -s
8192    ← 8MB (8192KB)

# Thay đổi OS thread stack size
$ ulimit -s 16384    ← 16MB

# Xem goroutine stack sizes
$ GODEBUG=gctrace=1 ./myserver 2>&1 | head -5
# gc 1 @0.012s 3%: ... stacks=2048->2048 ...
#                     ↑ stack memory in KB
```

#### POSIX Threads (pthread) — API tạo thread trên Unix

Trước khi hiểu goroutine, cần hiểu **pthread** — API chuẩn để tạo OS threads. Mọi threading library (Java threads, Python threads, Go M threads) đều gọi pthread bên dưới:

```c
// pthread_basic.c — Tạo thread đầy đủ
#include <pthread.h>
#include <stdio.h>
#include <string.h>

// Mỗi thread chạy function này
void* worker(void* arg) {
    int thread_num = *(int*)arg;
    printf("Thread %d: TID=%lu, đang xử lý...\n",
           thread_num, pthread_self());

    // Simulate work
    sleep(1);

    // Return value (hoặc NULL)
    int* result = malloc(sizeof(int));
    *result = thread_num * 100;
    return result;
}

int main() {
    pthread_t threads[4];  // Mảng thread handles
    int thread_args[4];

    // ① Tạo 4 threads
    for (int i = 0; i < 4; i++) {
        thread_args[i] = i;
        int err = pthread_create(
            &threads[i],     // Thread handle (output)
            NULL,            // Attributes (NULL = default: 8MB stack)
            worker,          // Function to run
            &thread_args[i]  // Argument
        );
        if (err != 0) {
            fprintf(stderr, "pthread_create failed: %s\n", strerror(err));
        }
    }
    // Tại đây: 5 threads đang chạy (main + 4 workers)

    // ② Join: chờ mỗi thread hoàn thành
    for (int i = 0; i < 4; i++) {
        void* result;
        pthread_join(threads[i], &result);  // Block cho đến thread exit
        printf("Thread %d returned: %d\n", i, *(int*)result);
        free(result);
    }
    // Tại đây: chỉ còn main thread

    return 0;
}

// Compile & run:
// $ gcc -pthread pthread_basic.c -o pthread_demo
// $ ./pthread_demo
// Thread 0: TID=140234, đang xử lý...
// Thread 1: TID=140235, đang xử lý...
// Thread 2: TID=140236, đang xử lý...
// Thread 3: TID=140237, đang xử lý...
// Thread 0 returned: 0
// Thread 1 returned: 100
// Thread 2 returned: 200
// Thread 3 returned: 300
```

```
╔═══════════════════════════════════════════════════════════════╗
║   PTHREAD API → GO MAPPING                                   ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ┌────────────────────┬─────────────────────────────────┐    ║
║  │ pthread (C)        │ Go equivalent                    │    ║
║  ├────────────────────┼─────────────────────────────────┤    ║
║  │ pthread_create()   │ go func() { ... }()             │    ║
║  │ pthread_join()     │ <-done  hoặc  wg.Wait()        │    ║
║  │ pthread_detach()   │ (không cần, GC tự dọn)         │    ║
║  │ pthread_mutex_*    │ sync.Mutex                       │    ║
║  │ pthread_cond_*     │ sync.Cond hoặc channel          │    ║
║  │ pthread_key_*      │ context.Context (goroutine-local)│    ║
║  │ pthread_cancel()   │ context.WithCancel()            │    ║
║  │ pthread_self()     │ runtime.NumGoroutine() (no ID!) │    ║
║  └────────────────────┴─────────────────────────────────┘    ║
║                                                               ║
║  Chú ý: Go KHÔNG có goroutine ID!                           ║
║  → Lý do: tránh thread-local storage pattern               ║
║  → "Goroutines should communicate, not identify"            ║
║  → Dùng context.Context thay vì goroutine ID               ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Thread-Local Storage (TLS) — Dữ liệu riêng cho mỗi thread

Có khi bạn muốn mỗi thread có **biến riêng** mà không cần lock (vì không ai khác truy cập). Đây là **Thread-Local Storage**:

```
╔═══════════════════════════════════════════════════════════════╗
║   THREAD-LOCAL STORAGE (TLS)                                 ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  VẤN ĐỀ:                                                    ║
║  → var errno int — global, 2 threads ghi cùng lúc = race!  ║
║  → Mỗi thread cần errno RIÊNG, nhưng cùng tên biến        ║
║                                                               ║
║  GIẢI PHÁP: Thread-Local Storage                             ║
║  → Mỗi thread có copy riêng của biến                        ║
║  → Cùng tên, nhưng GIÁ TRỊ KHÁC NHAU per thread           ║
║                                                               ║
║  ┌────────────────────────────────────────────┐              ║
║  │  Thread 1:  errno = 5  (EACCES)            │              ║
║  │  Thread 2:  errno = 2  (ENOENT)            │ ← Khác nhau!║
║  │  Thread 3:  errno = 0  (success)            │              ║
║  │                                              │              ║
║  │  Cùng tên "errno" nhưng mỗi thread đọc     │              ║
║  │  giá trị RIÊNG → KHÔNG cần lock!           │              ║
║  └────────────────────────────────────────────┘              ║
║                                                               ║
║  C:    __thread int errno;  (GCC extension)                  ║
║  Java: ThreadLocal<Integer> errno = new ThreadLocal<>();    ║
║  Go:   KHÔNG CÓ goroutine-local storage!                    ║
║        → Dùng context.Context thay thế                      ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// Go cách xử lý "goroutine-local" data: context.Context
func handleRequest(ctx context.Context) {
    // Mỗi request = 1 goroutine, mỗi goroutine mang context riêng
    userID := ctx.Value("user_id").(string)  // "goroutine-local"
    traceID := ctx.Value("trace_id").(string)

    // Truyền context xuống mọi function call
    result, err := queryDB(ctx, userID)  // ctx mang deadline, cancel, values
    if err != nil {
        log.Printf("[%s] Error for user %s: %v", traceID, userID, err)
    }
}

// Middleware gắn values vào context
func authMiddleware(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        ctx := r.Context()
        ctx = context.WithValue(ctx, "user_id", getUserFromToken(r))
        ctx = context.WithValue(ctx, "trace_id", generateTraceID())
        next.ServeHTTP(w, r.WithContext(ctx))
    })
}

// Tại sao Go KHÔNG có goroutine ID?
// 1. ThreadLocal pattern tạo "hidden global state" → khó test
// 2. "Ai cleanup TLS khi goroutine exit?" → memory leak
// 3. context.Context BẮT BUỘC truyền EXPLICITLY → code dễ đọc hơn
// 4. Rob Pike: "Goroutine ID is a bad design pattern"
```

#### Deadlock — Khi threads chờ nhau mãi mãi

**Deadlock** là tình trạng 2+ threads **chờ lẫn nhau mãi mãi**, không ai tiến được. Đây là bug kinh điển trong concurrent programming:

```
╔═══════════════════════════════════════════════════════════════╗
║   DEADLOCK — THREADS CHỜ NHAU MÃI MÃI                      ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Analogy: 2 NGƯỜI đứng 2 đầu CẦU HẸP                      ║
║  → Cả 2 đều muốn qua, không ai chịu lùi                   ║
║  → KẸT MÃI MÃI!                                            ║
║                                                               ║
║  ┌─────────────┐                       ┌─────────────┐      ║
║  │  Thread A    │                       │  Thread B    │      ║
║  │              │                       │              │      ║
║  │ Lock(Mutex1) │  ← A giữ Mutex1     │ Lock(Mutex2) │      ║
║  │  ✓ Got it!  │                       │  ✓ Got it!  │      ║
║  │              │                       │              │      ║
║  │ Lock(Mutex2) │  ← A chờ Mutex2     │ Lock(Mutex1) │      ║
║  │  ⏳ waiting..│  nhưng B đang giữ!  │  ⏳ waiting..│      ║
║  │              │                       │              │      ║
║  │  → DEADLOCK! ← ← ← ← ← ← ← ← ← │  → DEADLOCK!│      ║
║  │              │  A chờ B             │              │      ║
║  │              │  B chờ A              │              │      ║
║  │              │  = KHÔNG AI tiến!    │              │      ║
║  └─────────────┘                       └─────────────┘      ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  4 ĐIỀU KIỆN COFFMAN (deadlock xảy ra khi CẢ 4 đúng):     ║
║                                                               ║
║  ① Mutual Exclusion: resource chỉ 1 thread giữ tại 1 thời  ║
║     điểm (mutex by definition)                                ║
║                                                               ║
║  ② Hold and Wait: thread GIỮ resource và CHỜ resource khác  ║
║     (A giữ Mutex1, chờ Mutex2)                               ║
║                                                               ║
║  ③ No Preemption: KHÔNG AI cướp resource đang bị giữ        ║
║     (OS không tự unlock mutex)                                ║
║                                                               ║
║  ④ Circular Wait: A chờ B, B chờ A (chu trình chờ)          ║
║                                                               ║
║  → Phá bất kỳ 1 điều kiện = KHÔNG deadlock!                ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  PHÒNG CHỐNG:                                                 ║
║  ┌──────────────────────────────────────────────┐            ║
║  │ ① Lock ordering: LUÔN lock theo thứ tự cố định│           ║
║  │    → Mutex1 trước, Mutex2 sau (CẢ A và B)    │            ║
║  │    → Phá điều kiện ④ (Circular Wait)         │            ║
║  │                                                │            ║
║  │ ② Timeout: Lock với deadline                   │            ║
║  │    → Nếu không lock được trong 5s → bỏ cuộc  │            ║
║  │    → Go: select + time.After                   │            ║
║  │                                                │            ║
║  │ ③ Try-lock: Thử lock, nếu fail → release all │            ║
║  │    → sync.Mutex KHÔNG có TryLock              │            ║
║  │    → Dùng channel-based lock thay thế         │            ║
║  │                                                │            ║
║  │ ④ Avoid nested locks: Giảm số locks cần giữ  │            ║
║  │    → Thiết kế data structures tránh lock chồng │            ║
║  │    → "Share memory by communicating" (channels)│            ║
║  └──────────────────────────────────────────────┘            ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// ❌ DEADLOCK trong Go
func deadlockDemo() {
    var mu1, mu2 sync.Mutex

    // Goroutine A: lock mu1 → lock mu2
    go func() {
        mu1.Lock()
        time.Sleep(time.Millisecond) // Tạo window cho deadlock
        mu2.Lock() // ← CHỜ mu2 (B đang giữ!)
        defer mu1.Unlock()
        defer mu2.Unlock()
    }()

    // Goroutine B: lock mu2 → lock mu1 (THỨ TỰ NGƯỢC!)
    go func() {
        mu2.Lock()
        time.Sleep(time.Millisecond)
        mu1.Lock() // ← CHỜ mu1 (A đang giữ!) → DEADLOCK!
        defer mu2.Unlock()
        defer mu1.Unlock()
    }()

    time.Sleep(5 * time.Second) // Chương trình treo mãi!
}

// ✅ FIX: Lock ordering — LUÔN lock mu1 trước mu2
func fixedVersion() {
    var mu1, mu2 sync.Mutex

    lockBoth := func() {
        mu1.Lock()  // LUÔN mu1 trước
        mu2.Lock()  // LUÔN mu2 sau
    }
    unlockBoth := func() {
        mu2.Unlock()
        mu1.Unlock()
    }

    go func() {
        lockBoth()
        defer unlockBoth()
        // critical section
    }()
    go func() {
        lockBoth()   // Cùng thứ tự! Không deadlock!
        defer unlockBoth()
        // critical section
    }()
}

// Go runtime detect deadlock đơn giản:
// Nếu TẤT CẢ goroutines đều blocked → panic:
// fatal error: all goroutines are asleep - deadlock!
// Nhưng KHÔNG detect partial deadlock (chỉ 1 số goroutines stuck)
```

#### Thread Pool Pattern — Tái sử dụng threads

Tạo mới thread cho mỗi request rất tốn kém (~100μs + 8MB stack). **Thread pool** tạo sẵn N threads, tái sử dụng cho mỗi task:

```
╔═══════════════════════════════════════════════════════════════╗
║   THREAD POOL PATTERN                                        ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  KHÔNG có pool:                                              ║
║  Request 1 → create thread → work → destroy thread          ║
║  Request 2 → create thread → work → destroy thread          ║
║  Request 3 → create thread → work → destroy thread          ║
║  → Tốn 100μs × 3 = 300μs chỉ để create/destroy!          ║
║                                                               ║
║  CÓ pool (N=3):                                              ║
║  ┌──────────┐     ┌───────────────┐     ┌──────────────┐    ║
║  │ Incoming  │     │  Work Queue   │     │ Thread Pool  │    ║
║  │ Requests  │────►│  ┌─┬─┬─┬─┐  │────►│ ┌──┐ ┌──┐    │    ║
║  │           │     │  │R│R│R│R│  │     │ │T1│ │T2│    │    ║
║  │ R1,R2,R3  │     │  └─┴─┴─┴─┘  │     │ └──┘ └──┘    │    ║
║  │ R4,R5,R6  │     │   FIFO queue  │     │ ┌──┐         │    ║
║  │ ...        │     │              │     │ │T3│ idle     │    ║
║  └──────────┘     └───────────────┘     │ └──┘         │    ║
║                                          └──────────────┘    ║
║                                                               ║
║  → Threads KHÔNG bị destroy, chờ task tiếp theo            ║
║  → Amortize creation cost!                                   ║
║  → Giới hạn max threads → không OOM                        ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  REAL-WORLD THREAD POOLS:                                    ║
║  ┌────────────────────┬──────────────────────────────┐      ║
║  │ Technology          │ Pool Config                  │      ║
║  ├────────────────────┼──────────────────────────────┤      ║
║  │ Java Tomcat         │ maxThreads=200 (default)    │      ║
║  │ Java ExecutorService│ Executors.newFixedThreadPool │      ║
║  │ Nginx               │ worker_processes=auto       │      ║
║  │ PostgreSQL          │ max_connections=100          │      ║
║  │ MySQL               │ thread_cache_size=16         │      ║
║  │ Go                  │ KHÔNG CẦN thread pool!      │      ║
║  │                    │ Goroutines quá rẻ để pool!  │      ║
║  └────────────────────┴──────────────────────────────┘      ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// Go Worker Pool — dùng goroutines thay vì threads
// (Dù goroutine rẻ, worker pool vẫn hữu ích để GIỚI HẠN concurrency)

func workerPool() {
    const numWorkers = 10
    jobs := make(chan Job, 100)      // Buffered channel = work queue
    results := make(chan Result, 100)

    // ① Tạo pool workers (goroutines, KHÔNG phải OS threads!)
    var wg sync.WaitGroup
    for i := 0; i < numWorkers; i++ {
        wg.Add(1)
        go func(workerID int) {
            defer wg.Done()
            for job := range jobs {  // Chờ job từ queue
                result := process(job)
                results <- result
            }
        }(i)
    }

    // ② Gửi jobs
    go func() {
        for _, job := range allJobs {
            jobs <- job  // Nếu queue đầy → block cho đến worker rảnh
        }
        close(jobs) // Báo hiệu hết jobs
    }()

    // ③ Thu kết quả
    go func() {
        wg.Wait()
        close(results)
    }()

    for result := range results {
        fmt.Println(result)
    }
}

// Tại sao Go vẫn dùng worker pool dù goroutine rẻ?
// → Giới hạn concurrent DB connections (database/sql.DB.SetMaxOpenConns)
// → Giới hạn concurrent HTTP requests ra ngoài (semaphore pattern)
// → Rate limiting: không muốn overwhelm downstream services
// → Backpressure: buffered channel = natural queue
```

#### Go GMP Model Preview — Tại sao Goroutine "thần thánh"?

Đây là kiến thức nền tảng để hiểu tại sao Go xử lý concurrency tốt hơn Java/C. GMP là **Go's M:N scheduler** — sẽ nói chi tiết ở §2, nhưng preview ở đây vì nó liên quan trực tiếp đến threads:

```
╔═══════════════════════════════════════════════════════════════╗
║   GO GMP MODEL — GOROUTINE SCHEDULER                         ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  G = Goroutine (user-space "thread", 2KB stack)              ║
║  M = Machine (OS thread, kernel-scheduled)                   ║
║  P = Processor (logical CPU, run queue holder)               ║
║                                                               ║
║  ┌─────────────────────────────────────────────────────┐     ║
║  │                                                       │     ║
║  │   P0                    P1                            │     ║
║  │   ┌──────────────┐    ┌──────────────┐               │     ║
║  │   │ Local Run Q:  │    │ Local Run Q:  │               │     ║
║  │   │ [G1][G2][G3]  │    │ [G4][G5]      │               │     ║
║  │   └──────┬───────┘    └──────┬───────┘               │     ║
║  │          │                    │                         │     ║
║  │          ▼                    ▼                         │     ║
║  │   ┌──────────┐         ┌──────────┐                   │     ║
║  │   │  M0       │         │  M1       │                   │     ║
║  │   │ (thread)  │         │ (thread)  │                   │     ║
║  │   │ running:  │         │ running:  │                   │     ║
║  │   │   G1 ★    │         │   G4 ★    │                   │     ║
║  │   └──────────┘         └──────────┘                   │     ║
║  │       ║                     ║                           │     ║
║  │       ║ OS scheduled        ║ OS scheduled              │     ║
║  │       ▼                     ▼                           │     ║
║  │   ┌──────────┐         ┌──────────┐                   │     ║
║  │   │  CPU 0    │         │  CPU 1    │                   │     ║
║  │   └──────────┘         └──────────┘                   │     ║
║  │                                                       │     ║
║  │   Global Run Queue: [G6][G7][G8]                      │     ║
║  │   → Goroutines chưa gán cho P nào                    │     ║
║  │                                                       │     ║
║  └─────────────────────────────────────────────────────┘     ║
║                                                               ║
║  SCHEDULING FLOW:                                            ║
║                                                               ║
║  ① G1 gọi net.Read() → syscall → G1 PARK                  ║
║  ② P0 lấy G2 từ local queue → M0 chạy G2                  ║
║  ③ G1 data sẵn sàng → G1 đưa lại vào queue               ║
║  ④ Nếu P0 hết queue → STEAL từ P1 (work stealing!)        ║
║                                                               ║
║  TẠI SAO NHANH:                                              ║
║  → Goroutine switch: ~200ns (user-space, không syscall!)    ║
║  → Thread switch: ~1-5μs (kernel, save/restore, TLB flush) ║
║  → Process switch: ~5-10μs (kernel, TLB flush, cache cold) ║
║                                                               ║
║  → 1M goroutines trên 8 threads → hoàn toàn OK!           ║
║  → 1M threads → OS sập ngay! (RAM + scheduling overhead)  ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```go
// Xem GMP runtime info
package main

import (
    "fmt"
    "runtime"
)

func main() {
    // G = goroutines
    fmt.Printf("Goroutines: %d\n", runtime.NumGoroutine())

    // M = OS threads (không có API trực tiếp)
    // Nhưng có thể thấy qua /proc/PID/task/

    // P = processors (logical CPUs)
    fmt.Printf("GOMAXPROCS (P): %d\n", runtime.GOMAXPROCS(0))
    fmt.Printf("NumCPU: %d\n", runtime.NumCPU())

    // Trace GMP scheduling:
    // $ GODEBUG=schedtrace=1000 ./myserver
    // SCHED 1000ms: gomaxprocs=8 idleprocs=6 threads=10
    //   idlethreads=4 runqueue=0 [0 0 0 2 0 0 0 0]
    //
    // → gomaxprocs=8: 8 P (processors)
    // → idleprocs=6: 6 P đang rảnh (chỉ 2 P đang bận)
    // → threads=10: 10 M (OS threads) đã tạo
    // → runqueue=0: global queue rỗng
    // → [0 0 0 2 0 0 0 0]: local queues (P3 có 2 goroutines chờ)
}
```

### 1.4 Context Switch — Cái Giá Của Đa Nhiệm

#### Hiểu từ zero: "Context" nghĩa là gì?

Trước khi nói "context switch", hãy hiểu **"context"** là gì đã.

Tưởng tượng bạn đang đọc **cuốn sách 500 trang**. Bạn đọc đến trang 237, đoạn giữa, và đang nghĩ: "À, nhân vật A vừa nói câu đó vì lý do X...". Tại thời điểm này, **context** của bạn là:

- **Đang ở trang nào**: trang 237 → (giống **Program Counter** — CPU đang thực thi instruction nào)
- **Đang nghĩ gì**: "nhân vật A, lý do X" → (giống **Registers** — dữ liệu tạm trong CPU)
- **Đánh dấu bookmark ở đâu**: giữa đoạn 3 → (giống **Stack Pointer** — đang ở đâu trong call stack)
- **Sách nào đang mở**: "Sapiens" → (giống **CR3/Page Table** — address space nào đang active)

Bây giờ, ai đó bảo bạn: **"Ngưng đọc Sapiens, chuyển sang đọc Clean Code!"**

Bạn PHẢI:

1. **Ghi lại** trang 237, đoạn 3, và ý nghĩ đang dang dở → **SAVE context**
2. **Cất sách** Sapiens đi → **Unload current state**
3. **Lấy sách** Clean Code, mở lại trang đã bookmark lần trước → **LOAD new context**
4. **Đọc lại vài dòng** trước trang bookmark để "nhớ lại" đang đọc gì → **Cache warm-up**

Toàn bộ quá trình "chuyển sách" này = **Context Switch**. Trong suốt thời gian đó, **bạn không đọc được trang nào hết** — đó là overhead!

#### Context = trạng thái CPU tại 1 thời điểm

Với CPU, "context" của một process gồm chính xác:

```
╔═══════════════════════════════════════════════════════════════╗
║   "CONTEXT" CỦA 1 PROCESS = TẤT CẢ NHỮNG GÌ CPU CẦN      ║
║   ĐỂ TIẾP TỤC CHẠY PROCESS ĐÓ                             ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ① Program Counter (RIP):                                    ║
║     → "CPU đang chạy instruction nào?"                      ║
║     → Ví dụ: 0x401020 = dòng 42 trong hàm main()           ║
║                                                               ║
║  ② Stack Pointer (RSP):                                      ║
║     → "Đỉnh stack ở đâu?"                                   ║
║     → Ví dụ: 0x7fff8000 = đang trong hàm processOrder()    ║
║                                                               ║
║  ③ General Registers (RAX, RBX, RCX... 16 cái):             ║
║     → "Kết quả tính toán tạm thời"                         ║
║     → Ví dụ: RAX = 42 (đang tính sum = a + b, kết quả = 42)║
║                                                               ║
║  ④ Flags Register (RFLAGS):                                  ║
║     → "Kết quả so sánh gần nhất"                           ║
║     → Ví dụ: Zero Flag = 1 → phép so sánh trước = bằng    ║
║                                                               ║
║  ⑤ Page Table Pointer (CR3):                                 ║
║     → "Process nào đang chạy? Bộ nhớ nào?"                 ║
║     → Mỗi process có CR3 riêng → address space riêng       ║
║                                                               ║
║  ⑥ FPU/SSE/AVX Registers (nếu dùng floating point):         ║
║     → 16 XMM registers, mỗi cái 256 bits                    ║
║     → Tính toán số thực, encryption, SIMD...                ║
║                                                               ║
║  Tổng cộng: ~1KB data phải save/restore mỗi lần switch!    ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Nhìn thấy Context Switch trên timeline

Đây là điều quan trọng nhất để hiểu: **CPU chỉ có 1 "bộ não"** (1 core). Nó chỉ chạy được **1 process tại 1 thời điểm**. Để "chạy 2 cái cùng lúc", CPU phải **luân phiên** rất nhanh:

```
╔═══════════════════════════════════════════════════════════════╗
║   TIMELINE: CPU CHẠY 2 PROCESSES TRÊN 1 CORE                ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Thời gian (ms):                                              ║
║  0     4     5     9     10    14    15    19    20            ║
║  │     │     │     │     │     │     │     │     │             ║
║  ├─────┤─────├─────┤─────├─────┤─────├─────┤─────┤            ║
║  │  A  │SWIT-│  B  │SWIT-│  A  │SWIT-│  B  │SWIT-│           ║
║  │chạy │ CH  │chạy │ CH  │chạy │ CH  │chạy │ CH  │           ║
║  │4ms  │1ms  │4ms  │1ms  │4ms  │1ms  │4ms  │1ms  │           ║
║  ├─────┤─────├─────┤─────├─────┤─────├─────┤─────┤            ║
║                                                               ║
║  Phân tích 20ms:                                              ║
║  → Process A chạy thực sự:  4+4    = 8ms   (40%)            ║
║  → Process B chạy thực sự:  4+4    = 8ms   (40%)            ║
║  → Context switch overhead:  1×4    = 4ms   (20%!)           ║
║  → 20% thời gian CPU = LÃNG PHÍ chỉ để chuyển đổi!       ║
║                                                               ║
║  Nếu switch chi phí giảm xuống 0.01ms (goroutine):          ║
║  → Overhead: 0.01×4 = 0.04ms (0.2%) → gần như FREE!       ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Nhìn vào timeline, bạn thấy rõ: mỗi lần "SWITCH", CPU **dừng hoàn toàn** — không phục vụ A, cũng không phục vụ B. Nó chỉ đang "dọn dẹp" (save state A, load state B). Nếu chi phí mỗi lần switch = 1ms và switch 1000 lần/giây → **CPU mất 100% thời gian chỉ để switch** → không làm gì hữu ích!

Đây là lý do **Go tạo ra goroutine**: goroutine switch chỉ tốn ~0.0002ms (200 nanosecond), rẻ hơn process switch **5000 lần**. Server Go có thể switch hàng triệu lần/giây mà overhead gần bằng 0.

#### Nếu không có Context Switch thì sao?

```
╔═══════════════════════════════════════════════════════════════╗
║   KHÔNG CÓ CONTEXT SWITCH → THẢM HỌA!                      ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Scenario: máy tính chỉ có 1 CPU core                       ║
║                                                               ║
║  KHÔNG có context switch:                                     ║
║  ┌─────────────────────────────────────────────┐             ║
║  │  Process A (Chrome) chạy mãi mãi...         │             ║
║  │  Process B (VS Code) → ĐỨNG YÊN!           │             ║
║  │  Process C (Terminal) → ĐỨNG YÊN!          │             ║
║  │  Process D (Spotify) → ĐỨNG YÊN!           │             ║
║  │                                               │             ║
║  │  → Bạn mở Chrome → KHÔNG thể dùng VS Code! │             ║
║  │  → Phải TẮT Chrome → mới dùng được VS Code │             ║
║  │  → Giống máy tính thập niên 1960!           │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
║  CÓ context switch (hiện đại):                                ║
║  ┌─────────────────────────────────────────────┐             ║
║  │  CPU luân phiên: A→B→C→D→A→B→C→D→...      │             ║
║  │  Mỗi process chạy 4-10ms rồi nhường       │             ║
║  │  → Switch ~250 lần/giây                     │             ║
║  │  → Mắt người: thấy 4 apps chạy "cùng lúc" │             ║
║  │  → Thực tế: chỉ 1 chạy tại 1 thời điểm!  │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
║  Multi-core (máy tính hiện đại):                              ║
║  ┌─────────────────────────────────────────────┐             ║
║  │  Core 0: A → B → A → B →...                │             ║
║  │  Core 1: C → D → C → D →...                │             ║
║  │                                               │             ║
║  │  → 4 apps trên 2 cores = ÍT switch hơn!    │             ║
║  │  → 4 cores = mỗi app 1 core = KHÔNG switch! │             ║
║  │  → Nhưng apps luôn > cores → vẫn cần switch│             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Analogy chi tiết: Đầu bếp trong nhà hàng

> Tưởng tượng bạn là **đầu bếp duy nhất** trong nhà hàng. Bạn đang nấu phở (Process A), bỗng quản lý bảo "chuyển sang nấu bún bò" (Process B). Bạn phải:
>
> 1. **Đánh dấu**: phở đang ở bước nào, lửa bao nhiêu, gia vị đã bỏ gì (SAVE registers)
> 2. **Dọn bàn**: cất nguyên liệu phở sang 1 bên (FLUSH cache — TLB flush)
> 3. **Chuẩn bị**: lấy nguyên liệu bún bò ra, đọc lại ghi chú lần trước (RESTORE state)
> 4. **Tìm lại cảm giác**: bắt đầu nấu bún bò nhưng tay chân còn "nhớ" thao tác nấu phở, phải lỡ tay vài lần mới quen (CACHE COLD — cache miss!)
>
> Thời gian dọn dẹp + chuẩn bị = **không ai được ăn gì hết!** Đó là context switch cost.
>
> **So sánh đầu bếp**:
>
> - **Process switch** = chuyển từ phở sang pizza: phải thay TOÀN BỘ nguyên liệu, dao thớt, lò nướng vs bếp gas — rất tốn thời gian!
> - **Thread switch** = chuyển từ phở gà sang phở bò: cùng bếp, cùng nồi nước dùng, chỉ đổi thịt — nhanh hơn!
> - **Goroutine switch** = chuyển từ tô phở này sang tô phở kia: cùng nồi nước dùng, cùng rổ bánh phở, chỉ đổi tô — **gần như không mất thời gian!**

#### Khi nào Context Switch xảy ra?

Context switch không xảy ra ngẫu nhiên. Có 3 nguyên nhân chính:

**① Timer Interrupt (Preemptive scheduling)**: CPU có hardware timer. Cứ mỗi ~4-10ms, timer phát ra **interrupt**, CPU tạm dừng process hiện tại và nhảy vào kernel. Kernel scheduler quyết định: tiếp tục process này, hay chuyển sang process khác? Đây là cách Linux đảm bảo không process nào monopolize CPU.

**② I/O Wait (Voluntary)**: Khi process gọi `read()` để đọc từ disk/network, mà data chưa sẵn sàng, process **tự nguyện** nhường CPU. Kernel chuyển process sang trạng thái SLEEPING và schedule process khác. Khi data arrive, kernel "đánh thức" process ngủ và đặt lại vào run queue.

**③ Higher Priority Process (Preemption)**: Nếu process priority cao hơn vừa sẵn sàng (ví dụ: real-time process), kernel có thể **cưỡng chế** preempt process đang chạy, ngay cả khi chưa hết time slice.

```go
// Trong Go, context switch GIỮA goroutines xảy ra khi:
func handler(w http.ResponseWriter, r *http.Request) {
    // ① I/O call: goroutine park → switch sang goroutine khác
    data, _ := ioutil.ReadAll(r.Body)

    // ② Channel operation: goroutine park nếu channel chưa ready
    result := <-ch

    // ③ Function call: Go 1.14+ chèn preemption check
    // Mỗi function entry, Go kiểm tra "nên yield không?"
    compute(data)

    // ④ runtime.Gosched(): tự nguyện yield
    runtime.Gosched()

    // ⑤ Sync operations: mutex.Lock() nếu lock bị giữ
    mu.Lock()
    defer mu.Unlock()
}
```

#### Bên trong CPU: chuyện gì xảy ra từng nanosecond?

```
╔═══════════════════════════════════════════════════════════════╗
║   CONTEXT SWITCH — WHAT REALLY HAPPENS (DEEP DIVE)          ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Process A đang chạy → Timer interrupt! → Switch to B       ║
║                                                               ║
║  ┌─────────────────────────────────────────────────┐         ║
║  │ 1. SAVE Process A state:              ~0.5μs   │         ║
║  │    ├── Save tất cả CPU registers (16 GPR)      │         ║
║  │    ├── Save Program Counter (RIP)               │         ║
║  │    ├── Save Stack Pointer (RSP)                 │         ║
║  │    ├── Save FPU/SSE/AVX registers               │         ║
║  │    └── Save vào PCB của Process A              │         ║
║  │                                                   │         ║
║  │ 2. SCHEDULER quyết định:              ~0.2μs   │         ║
║  │    ├── Chọn process nào sẽ chạy tiếp?         │         ║
║  │    └── Dựa trên priority, fairness, timeslice  │         ║
║  │                                                   │         ║
║  │ 3. SWITCH memory context:             ~0.5-2μs  │         ║
║  │    ├── Load Page Table Base (CR3) → Process B   │         ║
║  │    ├── TLB FLUSH! (Translation Lookaside Buffer)│         ║
║  │    │   → TẤT CẢ cached translations mất!      │         ║
║  │    │   → Phải page walk lại: ~100 cycles/miss  │         ║
║  │    └── Thread switch: KHÔNG cần flush TLB!      │         ║
║  │                                                   │         ║
║  │ 4. RESTORE Process B state:           ~0.5μs   │         ║
║  │    ├── Load CPU registers từ PCB của B         │         ║
║  │    ├── Load Program Counter                     │         ║
║  │    └── Resume execution!                        │         ║
║  │                                                   │         ║
║  │ 5. CACHE POLLUTION (hidden cost):     ~5-50μs! │         ║
║  │    ├── L1/L2 cache: data của A vẫn ở đây     │         ║
║  │    │   nhưng B cần data KHÁC → cache miss!     │         ║
║  │    ├── Cache warm-up: phải load data B vào     │         ║
║  │    └── → Đây là chi phí LỚN NHẤT!            │         ║
║  └─────────────────────────────────────────────────┘         ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Giải thích từng bước:

**Bước 1 — SAVE state**: CPU phải lưu **toàn bộ trạng thái** của Process A vào PCB. Trên x86-64, đó là 16 general-purpose registers (RAX, RBX, RCX, RDX, RSI, RDI, RBP, RSP, R8-R15), Program Counter (RIP), Flags (RFLAGS), và nếu process dùng floating-point thì còn 16 XMM/YMM registers (mỗi cái 256 bits!). Tổng cộng khoảng **~1KB data** phải save.

**Bước 2 — Scheduler decision**: Kernel chạy scheduling algorithm (CFS trên Linux) để chọn process tiếp theo. CFS dùng red-black tree, lookup O(1) vì cache leftmost node → rất nhanh (~200ns).

**Bước 3 — Memory context switch (ĐẮT NHẤT cho process switch)**: Đây là bước khiến process switch đắt hơn thread switch RẤT NHIỀU. Khi switch sang process khác, CPU phải load **CR3 register** mới (page table base address), và điều này trigger **TLB flush**.

**TLB (Translation Lookaside Buffer)** là cache 64-1024 entries trong CPU, lưu ánh xạ `virtual address → physical address`. Mỗi lần truy cập memory, CPU check TLB trước (1 cycle) — nếu miss, phải **page table walk** (hàng trăm cycles!). Khi flush TLB, TẤT CẢ entries bị xóa → những memory access đầu tiên sau switch đều bị miss → rất chậm!

> ⚠️ Thread switch trong cùng process **KHÔNG flush TLB** vì threads share page table (cùng CR3)! Đây là lý do thread switch rẻ hơn process switch.

**Bước 4 — RESTORE state**: Load tất cả registers đã lưu của Process B. Sau bước này, CPU "nghĩ" rằng nó vẫn đang chạy B liên tục — B không hề biết mình đã bị tạm dừng!

**Bước 5 — Cache pollution (CHI PHÍ ẨN LỚN NHẤT!)**: Đây là phần hầu hết sách giáo khoa KHÔNG đề cập. Sau khi switch xong, L1/L2/L3 cache vẫn chứa data của Process A. Process B cần data hoàn toàn KHÁC → liên tục **cache miss** → phải fetch data từ RAM (chậm hơn ~100×). Phải mất ~5-50μs để cache "warm up" cho Process B.

#### So sánh chi phí: Process vs Thread vs Goroutine

```
╔═══════════════════════════════════════════════════════════════╗
║   CONTEXT SWITCH COST COMPARISON                             ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║           Process        Thread        Goroutine              ║
║  ┌──────────────────────────────────────────────────┐        ║
║  │ Save/Load   ~0.5μs       ~0.5μs       ~50ns     │        ║
║  │ registers                                         │        ║
║  │                                                    │        ║
║  │ TLB flush   ~1-2μs       ✗ KHÔNG!     ✗ KHÔNG!  │        ║
║  │             (phải flush)  (cùng page   (cùng      │        ║
║  │                           table)       process)   │        ║
║  │                                                    │        ║
║  │ Scheduler   ~0.2μs       ~0.2μs       ~50ns      │        ║
║  │ decision    (kernel CFS) (kernel CFS) (Go runtime)│        ║
║  │                                                    │        ║
║  │ Privilege   Ring3→0→3    Ring3→0→3    ✗ KHÔNG!   │        ║
║  │ change      (trap vào    (trap vào    (userspace   │        ║
║  │             kernel)      kernel)      only!)       │        ║
║  │                                                    │        ║
║  │ Cache       ~5-50μs      ~2-10μs      ~0μs       │        ║
║  │ pollution   (hoàn toàn   (partial     (cùng       │        ║
║  │             cold)         warm)        working     │        ║
║  │                                        set!)       │        ║
║  ├──────────────────────────────────────────────────┤        ║
║  │ TỔNG CỘNG: ~10-55μs     ~3-11μs      ~0.1-0.3μs│        ║
║  │             = 10,000ns   = 3,000ns    = 200ns!!  │        ║
║  └──────────────────────────────────────────────────┘        ║
║                                                               ║
║  Goroutine nhanh hơn process: ~100-500×!                     ║
║  Goroutine nhanh hơn thread:  ~15-50×!                       ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

**Tại sao goroutine switch rẻ đến vậy?**

1. **Không cần kernel**: goroutine switch hoàn toàn trong user space. Không trap vào kernel = không có Ring 3 → Ring 0 → Ring 3 overhead (~1μs mỗi lần).
2. **Không flush TLB**: tất cả goroutines chung 1 process → chung page table → TLB vẫn valid.
3. **Cache vẫn warm**: goroutines chung address space → data trên cache vẫn hữu ích. Goroutine A và B có thể share cùng data structures → cache hit thay vì miss.
4. **Ít registers**: Go chỉ save/restore ~13 registers (thay vì ~30+ cho full process context). Go calling convention đã thiết kế để minimize registers cần save.

```go
// Go goroutine switch internals (simplified):
// Source: runtime/proc.go → schedule()

func schedule() {
    gp := findRunnable()  // Tìm goroutine sẵn sàng chạy
    // 1. Save current goroutine's SP, PC vào g.sched
    // 2. Load new goroutine's SP, PC từ gp.sched
    // 3. JMP đến gp.sched.pc → tiếp tục chạy!
    // Tổng cộng: ~50 instructions = ~200ns
    execute(gp)
}
```

#### Tác động thực tế lên Production

```
╔═══════════════════════════════════════════════════════════════╗
║   CONTEXT SWITCH & PRODUCTION PERFORMANCE                    ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Scenario: Go HTTP server, 10K concurrent connections         ║
║                                                               ║
║  Java Tomcat (1 thread/connection):                          ║
║  → 10K threads, mỗi thread chờ DB I/O                       ║
║  → 10,000 context switches/sec (scheduler chạy liên tục)    ║
║  → 10,000 × 10μs = 100ms/sec → 10% CPU chỉ để switch!    ║
║  → Cache thrashing: mỗi thread có working set riêng        ║
║  → L3 cache 30MB / 10K threads = 3KB/thread → cache miss!  ║
║                                                               ║
║  Go (goroutines):                                            ║
║  → 10K goroutines chạy trên 8 OS threads (GOMAXPROCS=8)     ║
║  → Goroutine switch: ~200ns × 10K = 2ms/sec → 0.2% CPU!   ║
║  → 8 threads = 8 working sets → cache HIT phần lớn!        ║
║  → Netpoller: goroutine park/resume thay vì thread block    ║
║                                                               ║
║  Kết quả:                                                     ║
║  → Go: handle 10K connections ~ dễ dàng                      ║
║  → Java truyền thống: cần tuning thread pool, NIO           ║
║  → C10K problem: Go giải quyết by design!                    ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Đo lường Context Switch trong thực tế

```bash
# 1. Đếm số context switches của process
$ cat /proc/$(pidof myserver)/status | grep ctxt
voluntary_ctxt_switches:    15234    ← Tự nguyện (I/O wait)
nonvoluntary_ctxt_switches: 892      ← Bị preempt (timer)

# Nếu nonvoluntary >> voluntary → process bị preempt quá nhiều
# → Có thể GOMAXPROCS quá lớn, hoặc process CPU-bound trên busy server

# 2. Theo dõi realtime với perf
$ perf stat -e context-switches -p $(pidof myserver) sleep 5
# Performance counter stats:
#    12,345  context-switches  ← context switch trong 5 giây

# 3. vmstat — system-wide context switches
$ vmstat 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 150000  50000 800000    0    0     0     0  500 3000 15  5 80  0  0
#                                                          ^^^^ cs = context switches/sec
# cs > 50,000/sec trên server bình thường → có thể có vấn đề

# 4. Go runtime trace — xem goroutine scheduling
$ curl http://localhost:6060/debug/pprof/trace?seconds=5 > trace.out
$ go tool trace trace.out
# → Xem timeline: goroutine nào scheduled khi nào, trên M nào
# → Phát hiện goroutine bị park quá lâu hoặc switch quá nhiều
```

#### 5 Whys: Tại sao Context Switch tốn kém?

```
╔═══════════════════════════════════════════════════════════════╗
║   5 WHYS: TẠI SAO CONTEXT SWITCH ĐẮT?                      ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  WHY 1: Tại sao context switch tốn thời gian?               ║
║  → Vì phải save/restore CPU state + switch memory context.  ║
║                                                               ║
║  WHY 2: Tại sao switch memory context đắt?                  ║
║  → Vì phải thay page table (CR3) → TLB bị flush!          ║
║                                                               ║
║  WHY 3: Tại sao TLB flush đắt?                              ║
║  → Vì mỗi memory access phải page walk: 4 levels ×         ║
║    memory fetch = ~100 CPU cycles/miss (vs 1 cycle TLB hit) ║
║  → First ~100 unique pages truy cập → 100 × 100 = 10,000   ║
║    extra cycles = ~3μs trên 3GHz CPU!                       ║
║                                                               ║
║  WHY 4: Tại sao không cache nhiều hơn?                       ║
║  → CPU cache có giới hạn VẬT LÝ:                           ║
║    L1: 32-64KB (~1ns), L2: 256KB-1MB (~5ns)                  ║
║    L3: 8-30MB (~15ns), RAM: DDR (~50ns)                      ║
║  → Transistor cost: cache to = chip to = power to = heat!   ║
║                                                               ║
║  WHY 5: Giải pháp cuối cùng?                                 ║
║  → GIẢM SỐ SWITCHES: dùng goroutines (M:N model)           ║
║  → ÍT OS THREADS: GOMAXPROCS = số CPU cores                 ║
║  → ASYNC I/O: epoll/netpoller → không block thread          ║
║  → Đây là trade-off: app complexity vs OS overhead          ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 1.5 Fork & Exec — Cách Unix Tạo Process

#### Câu hỏi nền tảng: Làm sao tạo process mới?

Khi bạn gõ `./myserver` trong terminal, OS phải tạo process mới để chạy binary đó. Nhưng **làm sao tạo process từ hư không?** Trên Unix/Linux, câu trả lời là: **KHÔNG tạo từ hư không**. Mọi process mới đều được tạo bằng cách **clone process hiện tại** (fork), rồi **thay thế** nội dung bằng chương trình mới (exec).

Đây là thiết kế kinh điển của Unix từ 1969, và nó vẫn được dùng đến ngày nay — kể cả trong Docker containers, CI/CD pipelines, và khi Go chạy external commands.

> **Analogy**: Tưởng tượng bạn muốn tạo **hồ sơ nhân viên mới** trong công ty. Thay vì viết từ trang trắng, bạn:
>
> 1. **Photocopy** hồ sơ nhân viên hiện tại → có sẵn template, format, các trường cần điền (= **fork()**)
> 2. **Xóa nội dung cũ, ghi thông tin mới** → tên mới, phòng ban mới, chức vụ mới (= **exec()**)
>
> Tại sao không viết từ trang trắng? Vì photocopy+sửa **nhanh hơn** viết mới — template đã có sẵn format, không cần nhớ cần điền gì!

#### Bước 1: fork() — "Nhân bản" process

`fork()` là system call tạo **bản sao gần như giống hệt** của process hiện tại. Process gốc gọi là **parent**, bản sao gọi là **child**.

```
╔═══════════════════════════════════════════════════════════════╗
║   FORK() — CLONE PROCESS                                     ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  TRƯỚC fork():                                                ║
║  ┌──────────────────┐                                         ║
║  │ bash (PID 100)    │  ← Chỉ có 1 process                  ║
║  │ Code: bash binary │                                        ║
║  │ Heap: variables   │                                        ║
║  │ Stack: call stack  │                                        ║
║  │ Files: fd 0,1,2   │                                        ║
║  └──────────────────┘                                         ║
║                                                               ║
║  SAU fork():                                                   ║
║  ┌──────────────────┐       ┌──────────────────┐              ║
║  │ bash (PID 100)    │       │ bash (PID 101)    │             ║
║  │ Code: bash binary │═══════│ Code: bash binary │  ← SHARED  ║
║  │ Heap: variables   │·······│ Heap: variables   │  ← CoW!    ║
║  │ Stack: call stack  │       │ Stack: call stack  │  ← Copy   ║
║  │ Files: fd 0,1,2   │       │ Files: fd 0,1,2   │  ← Copy   ║
║  │                    │       │                    │             ║
║  │ fork() returns 101 │       │ fork() returns 0   │             ║
║  │ (child's PID)      │       │ (means "I'm child")│             ║
║  └──────────────────┘       └──────────────────┘              ║
║       PARENT                       CHILD                       ║
║                                                               ║
║  Hai process GIỐNG HỆT nhau, chạy CÙNG dòng code tiếp theo!║
║  Phân biệt parent/child bằng GIÁ TRỊ TRẢ VỀ của fork():    ║
║  → Parent nhận: child's PID (> 0)                            ║
║  → Child nhận: 0                                              ║
║  → Lỗi: -1                                                    ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Điều **kỳ diệu** của `fork()`: 1 lần gọi, **2 lần return**! Cùng 1 dòng code `pid = fork()`, nhưng:

- Trong parent process: `pid = 101` (PID của child)
- Trong child process: `pid = 0` (ý nghĩa: "tôi là child")

Code C minh họa:

```c
#include <unistd.h>
#include <stdio.h>

int main() {
    printf("Trước fork: PID = %d\n", getpid());

    pid_t pid = fork();  // ← 1 lần gọi, 2 lần return!

    if (pid == 0) {
        // === CHILD PROCESS ===
        // Code từ đây chạy trong process MỚI (PID 101)
        printf("Child: PID = %d, Parent PID = %d\n",
               getpid(), getppid());
        // Child thường gọi exec() ở đây để chạy chương trình khác
    } else if (pid > 0) {
        // === PARENT PROCESS ===
        // Code từ đây vẫn chạy trong process GỐC (PID 100)
        printf("Parent: PID = %d, Child PID = %d\n",
               getpid(), pid);
        wait(NULL);  // Chờ child kết thúc
    } else {
        // === LỖI === (pid == -1)
        perror("fork failed");
    }
    return 0;
}

// Output:
// Trước fork: PID = 100
// Parent: PID = 100, Child PID = 101
// Child: PID = 101, Parent PID = 100
```

#### Copy-on-Write (CoW) — Tại sao fork() nhanh?

Nếu fork() thực sự copy TOÀN BỘ memory của parent (ví dụ 500MB heap), nó sẽ rất chậm. Thực tế, Linux dùng **Copy-on-Write (CoW)** — kỹ thuật cực kỳ thông minh:

```
╔═══════════════════════════════════════════════════════════════╗
║   COPY-ON-WRITE (CoW) — FORK() NHANH NHỜ "LƯỜI"            ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Ngay sau fork():                                             ║
║  ┌────────────────────────────────────────────────┐           ║
║  │  Parent page table:    Child page table:       │           ║
║  │  VAddr → PAddr          VAddr → PAddr          │           ║
║  │  0x1000 → 0xA000 (R)   0x1000 → 0xA000 (R)   │           ║
║  │  0x2000 → 0xB000 (R)   0x2000 → 0xB000 (R)   │           ║
║  │  0x3000 → 0xC000 (R)   0x3000 → 0xC000 (R)   │           ║
║  │                                                 │           ║
║  │  → CẢ HAI trỏ đến CÙNG physical pages!       │           ║
║  │  → KHÔNG copy data gì hết! Chỉ copy page table│           ║
║  │  → Tất cả pages đánh dấu READ-ONLY!          │           ║
║  │  → fork() = copy page table = ~50μs!          │           ║
║  └────────────────────────────────────────────────┘           ║
║                                                               ║
║  Khi child GHI vào page 0x2000:                              ║
║  ┌────────────────────────────────────────────────┐           ║
║  │  1. CPU trigger PAGE FAULT (page = read-only!) │           ║
║  │  2. Kernel kiểm tra: "À, đây là CoW page"     │           ║
║  │  3. Kernel COPY page 0xB000 → 0xD000 (mới)   │           ║
║  │  4. Cập nhật child page table:                 │           ║
║  │     0x2000 → 0xD000 (R/W) ← page mới!        │           ║
║  │  5. Parent vẫn: 0x2000 → 0xB000 (không đổi)  │           ║
║  │                                                 │           ║
║  │  Parent page table:    Child page table:       │           ║
║  │  0x1000 → 0xA000 (R)   0x1000 → 0xA000 (R)   │           ║
║  │  0x2000 → 0xB000 (RW)  0x2000 → 0xD000 (RW)  │ ← ĐÃ TÁCH║
║  │  0x3000 → 0xC000 (R)   0x3000 → 0xC000 (R)   │           ║
║  │                                                 │           ║
║  │  → CHỈ page bị ghi mới được copy!             │           ║
║  │  → Pages không ai ghi: SHARE mãi mãi!        │           ║
║  └────────────────────────────────────────────────┘           ║
║                                                               ║
║  VÍ DỤ THỰC TẾ: Redis fork() để tạo RDB snapshot           ║
║  → Redis process: 10GB heap                                   ║
║  → fork(): chỉ copy page table (~1ms), KHÔNG copy 10GB!     ║
║  → Child (bgsave) đọc data → share pages → rất nhanh       ║
║  → Parent tiếp tục write → chỉ copy pages bị modify         ║
║  → Nếu 10% data thay đổi → chỉ copy 1GB! Tiết kiệm 9GB!  ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Bước 2: exec() — Thay thế bằng chương trình mới

Sau fork(), child process vẫn chạy code của parent (bash). Để chạy chương trình khác, child gọi **exec()** — system call này **thay thế TOÀN BỘ** code, data, heap, stack của process bằng chương trình mới.

```
╔═══════════════════════════════════════════════════════════════╗
║   EXEC() — THAY THẾ NỘI DUNG PROCESS                        ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Child process TRƯỚC exec():      SAU exec("./myserver"):    ║
║  ┌──────────────────┐             ┌──────────────────┐       ║
║  │ PID: 101          │             │ PID: 101 (GIỮA!) │       ║
║  │ Code: bash binary │──exec()──►  │ Code: myserver   │       ║
║  │ Heap: bash data   │             │ Heap: mới, rỗng  │       ║
║  │ Stack: bash stack  │             │ Stack: mới        │       ║
║  │ Files: fd 0,1,2   │             │ Files: fd 0,1,2  │       ║
║  └──────────────────┘             └──────────────────┘       ║
║                                                               ║
║  exec() thay đổi:          exec() GIỮ NGUYÊN:              ║
║  ✗ Code segment (mới!)    ✓ PID (vẫn 101)                  ║
║  ✗ Data segment (mới!)    ✓ PPID (vẫn 100)                 ║
║  ✗ Heap (reset!)          ✓ File descriptors (kế thừa!)    ║
║  ✗ Stack (reset!)         ✓ Working directory               ║
║                            ✓ Environment variables          ║
║                            ✓ User/Group ID                  ║
║                                                               ║
║  → exec() KHÔNG return nếu thành công!                      ║
║  → Vì code cũ đã bị thay thế — không còn gì để return!    ║
║  → Nếu exec() return → nghĩa là BỊ LỖI!                   ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Điểm quan trọng: exec() **giữ nguyên PID** và **file descriptors**! Đây là thiết kế cực kỳ hữu ích — parent có thể setup file descriptors (redirect stdin/stdout) TRƯỚC khi exec(), và child sẽ kế thừa. Đây chính là cơ chế đằng sau **pipe** trong shell:

```bash
# ls | grep ".go"
# Shell thực hiện:
# 1. Tạo pipe: pipe(fd[2]) → fd[0]=read, fd[1]=write
# 2. fork() → child 1: dup2(fd[1], STDOUT) → exec("ls")
#    → stdout của ls ghi vào pipe!
# 3. fork() → child 2: dup2(fd[0], STDIN) → exec("grep")
#    → stdin của grep đọc từ pipe!
# 4. Parent wait() cả 2 children
```

#### Toàn bộ flow: Khi bạn gõ "./myserver" trong terminal

```
╔═══════════════════════════════════════════════════════════════╗
║   TOÀN BỘ FLOW: TERMINAL → PROCESS MỚI                     ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  $ ./myserver --port 8080                                     ║
║                                                               ║
║  ① bash đọc input, parse command                            ║
║     → program = "./myserver"                                  ║
║     → args = ["--port", "8080"]                               ║
║                                                               ║
║  ② bash gọi fork()                                           ║
║     → Child bash (PID 101) = bản sao của parent bash        ║
║     → Parent bash (PID 100) gọi waitpid(101)                ║
║                                                               ║
║  ③ Child bash gọi execve("./myserver", args, env)           ║
║     → Kernel đọc ELF header của ./myserver                  ║
║     → Kernel thay thế code/data/heap/stack                  ║
║     → NHƯNG giữ PID 101, fd 0/1/2, env vars                ║
║                                                               ║
║  ④ myserver bắt đầu chạy! (PID 101)                        ║
║     → Go runtime khởi tạo (GC, scheduler, M threads)        ║
║     → main() được gọi                                        ║
║     → http.ListenAndServe(":8080", nil)                      ║
║                                                               ║
║  ⑤ Khi myserver exit (Ctrl+C):                              ║
║     → exit_group(0) → kernel cleanup                        ║
║     → Process trở thành ZOMBIE (vẫn có entry trong table)  ║
║     → Parent bash's waitpid() return → thu hồi ZOMBIE     ║
║     → bash hiện lại prompt: $                                ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Go và fork/exec

Go **không expose fork()** trực tiếp vì goroutines + fork = thảm họa. Khi fork(), child chỉ có **1 thread** (thread gọi fork), nhưng Go runtime cần nhiều threads (GC, scheduler, sysmon). Child process có state không nhất quán — locks bị giữ bởi threads đã biến mất!

```go
// ✅ Cách đúng trong Go: dùng os/exec
package main

import (
    "os/exec"
    "fmt"
)

func main() {
    // exec.Command internally sử dụng:
    // 1. fork() hoặc clone() — tạo child process
    // 2. Trong child: setup file descriptors
    // 3. exec() — thay thế bằng "ls"
    cmd := exec.Command("ls", "-la", "/tmp")
    output, err := cmd.Output()
    if err != nil {
        panic(err)
    }
    fmt.Println(string(output))

    // Ví dụ phức tạp hơn: pipe + redirect
    cmd = exec.Command("grep", "error", "/var/log/app.log")
    cmd.Stdout = os.Stdout  // Kế thừa stdout của parent
    cmd.Stderr = os.Stderr  // Kế thừa stderr
    cmd.Env = append(os.Environ(), "MY_VAR=hello")  // Thêm env var
    cmd.Run()

    // Nếu cần chạy nền (không wait):
    cmd = exec.Command("./background-worker")
    cmd.Start()       // fork+exec, KHÔNG chờ
    // cmd.Process.Pid  ← PID của child
    // cmd.Wait()       ← gọi sau khi muốn chờ
}
```

#### 5 Whys: Tại sao fork-then-exec thay vì "create process trực tiếp"?

```
╔═══════════════════════════════════════════════════════════════╗
║   5 WHYS: TẠI SAO FORK + EXEC?                              ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  WHY 1: Tại sao Unix dùng 2 bước (fork+exec) thay vì 1?    ║
║  → Vì giữa fork và exec, parent có thể SETUP cho child:    ║
║    redirect I/O, thay đổi env, set UID, chdir...            ║
║  → Windows dùng CreateProcess() = 1 bước, nhưng cần        ║
║    truyền MỌI THỨ qua parameters → API cực phức tạp!      ║
║                                                               ║
║  WHY 2: Tại sao fork thay vì tạo process rỗng?             ║
║  → Vì child kế thừa MỌI THỨ từ parent: env vars, fd,      ║
║    cwd, signal handlers... Không cần setup lại!             ║
║  → Trên Unix: "mặc định = kế thừa" → đơn giản!           ║
║  → Trên Windows: "mặc định = rỗng" → phải chỉ định hết!  ║
║                                                               ║
║  WHY 3: Fork có chậm không? Copy toàn bộ memory?           ║
║  → KHÔNG! Copy-on-Write: chỉ copy page table (~50μs)       ║
║  → Data chỉ copy khi BỊ GHI → nếu exec() ngay → gần 0!  ║
║                                                               ║
║  WHY 4: Tại sao Go không dùng fork() trực tiếp?            ║
║  → Vì fork() chỉ clone 1 thread, nhưng Go runtime cần     ║
║    nhiều threads (GC, scheduler). Child bị inconsistent!     ║
║  → os/exec.Command() xử lý mọi edge case trong Go runtime  ║
║                                                               ║
║  WHY 5: Fork còn dùng ở đâu ngoài exec command?            ║
║  → Redis: fork() để tạo RDB snapshot (bgsave)              ║
║  → Nginx: fork() workers để handle requests (pre-fork)      ║
║  → Docker: containerd fork+exec container processes         ║
║  → CI/CD: mỗi build step = fork+exec                       ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Zombie Process & Orphan Process — 2 trạng thái đặc biệt

Đây là 2 khái niệm hay gặp trong interview và production debugging:

**Zombie Process** (process ma): Child đã exit, nhưng parent **chưa gọi wait()** để đọc exit status. Kernel giữ lại entry trong process table (PID, exit code) để parent có thể hỏi "child kết thúc thế nào?". Zombie **không chiếm CPU hay memory** (code/heap/stack đã giải phóng), nhưng chiếm 1 slot trong process table.

```
╔═══════════════════════════════════════════════════════════════╗
║   ZOMBIE & ORPHAN PROCESSES                                  ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ZOMBIE — "Child đã chết nhưng chưa được chôn cất":         ║
║                                                               ║
║  Timeline:                                                    ║
║  ┌──────────┐    fork()    ┌──────────┐                      ║
║  │ Parent    │────────────►│ Child    │                       ║
║  │ PID 100   │             │ PID 101  │                       ║
║  └─────┬────┘             └─────┬────┘                       ║
║        │                        │                              ║
║        │ (đang làm              │ exit(0)  ← Child kết thúc ║
║        │  việc khác,            ▼                              ║
║        │  QUÊN gọi        ┌──────────┐                       ║
║        │  wait()!)        │ ZOMBIE!   │ ← state = "Z"        ║
║        │                  │ PID 101   │ ← Chỉ giữ exit code ║
║        │                  │ (defunct)  │ ← CPU=0, RAM=0      ║
║        │                  └──────────┘                        ║
║        │                                                       ║
║        │ wait()  ← Cuối cùng parent gọi wait()              ║
║        ▼                                                       ║
║  Kernel xóa zombie khỏi process table ✓                     ║
║                                                               ║
║  ⚠️ Vấn đề: Nếu parent KHÔNG BAO GIỜ gọi wait():          ║
║  → Zombie tồn tại MÃI MÃI cho đến khi parent exit          ║
║  → Nhiều zombies = mỗi cái chiếm 1 PID entry                ║
║  → Linux giới hạn ~32,768 PIDs (cat /proc/sys/kernel/pid_max)║
║  → Hết PIDs → KHÔNG tạo được process mới → server chết!   ║
║                                                               ║
║  ─────────────────────────────────────────────────────────    ║
║                                                               ║
║  ORPHAN — "Parent chết trước child":                         ║
║                                                               ║
║  ┌──────────┐    fork()    ┌──────────┐                      ║
║  │ Parent    │────────────►│ Child    │                       ║
║  │ PID 100   │             │ PID 101  │                       ║
║  └─────┬────┘             └─────┬────┘                       ║
║        │                        │                              ║
║        │ exit()                  │ (vẫn đang chạy)            ║
║        ▼                        │                              ║
║   Parent CHẾT!                  │                              ║
║                                  │                              ║
║   Kernel: "Child 101 mồ côi rồi│                              ║
║   → Gán PPID = 1 (init/systemd)"│                              ║
║                                  │                              ║
║                                  │ exit()                      ║
║                                  ▼                              ║
║   init (PID 1) tự động wait() → Thu hồi sạch sẽ ✓         ║
║                                                               ║
║  → Orphan KHÔNG phải vấn đề! init xử lý tự động           ║
║  → Đây là lý do PID 1 (init/systemd) quan trọng!           ║
║  → Docker: PID 1 trong container CẦN handle wait()!         ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

Ví dụ Go tạo zombie (và cách phòng tránh):

```go
// ❌ BUG: Tạo zombie nếu không wait()
func bad() {
    cmd := exec.Command("sleep", "1")
    cmd.Start()       // fork+exec
    // QUÊN gọi cmd.Wait()!
    // → Khi "sleep 1" kết thúc → zombie!
    time.Sleep(10 * time.Second)
    // Trong 9 giây, process "sleep" = zombie
}

// ✅ FIX 1: Luôn gọi Wait()
func good() {
    cmd := exec.Command("sleep", "1")
    cmd.Start()
    cmd.Wait()  // ← Đợi child exit, thu hồi zombie
}

// ✅ FIX 2: Dùng Run() (= Start() + Wait())
func better() {
    cmd := exec.Command("sleep", "1")
    cmd.Run()  // ← Tự động wait()
}

// ✅ FIX 3: Background task + goroutine wait
func background() {
    cmd := exec.Command("./long-running-task")
    cmd.Start()
    go func() {
        cmd.Wait()  // Background goroutine reap zombie
    }()
    // Main goroutine tiếp tục làm việc khác
}
```

Debug zombie trong production:

```bash
# Tìm zombie processes
$ ps aux | grep 'Z'
#  USER  PID  %CPU %MEM    VSZ   RSS  TTY STAT  TIME COMMAND
#  root  5678  0.0  0.0      0     0  ?   Z     0:00 [myworker] <defunct>
#                                         ^^^ Z = zombie!

# Đếm zombies
$ ps aux | awk '$8=="Z"' | wc -l
42    ← 42 zombies! Có thể leak nếu tăng liên tục

# Tìm parent của zombie (ai quên wait()?)
$ ps -o pid,ppid,stat,comm | grep Z
#  PID    PPID  STAT  COMMAND
#  5678   1234  Z     myworker

# → Parent PID 1234 quên gọi wait()!
# → Fix: sửa code parent, hoặc kill parent → init reap

# Docker: dùng tini làm PID 1 (reap zombies tự động)
# Dockerfile: ENTRYPOINT ["/tini", "--", "./myserver"]
```

#### Họ hàm exec() — 6 biến thể

exec() thực ra là **nhóm 6 hàm** với tên khác nhau. Tên mã hóa behavior:

```
╔═══════════════════════════════════════════════════════════════╗
║   EXEC() FAMILY — 6 BIẾN THỂ                                ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Cách đọc tên:                                               ║
║  exec + l/v + p (optional) + e (optional)                    ║
║                                                               ║
║  l = List args:    execl("/bin/ls", "ls", "-la", NULL)       ║
║  v = Vector args:  char *args[] = {"ls", "-la", NULL};       ║
║                    execv("/bin/ls", args)                     ║
║  p = PATH search:  execlp("ls", "ls", "-la", NULL)          ║
║                    (không cần full path, tìm trong $PATH)    ║
║  e = Environment:  execve("/bin/ls", args, envp)             ║
║                    (truyền environment variables)            ║
║                                                               ║
║  ┌──────────┬────────────┬────────────┬──────────────────┐   ║
║  │ Hàm       │ Args format│ PATH search│ Custom env       │   ║
║  ├──────────┼────────────┼────────────┼──────────────────┤   ║
║  │ execl    │ list       │ ✗ cần path│ ✗ kế thừa       │   ║
║  │ execv    │ array      │ ✗ cần path│ ✗ kế thừa       │   ║
║  │ execlp   │ list       │ ✓ $PATH   │ ✗ kế thừa       │   ║
║  │ execvp   │ array      │ ✓ $PATH   │ ✗ kế thừa       │   ║
║  │ execle   │ list       │ ✗ cần path│ ✓ custom envp   │   ║
║  │ execve ★ │ array      │ ✗ cần path│ ✓ custom envp   │   ║
║  └──────────┴────────────┴────────────┴──────────────────┘   ║
║                                                               ║
║  ★ execve() là SYSTEM CALL duy nhất. 5 hàm kia là          ║
║    library wrappers gọi execve() bên dưới!                   ║
║                                                               ║
║  Go's exec.Command() dùng execve() internally:              ║
║  cmd.Path  → argument 1 của execve (full path)              ║
║  cmd.Args  → argument 2 (argv array)                        ║
║  cmd.Env   → argument 3 (envp array)                        ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

#### Process Tree — Mọi process đều có tổ tiên

Vì mọi process đều được fork() từ process khác, tất cả processes tạo thành 1 **cây (tree)** với gốc là PID 1 (init/systemd):

```
╔═══════════════════════════════════════════════════════════════╗
║   PROCESS TREE — "GIA PHẢ" PROCESSES                        ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  PID 0: idle (kernel scheduler)    ← Không ai tạo!         ║
║  │                                                            ║
║  ├── PID 1: init/systemd           ← Kernel tạo trực tiếp  ║
║  │   │                              ← "Parent of all"       ║
║  │   ├── PID 500: sshd             ← systemd fork()        ║
║  │   │   └── PID 1200: sshd        ← Accept SSH connection  ║
║  │   │       └── PID 1201: bash     ← Login shell           ║
║  │   │           ├── PID 1500: vim  ← User gõ "vim"        ║
║  │   │           └── PID 1600: go   ← User gõ "go run"     ║
║  │   │               └── PID 1601: main  ← Go binary       ║
║  │   │                                                       ║
║  │   ├── PID 600: dockerd          ← Docker daemon         ║
║  │   │   ├── PID 800: containerd                            ║
║  │   │   │   └── PID 900: myserver  ← Container process    ║
║  │   │   │                                                   ║
║  │   ├── PID 700: nginx (master)    ← Nginx master         ║
║  │   │   ├── PID 701: nginx worker  ← Pre-fork worker      ║
║  │   │   ├── PID 702: nginx worker                          ║
║  │   │   └── PID 703: nginx worker                          ║
║  │   │                                                       ║
║  │   └── PID 400: cron             ← Job scheduler         ║
║  │                                                            ║
║  └── PID 2: kthreadd              ← Kernel threads parent  ║
║      ├── PID 3: ksoftirqd/0                                 ║
║      ├── PID 4: kworker/0:0                                 ║
║      └── ...                                                  ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

```bash
# Xem process tree thực tế
$ pstree -p
systemd(1)─┬─dockerd(600)─┬─containerd(800)───myserver(900)
           │              │
           ├─sshd(500)───sshd(1200)───bash(1201)───vim(1500)
           │
           ├─nginx(700)─┬─nginx(701)
           │            ├─nginx(702)
           │            └─nginx(703)
           └─cron(400)

# Xem parent chain của 1 process
$ cat /proc/1601/status | grep PPid
PPid: 1600    ← Parent là Go compiler

# Trong Go
fmt.Println(os.Getpid())   // PID của process hiện tại
fmt.Println(os.Getppid())  // PID của parent
```

#### Fork Bomb — Khi fork() bị lạm dụng

Kiến thức bảo mật quan trọng: **Fork bomb** là attack đơn giản nhất nhưng hiệu quả nhất — liên tục fork() cho đến khi hệ thống hết PIDs/RAM:

```bash
# ⚠️ ĐỪNG CHẠY CÁI NÀY! Fork bomb kinh điển:
:(){ :|:& };:
# Giải thích:
# :()      — định nghĩa function tên ":"
# { :|:& } — function body: gọi chính nó 2 lần (pipe + background)
# ;:       — gọi function lần đầu

# → 1 → 2 → 4 → 8 → 16 → ... → 2^30 processes trong vài giây!
# → Hết PID table → hết RAM → server ĐỨNG YÊN!
```

Phòng chống:

```bash
# Linux ulimit — giới hạn số processes per user
$ ulimit -u 500   # Tối đa 500 processes
$ ulimit -u       # Xem giới hạn hiện tại

# /etc/security/limits.conf — permanent
# student    hard    nproc    200
# @devops    hard    nproc    1000

# Docker — giới hạn PIDs per container
# docker run --pids-limit 100 myimage

# Kubernetes — Pod PID limits
# spec.containers.resources.limits.pid: "100"

# Go: os/exec tự động không fork bomb vì
# exec.Command() luôn fork+exec (thay code ngay)
# Nhưng nếu Start() không Wait() → zombie leak!
```

### 1.6 Process States — Vòng Đời Của Process

Process không phải lúc nào cũng chạy. Nó có nhiều trạng thái khác nhau, và hiểu các trạng thái này giải thích tại sao server đôi khi "nặng" dù CPU usage thấp (nhiều process ở trạng thái Waiting).

```
╔═══════════════════════════════════════════════════════════════╗
║   PROCESS STATE MACHINE                                      ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║                     ┌──────────┐                              ║
║            fork()──►│  CREATED │                              ║
║                     └────┬─────┘                              ║
║                          │ admitted                           ║
║                          ▼                                    ║
║                     ┌──────────┐  scheduler  ┌──────────┐    ║
║              ┌─────►│  READY   │────────────►│ RUNNING  │    ║
║              │      └──────────┘  dispatch   └─────┬────┘    ║
║              │           ▲                         │  │       ║
║              │           │ preempt (timer IRQ)     │  │       ║
║              │           └─────────────────────────┘  │       ║
║              │                                        │       ║
║              │  I/O done                   I/O wait   │       ║
║              │  / signal                   / sleep     │       ║
║              │                                        │       ║
║              │      ┌──────────┐                      │       ║
║              └──────│ WAITING  │◄─────────────────────┘       ║
║                     │(Blocked) │                              ║
║                     └──────────┘                              ║
║                          │                                    ║
║                          │                                    ║
║                     ┌──────────┐                              ║
║     (parent wait())│TERMINATED│◄── exit() / signal            ║
║              ┌─────│ (Zombie) │                               ║
║              │     └──────────┘                               ║
║              ▼                                                ║
║         [Reaped — resources freed]                            ║
║                                                               ║
║  LINUX-SPECIFIC STATES:                                      ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  R  Running/Runnable (đang hoặc sẵn sàng)   │             ║
║  │  S  Sleeping (interruptible — chờ I/O)       │             ║
║  │  D  Disk sleep (uninterruptible — chờ disk) │             ║
║  │     → KHÔNG THỂ kill! → "D state" bug       │             ║
║  │  T  Stopped (SIGSTOP / debugger)              │             ║
║  │  Z  Zombie (exited, chờ parent wait())       │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ BACKEND RELEVANCE:                                         ║
║  → ps aux: nhiều process "S" = I/O-bound (bình thường)     ║
║  → ps aux: nhiều process "D" = disk/NFS bottleneck! ⚠️      ║
║  → ps aux: nhiều "Z" = parent không wait() → bug!          ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 1.7 Goroutine vs Thread vs Process — So Sánh Toàn Diện

Đây là **phần quan trọng nhất** cho Go backend engineer. Hiểu sự khác biệt giải thích tại sao Go handle 100K concurrent connections dễ dàng trong khi Java truyền thống gặp khó.

```
╔═══════════════════════════════════════════════════════════════╗
║   GOROUTINE vs THREAD vs PROCESS                             ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ┌──────────────┬──────────┬──────────┬──────────────┐       ║
║  │ Tiêu chí      │ Process  │ Thread   │ Goroutine    │       ║
║  ├──────────────┼──────────┼──────────┼──────────────┤       ║
║  │ Quản lý bởi  │ OS       │ OS       │ Go runtime   │       ║
║  │ Stack size   │ 8MB      │ 1-8MB    │ 2KB (grow!)  │       ║
║  │ Tạo mới      │ ~100μs   │ ~10μs    │ ~0.3μs       │       ║
║  │ Switch cost  │ ~50μs    │ ~10μs    │ ~0.2μs       │       ║
║  │ Memory/unit  │ ~10MB    │ ~1MB     │ ~2KB         │       ║
║  │ Max practical│ ~1K      │ ~10K     │ ~1M          │       ║
║  │ Scheduling   │ Kernel   │ Kernel   │ Userspace    │       ║
║  │ Isolation    │ Full     │ Partial  │ None (same   │       ║
║  │              │          │          │ address)     │       ║
║  │ Crash impact │ Only self│ Entire   │ Entire       │       ║
║  │              │          │ process  │ process      │       ║
║  └──────────────┴──────────┴──────────┴──────────────┘       ║
║                                                               ║
║  TRỰC QUAN HÓA - 10K concurrent connections:                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Apache (1 process/conn):                     │             ║
║  │  10K × 10MB = 100GB RAM ← KHÔNG KHẢ THI!    │             ║
║  │                                                │             ║
║  │  Java Tomcat (1 thread/conn):                 │             ║
║  │  10K × 1MB = 10GB RAM ← Nặng nhưng khả thi  │             ║
║  │  10K context switches/s = vấn đề!            │             ║
║  │                                                │             ║
║  │  Go (1 goroutine/conn):                       │             ║
║  │  10K × 2KB = 20MB RAM ← DƯ SỨC!             │             ║
║  │  Switch: userspace, microseconds              │             ║
║  │  Thậm chí 100K goroutines = 200MB → OK!      │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 1.8 GMP Model — Go Runtime Scheduler

Go scheduler là trái tim của concurrency trong Go. Nó implement **M:N threading** — map M goroutines lên N OS threads, kết hợp ưu điểm của cả hai thế giới.

```
╔═══════════════════════════════════════════════════════════════╗
║   GMP MODEL — GO RUNTIME SCHEDULER                          ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  G = Goroutine (đơn vị work)                                ║
║  M = Machine  (OS thread thật)                              ║
║  P = Processor (logical processor, = GOMAXPROCS)            ║
║                                                               ║
║  ┌──────────────────────────────────────────────────┐        ║
║  │                                                    │        ║
║  │  P0 (Local Queue)     P1 (Local Queue)            │        ║
║  │  ┌─────────────┐     ┌─────────────┐              │        ║
║  │  │ G1 G2 G3 G4 │     │ G5 G6 G7    │              │        ║
║  │  └──────┬──────┘     └──────┬──────┘              │        ║
║  │         │                    │                      │        ║
║  │         ▼                    ▼                      │        ║
║  │  ┌──────────┐        ┌──────────┐                  │        ║
║  │  │ M0       │        │ M1       │                  │        ║
║  │  │(OS thrd) │        │(OS thrd) │                  │        ║
║  │  └──────────┘        └──────────┘                  │        ║
║  │       ↕                    ↕                        │        ║
║  │  ┌──────────┐        ┌──────────┐                  │        ║
║  │  │CPU Core 0│        │CPU Core 1│                  │        ║
║  │  └──────────┘        └──────────┘                  │        ║
║  │                                                    │        ║
║  │  Global Queue: [ G8, G9, G10 ]                    │        ║
║  │  → Khi local queue đầy, goroutine vào đây       │        ║
║  │                                                    │        ║
║  └──────────────────────────────────────────────────┘        ║
║                                                               ║
║  WORK STEALING:                                              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  P0: [G1, G2, G3, G4]    P1: [  ] (empty!)   │             ║
║  │                                                │             ║
║  │  P1 idle → steal NỬA queue từ P0:           │             ║
║  │  P0: [G1, G2]            P1: [G3, G4]         │             ║
║  │                                                │             ║
║  │  → Load balancing tự động!                    │             ║
║  │  → Không cần global lock (mỗi P có local!)   │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  KHI GOROUTINE BỊ BLOCK (syscall):                          ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  G1 gọi syscall (disk read, CGo call):       │             ║
║  │                                                │             ║
║  │  BEFORE:  P0 ←→ M0 running G1                │             ║
║  │                                                │             ║
║  │  1. G1 blocks on syscall                       │             ║
║  │  2. P0 DETACH from M0                          │             ║
║  │  3. P0 tìm M2 (hoặc create mới) để chạy G2  │             ║
║  │  4. M0 blocked trên syscall cùng G1           │             ║
║  │                                                │             ║
║  │  AFTER:   P0 ←→ M2 running G2                │             ║
║  │           M0 blocked (G1 waiting)              │             ║
║  │                                                │             ║
║  │  → P0 KHÔNG BAO GIỜ bị block!                │             ║
║  │  → Luôn có M available để chạy G!            │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 1.9 IPC — Giao Tiếp Giữa Các Process

Khi 2 processes cần nói chuyện với nhau (ví dụ web server gửi request đến database), chúng cần **IPC (Inter-Process Communication)** vì mỗi process có address space riêng.

```
╔═══════════════════════════════════════════════════════════════╗
║   IPC — INTER-PROCESS COMMUNICATION                         ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ┌──────────────────┬───────────┬──────────┬────────────┐    ║
║  │ Mechanism        │ Speed     │ Data     │ Use Case   │    ║
║  ├──────────────────┼───────────┼──────────┼────────────┤    ║
║  │ Pipe             │ Fast      │ Stream   │ cmd1|cmd2  │    ║
║  │ Named Pipe(FIFO) │ Fast      │ Stream   │ Unrelated  │    ║
║  │ Unix Socket      │ Very fast │ Stream/  │ Same host  │    ║
║  │                  │           │ Datagram │ DB conns   │    ║
║  │ TCP Socket       │ Medium    │ Stream   │ Network    │    ║
║  │ Shared Memory    │ Fastest!  │ Random   │ High perf  │    ║
║  │ Message Queue    │ Medium    │ Messages │ Async      │    ║
║  │ Signal           │ Fast      │ Tiny     │ SIGTERM    │    ║
║  │ mmap file        │ Fast      │ Random   │ Shared DB  │    ║
║  └──────────────────┴───────────┴──────────┴────────────┘    ║
║                                                               ║
║  PIPE — Shell pipeline:                                      ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  $ cat access.log | grep "500" | wc -l       │             ║
║  │                                                │             ║
║  │  cat ──pipe──► grep ──pipe──► wc             │             ║
║  │  (write end)   (read|write)   (read end)      │             ║
║  │                                                │             ║
║  │  → Kernel buffer: 64KB (pipe_buf)             │             ║
║  │  → Nếu buffer FULL: writer BLOCKS            │             ║
║  │  → Nếu buffer EMPTY: reader BLOCKS           │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  UNIX DOMAIN SOCKET:                                         ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  PostgreSQL default connection:               │             ║
║  │  → /var/run/postgresql/.s.PGSQL.5432         │             ║
║  │                                                │             ║
║  │  Go → PostgreSQL (Unix socket):              │             ║
║  │  db, _ := sql.Open("postgres",               │             ║
║  │    "host=/var/run/postgresql dbname=mydb")    │             ║
║  │                                                │             ║
║  │  → 2-3× faster than TCP (skip network stack!)│             ║
║  │  → No TCP overhead (handshake, checksums)     │             ║
║  │  → Same machine only!                         │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ Go PHILOSOPHY:                                             ║
║  → "Do not communicate by sharing memory;                    ║
║     share memory by communicating." — Go Proverb             ║
║  → Channels > Shared Memory cho goroutines!                  ║
║  → Nhưng IPC (socket/pipe) vẫn cần cho cross-process!      ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 1.10 Zombie & Orphan — Hai Loại Process Bất Thường

Trong production, **zombie processes** và **orphan processes** là source of bugs phổ biến nhưng developer thường không biết.

```
╔═══════════════════════════════════════════════════════════════╗
║   ZOMBIE & ORPHAN PROCESSES                                  ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ZOMBIE PROCESS — "Chết nhưng chưa được chôn":             ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Parent (PID 100)         Child (PID 101)     │             ║
║  │  ┌──────────────┐        ┌──────────────┐     │             ║
║  │  │ Running...   │        │ exit(0)      │     │             ║
║  │  │              │        │ ↓            │     │             ║
║  │  │ KHÔNG gọi   │        │ ZOMBIE (Z)   │     │             ║
║  │  │ wait()!      │        │ → PID vẫn    │     │             ║
║  │  │              │        │   tồn tại    │     │             ║
║  │  │              │        │ → PCB vẫn    │     │             ║
║  │  │              │        │   chiếm slot │     │             ║
║  │  └──────────────┘        └──────────────┘     │             ║
║  │                                                │             ║
║  │  Tại sao nguy hiểm?                           │             ║
║  │  → Mỗi zombie chiếm 1 PID slot               │             ║
║  │  → Linux default: max 32768 PIDs              │             ║
║  │  → 32K zombies = KHÔNG THỂ tạo process mới! │             ║
║  │                                                │             ║
║  │  Fix: Parent phải gọi wait()/waitpid()        │             ║
║  │  Go: os/exec.Cmd.Wait() tự handle!           │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ORPHAN PROCESS — "Mồ côi":                                 ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Parent exit TRƯỚC child:                     │             ║
║  │                                                │             ║
║  │  Parent (PID 100)         Child (PID 101)     │             ║
║  │  ┌──────────────┐        ┌──────────────┐     │             ║
║  │  │ exit()       │        │ Running...   │     │             ║
║  │  │ (chết)       │        │              │     │             ║
║  │  └──────────────┘        │ PPID: 100→1 │     │             ║
║  │                           │ (adopted by │     │             ║
║  │                           │  PID 1 init)│     │             ║
║  │                           └──────────────┘     │             ║
║  │                                                │             ║
║  │  → PID 1 (init/systemd) "nhận nuôi"         │             ║
║  │  → PID 1 tự động gọi wait() khi orphan exit │             ║
║  │  → Orphan ít nguy hiểm hơn zombie           │             ║
║  │                                                │             ║
║  │  ⚠️ Docker: container PID 1 = entrypoint!    │             ║
║  │  → Nếu Go binary là PID 1, phải handle       │             ║
║  │    SIGCHLD để reap zombies!                   │             ║
║  │  → Hoặc dùng tini/dumb-init làm PID 1       │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

---

## §2. CPU Scheduling — Ai Được Chạy Tiếp?

### 2.1 Bài Toán Scheduling — Tại Sao Cần Thuật Toán?

Khi có 100 processes muốn chạy nhưng chỉ có 4 CPU cores, OS phải quyết định: **ai chạy trước, ai chờ, chạy bao lâu?** Quyết định sai = user thấy lag, database timeout, API chậm.

Scheduling giống như quản lý hàng đợi tại ngân hàng: bạn có thể phục vụ ai đến trước (FIFO), ai cần ít thời gian nhất (SJF), hay ai VIP (Priority)?

```
╔═══════════════════════════════════════════════════════════════╗
║   SCHEDULING ALGORITHMS                                     ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  1. FCFS (First Come First Served):                          ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Queue: [P1:24ms] [P2:3ms] [P3:3ms]          │             ║
║  │                                                │             ║
║  │  Timeline:                                     │             ║
║  │  |───P1 (24ms)───────────|P2(3)|P3(3)|        │             ║
║  │  0                       24    27    30        │             ║
║  │                                                │             ║
║  │  Waiting: P1=0, P2=24, P3=27                  │             ║
║  │  Average wait: (0+24+27)/3 = 17ms ← TỆ!     │             ║
║  │                                                │             ║
║  │  "Convoy Effect": 1 task dài block hết!      │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  2. SJF (Shortest Job First):                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Queue: [P2:3ms] [P3:3ms] [P1:24ms]          │             ║
║  │                                                │             ║
║  │  Timeline:                                     │             ║
║  │  |P2(3)|P3(3)|───P1 (24ms)───────────|        │             ║
║  │  0     3     6                       30        │             ║
║  │                                                │             ║
║  │  Waiting: P2=0, P3=3, P1=6                    │             ║
║  │  Average wait: (0+3+6)/3 = 3ms ← TỐT!      │             ║
║  │                                                │             ║
║  │  Vấn đề: Làm sao biết task dài bao lâu?     │             ║
║  │  → Prediction based on history!                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  3. ROUND ROBIN (time slicing):                              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Quantum = 4ms                                 │             ║
║  │                                                │             ║
║  │  |P1|P2|P3|P1|P1|P1|P1|P1|                    │             ║
║  │  0  4  7  10 14 18 22 26 30                    │             ║
║  │                                                │             ║
║  │  → Mỗi process chạy tối đa 4ms rồi nhường  │             ║
║  │  → Fair! Không ai bị starve                   │             ║
║  │  → Trade-off: context switch overhead         │             ║
║  │                                                │             ║
║  │  Quantum quá nhỏ (1ms): switch liên tục!     │             ║
║  │  Quantum quá lớn (100ms): giống FCFS!        │             ║
║  │  Sweet spot: 10-100ms (Linux default)         │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  4. PRIORITY SCHEDULING:                                     ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Mỗi process có priority (nice: -20 đến +19)│             ║
║  │  → DB process: nice -5 (ưu tiên cao)        │             ║
║  │  → Backup cron: nice +19 (ưu tiên thấp)     │             ║
║  │                                                │             ║
║  │  ⚠️ Starvation: low priority KHÔNG BAO GIỜ  │             ║
║  │     được chạy nếu luôn có high priority      │             ║
║  │  → Fix: Aging — tăng priority theo wait time │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 2.2 Preemptive vs Cooperative Scheduling

```
╔═══════════════════════════════════════════════════════════════╗
║   PREEMPTIVE vs COOPERATIVE                                  ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  COOPERATIVE (Non-preemptive):                               ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Process tự "nhường" CPU khi sẵn sàng:     │             ║
║  │  → yield() / sleep() / I/O                    │             ║
║  │                                                │             ║
║  │  Vấn đề: Nếu process KHÔNG nhường?          │             ║
║  │  → Mọi process khác bị ĐÓNG BĂNG!           │             ║
║  │  → Windows 3.1, Classic Mac OS dùng kiểu này│             ║
║  │  → 1 app treo = CẢ hệ thống treo!          │             ║
║  │                                                │             ║
║  │  Go trước 1.14: goroutines cooperative!       │             ║
║  │  → for {} loop = KHÔNG BAO GIỜ yield!       │             ║
║  │  → Block tất cả goroutines trên P đó!       │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  PREEMPTIVE:                                                  ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  OS CƯỠNG CHẾ dừng process via timer IRQ:   │             ║
║  │                                                │             ║
║  │  ┌─────────┐    timer    ┌──────────┐         │             ║
║  │  │Process A│───IRQ!────►│ Kernel   │         │             ║
║  │  │ running │            │scheduler │         │             ║
║  │  └─────────┘            │→ switch  │         │             ║
║  │                          │  to B    │         │             ║
║  │                          └──────────┘         │             ║
║  │                                                │             ║
║  │  → Process KHÔNG THỂ monopolize CPU          │             ║
║  │  → Linux, Windows NT, macOS đều preemptive   │             ║
║  │                                                │             ║
║  │  Go 1.14+: goroutines preemptive!             │             ║
║  │  → Async preemption via signals (SIGURG)      │             ║
║  │  → for {} loop bị preempt sau ~10ms!         │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 2.3 CFS — Linux Completely Fair Scheduler

CFS là scheduler default của Linux từ kernel 2.6.23 (2007). Thay vì time slices cố định, CFS dùng **virtual runtime** — ai đã được chạy ÍT nhất sẽ chạy tiếp.

```
╔═══════════════════════════════════════════════════════════════╗
║   CFS — COMPLETELY FAIR SCHEDULER                           ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Ý TƯỞNG: "Ideal fair CPU" chia đều thời gian             ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  4 processes, 1 CPU:                          │             ║
║  │  Ideal: mỗi process chạy 25% thời gian      │             ║
║  │                                                │             ║
║  │  CFS track vruntime (virtual runtime):        │             ║
║  │  → vruntime = actual_runtime / weight         │             ║
║  │  → weight phụ thuộc vào nice value           │             ║
║  │  → nice 0: weight 1024                        │             ║
║  │  → nice -5: weight 3121 (chạy NHIỀU hơn)    │             ║
║  │  → nice +5: weight 335 (chạy ÍT hơn)        │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  RED-BLACK TREE (Data Structure của CFS):                    ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Leftmost node = process có vruntime THẤP nhất│             ║
║  │  → Luôn được chạy tiếp!                      │             ║
║  │                                                │             ║
║  │         ┌───[50ms]───┐                         │             ║
║  │     [20ms]        [80ms]                       │             ║
║  │    /     \       /      \                      │             ║
║  │  [10ms] [30ms] [60ms] [100ms]                  │             ║
║  │  ↑                                              │             ║
║  │  Leftmost! → chạy process này!               │             ║
║  │                                                │             ║
║  │  Lookup: O(1) (cached leftmost pointer!)      │             ║
║  │  Insert/Delete: O(log n)                       │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ BACKEND TUNING:                                            ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  # Đặt DB process priority cao hơn:          │             ║
║  │  $ nice -n -5 postgres                        │             ║
║  │  $ renice -5 -p $(pidof postgres)             │             ║
║  │                                                │             ║
║  │  # Scheduler tunables:                        │             ║
║  │  sched_latency_ns = 6000000    # 6ms target   │             ║
║  │  sched_min_granularity = 750000 # min 0.75ms  │             ║
║  │                                                │             ║
║  │  # Real-time priority cho latency-critical:   │             ║
║  │  $ chrt -f 99 ./my-realtime-service           │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 2.4 Go Scheduler vs OS Scheduler — Hai Tầng Scheduling

Go có **2 tầng scheduling**: OS scheduler quản lý M (threads), Go scheduler quản lý G (goroutines) trên P. Hiểu sự khác biệt này là chìa khóa để tune Go performance.

```
╔═══════════════════════════════════════════════════════════════╗
║   TWO-LEVEL SCHEDULING                                       ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ┌─────────────────────────────────────────────┐             ║
║  │                                               │             ║
║  │  TẦNG 1: Go Runtime Scheduler                │             ║
║  │  ┌─────────────────────────────────────────┐ │             ║
║  │  │ G1 G2 G3 ... G100000                    │ │             ║
║  │  │         ↓ (M:N mapping)                 │ │             ║
║  │  │ P0→M0   P1→M1   P2→M2   P3→M3         │ │             ║
║  │  └──────────────────────┬──────────────────┘ │             ║
║  │                          │                     │             ║
║  │  TẦNG 2: OS (Linux CFS)                      │             ║
║  │  ┌──────────────────────┴──────────────────┐ │             ║
║  │  │ M0 M1 M2 M3 (OS threads)               │ │             ║
║  │  │         ↓ (CFS scheduling)              │ │             ║
║  │  │ Core0   Core1   Core2   Core3           │ │             ║
║  │  └─────────────────────────────────────────┘ │             ║
║  │                                               │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
║  KHI NÀO MỖI TẦNG HOẠT ĐỘNG:                               ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Go scheduler (goroutine switch):             │             ║
║  │  → channel send/receive                        │             ║
║  │  → network I/O (netpoller)                     │             ║
║  │  → time.Sleep()                                │             ║
║  │  → runtime.Gosched()                           │             ║
║  │  → async preemption (>10ms running)           │             ║
║  │  → sync.Mutex contention                       │             ║
║  │                                                │             ║
║  │  OS scheduler (thread switch):                │             ║
║  │  → syscall (disk I/O, CGo)                    │             ║
║  │  → Timer interrupt (CFS preemption)           │             ║
║  │  → Page fault                                  │             ║
║  │  → Signal delivery                             │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  GOMAXPROCS:                                                  ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  GOMAXPROCS = số P = số goroutines chạy     │             ║
║  │  ĐỒNG THỜI thực sự (parallel)                │             ║
║  │                                                │             ║
║  │  Default: runtime.NumCPU()                     │             ║
║  │  → 4 cores → GOMAXPROCS=4 → 4P → 4M min    │             ║
║  │                                                │             ║
║  │  Container gotcha:                             │             ║
║  │  → Docker CPU limit = 2 cores                 │             ║
║  │  → Nhưng runtime.NumCPU() = 16 (host CPUs!)  │             ║
║  │  → GOMAXPROCS=16 → quá nhiều context switch! │             ║
║  │  → Fix: import _ "go.uber.org/automaxprocs"   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 2.5 Priority Inversion — Bug Kinh Điển

Priority Inversion xảy ra khi **high-priority task bị chờ low-priority task** giữ lock. Bug này từng làm crash Mars Pathfinder năm 1997!

```
╔═══════════════════════════════════════════════════════════════╗
║   PRIORITY INVERSION                                         ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  3 tasks: H (High), M (Medium), L (Low priority)            ║
║                                                               ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  1. L lấy mutex lock                          │             ║
║  │  2. H cần mutex → BLOCKED (chờ L unlock)    │             ║
║  │  3. M ready → M chạy (priority cao hơn L!)  │             ║
║  │  4. L KHÔNG ĐƯỢC CHẠY vì M đang chạy!       │             ║
║  │  5. H VẪN BLOCKED vì L chưa unlock!         │             ║
║  │                                                │             ║
║  │  Timeline:                                     │             ║
║  │  L: |██lock██|----blocked by M----|██unlock██|│             ║
║  │  M: |        |████████████████████|          | │             ║
║  │  H: |        blocked by L!!!!!!!!!!!|████████| │             ║
║  │                                                │             ║
║  │  → H (highest!) chạy SAU M (medium)!        │             ║
║  │  → Priority bị "đảo ngược"!                   │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  FIX — Priority Inheritance:                                 ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Khi H chờ lock do L giữ:                    │             ║
║  │  → Tạm NÂNG priority L lên = H!             │             ║
║  │  → L chạy ngay (priority cao), unlock nhanh  │             ║
║  │  → H được lock, chạy ngay                    │             ║
║  │  → L trở về priority gốc                    │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ Go KHÔNG CÓ priority cho goroutines!                     ║
║  → Tất cả goroutines "bình đẳng"                           ║
║  → Không bị priority inversion kiểu OS                      ║
║  → Nhưng sync.Mutex starvation vẫn có thể xảy ra!         ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 2.6 Starvation & Livelock

```
╔═══════════════════════════════════════════════════════════════╗
║   STARVATION & LIVELOCK                                      ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  STARVATION — "Chết đói":                                   ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Process/goroutine KHÔNG BAO GIỜ được chạy  │             ║
║  │  vì luôn có cái khác ưu tiên hơn            │             ║
║  │                                                │             ║
║  │  Ví dụ: sync.RWMutex trong Go                │             ║
║  │  → Continuous readers = writer STARVED!       │             ║
║  │  → Go fix: writer có priority sau khi chờ    │             ║
║  │                                                │             ║
║  │  Ví dụ: Kafka consumer với slow processing   │             ║
║  │  → Consumer lag tăng mãi không giảm          │             ║
║  │  → Messages cũ bị "starved" không xử lý     │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  LIVELOCK — "Chạy nhưng không tiến":                        ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  2 goroutines nhường nhau mãi:               │             ║
║  │                                                │             ║
║  │  G1: lock A → try B → fail → unlock A       │             ║
║  │  G2: lock B → try A → fail → unlock B       │             ║
║  │  G1: lock A → try B → fail → unlock A       │             ║
║  │  ... lặp mãi, không ai tiến!                 │             ║
║  │                                                │             ║
║  │  Khác deadlock: processes ĐANG chạy (CPU > 0%)│             ║
║  │  Nhưng KHÔNG làm được gì useful!             │             ║
║  │                                                │             ║
║  │  Fix: Random backoff (giống Ethernet CSMA/CD)│             ║
║  │  → time.Sleep(rand.Intn(100) * time.Ms)      │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 2.7 Real-time vs General Purpose Scheduling

```
╔═══════════════════════════════════════════════════════════════╗
║   REAL-TIME SCHEDULING                                       ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  GENERAL PURPOSE (Linux CFS):                                ║
║  → "Best effort" fairness                                    ║
║  → No deadline guarantees                                    ║
║  → OK cho web servers, databases                             ║
║                                                               ║
║  REAL-TIME:                                                   ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Hard real-time:                               │             ║
║  │  → Deadline BẮT BUỘC (miss = failure!)       │             ║
║  │  → VD: airbag controller, ABS brakes         │             ║
║  │                                                │             ║
║  │  Soft real-time:                               │             ║
║  │  → Deadline ưu tiên (miss = degraded)        │             ║
║  │  → VD: video streaming, VoIP                  │             ║
║  │  → Linux SCHED_FIFO, SCHED_RR                 │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ BACKEND: Khi nào cần real-time priority?                  ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  → Trading systems (microsecond latency)      │             ║
║  │  → Game servers (tick rate consistency)        │             ║
║  │  → Thường KHÔNG cần cho web APIs             │             ║
║  │  → Go GC: STW pause = soft real-time concern!│             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 2.8 Multi-Queue Scheduling — Server Đa CPU

```
╔═══════════════════════════════════════════════════════════════╗
║   MULTI-QUEUE MULTIPROCESSOR SCHEDULING (MQMS)              ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  SINGLE QUEUE (SMP — đơn giản nhưng chậm):                 ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Global Queue: [P1, P2, P3, P4, P5, P6...]   │             ║
║  │       ↓        ↓        ↓        ↓             │             ║
║  │    Core 0   Core 1   Core 2   Core 3          │             ║
║  │                                                │             ║
║  │  Vấn đề: Global lock contention!             │             ║
║  │  → 64 cores tranh 1 lock = bottleneck!       │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  MULTI-QUEUE (Per-CPU — Linux & Go dùng):                   ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Queue 0: [P1, P3]     → Core 0              │             ║
║  │  Queue 1: [P2, P4]     → Core 1              │             ║
║  │  Queue 2: [P5]         → Core 2              │             ║
║  │  Queue 3: [  ]         → Core 3 (idle!)      │             ║
║  │                                                │             ║
║  │  → Mỗi core lock riêng = no contention!      │             ║
║  │  → Nhưng: load imbalance!                     │             ║
║  │  → Fix: work stealing (Core 3 steal từ Q0)  │             ║
║  │  → Go P's local queue = chính xác pattern này!│             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  CACHE AFFINITY:                                              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Process P1 chạy trên Core 0:                │             ║
║  │  → Data P1 đã nằm trong L1/L2 cache Core 0  │             ║
║  │  → Nếu migrate P1 sang Core 1: cache COLD!  │             ║
║  │  → Cache warm-up = ~5-50μs!                   │             ║
║  │                                                │             ║
║  │  → CFS: prefer giữ process trên cùng core   │             ║
║  │  → Go: goroutine "sticky" to P, P "sticky"   │             ║
║  │    to M → cache locality                      │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

---

## §3. Memory Management — Bộ Nhớ Ảo

### 3.1 Virtual Memory — Ảo Giác Vĩ Đại

Virtual Memory là một trong những phát minh quan trọng nhất của OS. Nó cho mỗi process **ảo giác** rằng nó sở hữu toàn bộ bộ nhớ, trong khi thực tế nhiều process chia sẻ cùng RAM vật lý.

```
╔═══════════════════════════════════════════════════════════════╗
║   VIRTUAL MEMORY — THE BIG PICTURE                          ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Process A             Process B          Physical RAM       ║
║  Virtual Space          Virtual Space                        ║
║  ┌──────────┐          ┌──────────┐     ┌──────────┐        ║
║  │ 0x400000 │───map───►│          │     │ Frame 0  │        ║
║  │ (code)   │          │ 0x400000 │──┐  │ Frame 1  │◄──A    ║
║  │          │          │ (code)   │  │  │ Frame 2  │◄──B    ║
║  │ 0x600000 │───map──┐ │          │  │  │ Frame 3  │◄──A    ║
║  │ (heap)   │        │ │ 0x600000 │──┼─►│ Frame 4  │        ║
║  │          │        └─┼──────────┼──┘  │ Frame 5  │◄──B    ║
║  │ 0x7fff.. │───map──┐ │ 0x7fff.. │──┐  │ Frame 6  │        ║
║  │ (stack)  │        └─┼──────────┼──┼─►│ Frame 7  │        ║
║  └──────────┘          └──────────┘  │  │ ....     │        ║
║                                       │  └──────────┘        ║
║  → A và B dùng CÙNG virtual address  │                       ║
║    nhưng map tới KHÁC physical frame! │                       ║
║  → Process A KHÔNG THỂ đọc RAM       │                       ║
║    của Process B! (isolation!)        │                       ║
║                                                               ║
║  TẠI SAO CẦN VIRTUAL MEMORY?                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  1. ISOLATION: mỗi process address space riêng│             ║
║  │     → Crash A không ảnh hưởng B              │             ║
║  │  2. ABSTRACTION: process không cần biết       │             ║
║  │     physical layout                            │             ║
║  │  3. OVERCOMMIT: cấp phát > RAM thật          │             ║
║  │     → 16GB RAM nhưng tổng process dùng 50GB? │             ║
║  │     → OK! Vì không phải tất cả access cùng lúc│             ║
║  │  4. SHARING: shared libraries (libc) map 1 lần│             ║
║  │     → 100 process dùng chung 1 copy!          │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 3.2 Page Table — Bản Dịch Địa Chỉ

OS chia memory thành các **page** cố định (4KB trên Linux). Page table chuyển virtual address → physical address.

```
╔═══════════════════════════════════════════════════════════════╗
║   PAGE TABLE — ADDRESS TRANSLATION                          ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Virtual Address (64-bit, nhưng chỉ dùng 48-bit):           ║
║  ┌────────────────────────────────────────┐                   ║
║  │ PML4 │ PDPT │  PD  │  PT  │  OFFSET  │                   ║
║  │ 9bit │ 9bit │ 9bit │ 9bit │  12bit   │                   ║
║  └──┬───┴──┬───┴──┬───┴──┬───┴────┬─────┘                   ║
║     │      │      │      │        │                           ║
║     │   4-LEVEL PAGE TABLE WALK:  │                           ║
║     │      │      │      │        │                           ║
║     ▼      ▼      ▼      ▼        │                           ║
║  ┌─────┐┌─────┐┌─────┐┌─────┐    │                           ║
║  │PML4 ││PDPT ││ PD  ││ PT  │    │                           ║
║  │table││table││table││table│    │                           ║
║  │[idx]││[idx]││[idx]││[idx]│────┼──► Physical Frame         ║
║  └──┬──┘└──┬──┘└──┬──┘└──┬──┘    │      + OFFSET            ║
║     └──►   └──►   └──►   └──►    │      = Physical Address!  ║
║                                   │                           ║
║  → Mỗi level = 1 memory access: 4 levels = 4 accesses!     ║
║  → 4 × ~100ns = 400ns per translation! ← QUÁ CHẬM!       ║
║  → Giải pháp: TLB cache (xem 3.3)                          ║
║                                                               ║
║  PAGE TABLE ENTRY (PTE):                                     ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Physical Frame Number  │ Flags:              │             ║
║  │  (chỉ tới RAM thật)    │ P  = Present        │             ║
║  │                          │ R/W = Read/Write    │             ║
║  │                          │ U/S = User/Kernel   │             ║
║  │                          │ D  = Dirty          │             ║
║  │                          │ A  = Accessed        │             ║
║  │                          │ NX = No Execute      │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ Huge Pages (2MB/1GB thay vì 4KB):                        ║
║  → Giảm số TLB entries cần → ít TLB miss hơn              ║
║  → PostgreSQL: huge_pages = try                               ║
║  → Redis: echo always > /sys/transparent_hugepage/enabled    ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 3.3 TLB — Cache Cho Địa Chỉ

**TLB (Translation Lookaside Buffer)** cache kết quả page table lookup. Không có TLB, mọi memory access chậm 4-5× vì page walk.

```
╔═══════════════════════════════════════════════════════════════╗
║   TLB — TRANSLATION LOOKASIDE BUFFER                        ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  CPU muốn access 0x7fff8004:                                 ║
║                                                               ║
║  ┌──────────┐    TLB hit?     ┌───────────┐                  ║
║  │   CPU    │───────────────►│   TLB     │                  ║
║  │          │   YES (~1ns)    │ 64-1024   │                  ║
║  │          │◄───────────────│ entries   │                  ║
║  │          │                 └───────────┘                  ║
║  │          │   NO (~400ns!)                                  ║
║  │          │───────────────► Page Table Walk                ║
║  │          │                 (4 memory accesses)             ║
║  │          │◄───────────────                                ║
║  └──────────┘                                                 ║
║                                                               ║
║  TLB PERFORMANCE:                                             ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  TLB hit rate: 99%+ cho normal workloads     │             ║
║  │  → 99% × 1ns + 1% × 400ns = ~5ns average    │             ║
║  │                                                │             ║
║  │  TLB miss rate cao khi:                       │             ║
║  │  • Random memory access (pointer chasing)     │             ║
║  │  • Working set > TLB coverage                 │             ║
║  │    (1024 entries × 4KB = 4MB coverage)       │             ║
║  │  • Context switch (TLB flush!)                │             ║
║  │                                                │             ║
║  │  Huge Pages: 1024 entries × 2MB = 2GB!       │             ║
║  │  → Database workloads TLB miss giảm 10×!    │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 3.4 Page Fault — Khi Trang Chưa Sẵn Sàng

**Page fault** xảy ra khi process truy cập page chưa có trong RAM. Không phải error — đây là cơ chế bình thường của demand paging!

```
╔═══════════════════════════════════════════════════════════════╗
║   PAGE FAULTS                                                ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  3 LOẠI PAGE FAULT:                                          ║
║                                                               ║
║  1. MINOR (Soft) Page Fault:           ~1-10μs              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Page nằm trong RAM nhưng chưa map          │             ║
║  │  → OS cập nhật page table, done!            │             ║
║  │  → malloc(): OS chưa cấp RAM thật ngay     │             ║
║  │  → First access → minor fault → map page   │             ║
║  │                                                │             ║
║  │  Go: runtime allocator pre-map → ít fault   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  2. MAJOR (Hard) Page Fault:           ~1-10ms!             ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Page KHÔNG nằm trong RAM!                    │             ║
║  │  → Đã bị swap ra disk                        │             ║
║  │  → OS phải đọc disk → load vào RAM          │             ║
║  │  → Nếu RAM full: swap page KHÁC ra disk!    │             ║
║  │                                                │             ║
║  │  ⚠️ CHẬM 1000× so với minor fault!          │             ║
║  │  → Đây là "performance cliff" khi OOM!      │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  3. INVALID Page Fault → SIGSEGV!                            ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Access ĐỊA CHỈ KHÔNG HỢP LỆ:              │             ║
║  │  → NULL pointer dereference (0x0000)          │             ║
║  │  → Write vào read-only memory                 │             ║
║  │  → Access kernel space từ userspace           │             ║
║  │                                                │             ║
║  │  → OS gửi SIGSEGV signal                     │             ║
║  │  → Go: panic: runtime error: invalid memory   │             ║
║  │    address or nil pointer dereference          │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  MONITOR:                                                     ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  $ ps -o min_flt,maj_flt -p <PID>            │             ║
║  │  MIN_FLT  MAJ_FLT                            │             ║
║  │  423891   12          ← maj_flt thấp = OK   │             ║
║  │                                                │             ║
║  │  $ perf stat -e page-faults ./myserver        │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 3.5 Demand Paging & Overcommit

```
╔═══════════════════════════════════════════════════════════════╗
║   DEMAND PAGING & OVERCOMMIT                                ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  DEMAND PAGING — "Lazy Allocation":                         ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  malloc(100MB):                                │             ║
║  │                                                │             ║
║  │  1. OS: "OK, đánh dấu 100MB virtual space"  │             ║
║  │     → KHÔNG cấp RAM thật!                    │             ║
║  │     → Chỉ update page table entries           │             ║
║  │                                                │             ║
║  │  2. First access page 0: Minor fault!         │             ║
║  │     → OS cấp 1 physical page (4KB)           │             ║
║  │                                                │             ║
║  │  3. First access page 1: Minor fault!         │             ║
║  │     → OS cấp thêm 1 page...                  │             ║
║  │                                                │             ║
║  │  → Chỉ cấp pages THỰC SỰ được dùng!        │             ║
║  │  → malloc(100MB) nhưng dùng 1MB → 1MB RAM!  │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  OVERCOMMIT MEMORY (Linux):                                  ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  /proc/sys/vm/overcommit_memory:              │             ║
║  │                                                │             ║
║  │  0 (default): Heuristic — OS quyết định      │             ║
║  │  1: Always — cho phép overcommit vô hạn!     │             ║
║  │  2: Never — chỉ commit ≤ swap + RAM×ratio   │             ║
║  │                                                │             ║
║  │  Khi overcommit + dùng hết:                   │             ║
║  │  → OOM Killer! OS kill process chiếm RAM nhất│             ║
║  │  → oom_score_adj = -1000: never kill (DB!)   │             ║
║  │  → oom_score_adj = +1000: kill first (cache) │             ║
║  │                                                │             ║
║  │  Go: GOGC=100 (default) → heap ≤ 2× live    │             ║
║  │  → GOMEMLIMIT=1GiB (Go 1.19+) hard limit!   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 3.6 Copy-on-Write (CoW)

```
╔═══════════════════════════════════════════════════════════════╗
║   COPY-ON-WRITE (CoW)                                        ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  fork() KHÔNG copy toàn bộ memory!                          ║
║                                                               ║
║  STEP 1 — Fork:                                              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Parent & Child SHARE same physical pages     │             ║
║  │  → Tất cả pages đánh dấu READ-ONLY!        │             ║
║  │                                                │             ║
║  │  Parent   Physical    Child                    │             ║
║  │  VPage 0 ──→ Frame 5 ←── VPage 0             │             ║
║  │  VPage 1 ──→ Frame 8 ←── VPage 1             │             ║
║  │  VPage 2 ──→ Frame 2 ←── VPage 2             │             ║
║  │                                                │             ║
║  │  → fork() = gần như FREE! (chỉ copy table)  │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  STEP 2 — Write (Parent writes VPage 1):                     ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  1. CPU trap: write to read-only page!        │             ║
║  │  2. OS: đây là CoW page                       │             ║
║  │  3. OS copy Frame 8 → Frame 12 (new!)       │             ║
║  │  4. Parent VPage 1 → Frame 12 (private!)    │             ║
║  │  5. Child VPage 1 vẫn → Frame 8 (original)  │             ║
║  │                                                │             ║
║  │  → Chỉ page THỰC SỰ WRITE mới bị copy!     │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ Redis fork() cho RDB snapshot:                             ║
║  → Redis fork child để dump data ra disk                    ║
║  → CoW: child share memory với parent                       ║
║  → Chỉ pages parent WRITE mới copy                         ║
║  → 10GB Redis, 5% write → chỉ cần thêm 500MB!            ║
║  → Nếu không có CoW: cần 20GB RAM! (2× copy)              ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 3.7 mmap — Memory-Mapped Files

```
╔═══════════════════════════════════════════════════════════════╗
║   mmap — MEMORY-MAPPED I/O                                   ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  TRADITIONAL I/O vs mmap:                                    ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  read() syscall:                               │             ║
║  │  Disk → Kernel buffer → User buffer (2 copy!)│             ║
║  │                                                │             ║
║  │  mmap:                                         │             ║
║  │  File mapped vào virtual address space        │             ║
║  │  → Access file = access memory!               │             ║
║  │  → Page fault → OS load page từ disk         │             ║
║  │  → ZERO copy vào userspace!                   │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  USE CASES:                                                   ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  • Shared libraries: libc.so mmap 1 lần     │             ║
║  │    → 100 processes share cùng physical pages  │             ║
║  │  • Database: MongoDB WiredTiger mmap engine   │             ║
║  │  • SQLite: mmap mode cho reads               │             ║
║  │  • Log files: mmap cho random access          │             ║
║  │                                                │             ║
║  │  Go: syscall.Mmap() hoặc                     │             ║
║  │      golang.org/x/exp/mmap                     │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ⚠️ CAVEATS:                                                  ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  • mmap NOT good cho sequential large reads   │             ║
║  │    → read() + readahead tốt hơn!            │             ║
║  │  • mmap page faults unpredictable              │             ║
║  │    → Latency spike! (1-10ms per major fault)  │             ║
║  │  • Error handling khó: SIGBUS on I/O error!  │             ║
║  │  • Andy Pavlo (CMU): "mmap for DB is bad"     │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 3.8 malloc/free — Dynamic Memory Allocation

```
╔═══════════════════════════════════════════════════════════════╗
║   malloc/free & MEMORY ALLOCATORS                           ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  malloc(size) — Cấp phát bộ nhớ trên heap:                 ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Kernel cấp pages qua brk()/mmap()           │             ║
║  │  → brk(): mở rộng heap (small allocations)   │             ║
║  │  → mmap(): anonymous mapping (large allocs)   │             ║
║  │                                                │             ║
║  │  Allocator (glibc ptmalloc, jemalloc, tcmalloc)│             ║
║  │  quản lý sub-division pages thành chunks:     │             ║
║  │                                                │             ║
║  │  ┌────┐┌────────┐┌──┐┌──────┐┌───────────┐   │             ║
║  │  │ 16 ││  128   ││32││  64  ││   free    │   │             ║
║  │  └────┘└────────┘└──┘└──────┘└───────────┘   │             ║
║  │  ↑ allocated     ↑     ↑        ↑ free        │             ║
║  │                                                │             ║
║  │  Fragmentation: có free space nhưng            │             ║
║  │  KHÔNG đủ contiguous cho request lớn!        │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  COMMON BUGS:                                                 ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  C:                   Go:                      │             ║
║  │  ─────                ────                     │             ║
║  │  Memory leak          goroutine leak          │             ║
║  │  (забыl free)        (goroutine chờ mãi)    │             ║
║  │                                                │             ║
║  │  Double free          reference kept alive    │             ║
║  │  (crash!)             (GC không collect)     │             ║
║  │                                                │             ║
║  │  Use-after-free       N/A (GC prevents this!) │             ║
║  │  (undefined behavior!)                         │             ║
║  │                                                │             ║
║  │  Buffer overflow      slice bounds check!     │             ║
║  │  (security exploit!)  (runtime panic)          │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 3.9 Garbage Collection — Tự Động Dọn Rác

```
╔═══════════════════════════════════════════════════════════════╗
║   GARBAGE COLLECTION                                         ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  GC ALGORITHMS:                                              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  1. Reference Counting (Python, Swift):       │             ║
║  │     Mỗi object đếm số references            │             ║
║  │     → count = 0 → free immediately!          │             ║
║  │     → Vấn đề: circular references!           │             ║
║  │     A → B → A (count never = 0!)            │             ║
║  │                                                │             ║
║  │  2. Mark-and-Sweep (Go, Java, JS):            │             ║
║  │     Phase 1 (Mark): trace từ root objects    │             ║
║  │     → Đánh dấu tất cả reachable objects     │             ║
║  │     Phase 2 (Sweep): free unreachable objects │             ║
║  │                                                │             ║
║  │  3. Generational (Java, .NET):                │             ║
║  │     Objects chia thành generations:            │             ║
║  │     Gen 0 (Young): collect thường xuyên      │             ║
║  │     Gen 1 (Old): collect ít hơn              │             ║
║  │     → Hypothesis: "most objects die young"    │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  GO GC — TRI-COLOR MARK & SWEEP:                            ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  WHITE: chưa visit (potential garbage)        │             ║
║  │  GRAY:  visited, chưa scan children          │             ║
║  │  BLACK: visited + scanned (definitely alive)  │             ║
║  │                                                │             ║
║  │  Step 1: Root scan (stacks, globals)          │             ║
║  │  → Root objects = GRAY                        │             ║
║  │                                                │             ║
║  │  Step 2: Trace (concurrent!)                  │             ║
║  │  → GRAY → scan children → children = GRAY   │             ║
║  │  → Original object = BLACK                    │             ║
║  │  → Lặp lại đến hết GRAY                     │             ║
║  │                                                │             ║
║  │  Step 3: Sweep                                 │             ║
║  │  → WHITE objects = garbage → free!            │             ║
║  │                                                │             ║
║  │  Key insight: CONCURRENT với application!     │             ║
║  │  → STW pause chỉ ~0.1-0.5ms (Go 1.18+)     │             ║
║  │  → Write barrier để track mutations          │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  GO GC TUNING:                                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  GOGC=100 (default): GC khi heap = 2× live   │             ║
║  │  GOGC=50: GC thường hơn, ít RAM, nhiều CPU │             ║
║  │  GOGC=200: GC ít hơn, nhiều RAM, ít CPU    │             ║
║  │  GOGC=off: TẮT GC (nguy hiểm!)              │             ║
║  │                                                │             ║
║  │  GOMEMLIMIT=512MiB (Go 1.19+):               │             ║
║  │  → Hard memory limit                          │             ║
║  │  → GC chạy aggressive hơn khi gần limit     │             ║
║  │  → Container-friendly! (match Docker limit)   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 3.10 Go Memory Allocator — TCMalloc-Inspired

```
╔═══════════════════════════════════════════════════════════════╗
║   GO MEMORY ALLOCATOR                                        ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Go allocator lấy cảm hứng từ TCMalloc (Google):           ║
║                                                               ║
║  3-LEVEL HIERARCHY:                                          ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  mcache (per-P, lock-free!):                  │             ║
║  │  ┌─────────────────────────────────────┐      │             ║
║  │  │ Size class 8:   [span] [span]       │      │             ║
║  │  │ Size class 16:  [span]              │      │             ║
║  │  │ Size class 32:  [span] [span] [span]│      │             ║
║  │  │ ...                                  │      │             ║
║  │  │ Size class 32KB: [span]             │      │             ║
║  │  └─────────────────────────────────────┘      │             ║
║  │  → Mỗi P (processor) có mcache riêng        │             ║
║  │  → NO LOCK cho small allocations!             │             ║
║  │                                                │             ║
║  │  mcentral (shared, per-size-class):           │             ║
║  │  ┌─────────────────────────────────────┐      │             ║
║  │  │ Khi mcache hết span → lấy từ đây │      │             ║
║  │  │ → Cần lock (nhưng per-size-class!) │      │             ║
║  │  └─────────────────────────────────────┘      │             ║
║  │                                                │             ║
║  │  mheap (global, OS memory):                   │             ║
║  │  ┌─────────────────────────────────────┐      │             ║
║  │  │ Khi mcentral hết → mheap xin OS   │      │             ║
║  │  │ → mmap() syscall lấy pages mới     │      │             ║
║  │  │ → Cần global lock (hiếm khi dùng) │      │             ║
║  │  └─────────────────────────────────────┘      │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  SIZE CLASSES:                                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  ≤ 32KB: small object → mcache (fast!)      │             ║
║  │  32KB-32MB: large object → mheap direct     │             ║
║  │  > 32MB: very large → direct mmap()          │             ║
║  │                                                │             ║
║  │  ~67 size classes: 8, 16, 32, 48, 64...       │             ║
║  │  → Giảm fragmentation!                        │             ║
║  │  → Request 20B → allocate 32B (12B wasted)   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ ZERO-ALLOC PATTERNS (Performance):                        ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  // ❌ Allocates: slice header escape to heap │             ║
║  │  func getIDs() []int { return []int{1,2,3} }  │             ║
║  │                                                │             ║
║  │  // ✅ Pre-allocate:                          │             ║
║  │  ids := make([]int, 0, expectedCap)            │             ║
║  │                                                │             ║
║  │  // ✅ sync.Pool: reuse objects               │             ║
║  │  var bufPool = sync.Pool{                      │             ║
║  │    New: func() any { return new(bytes.Buffer) }│             ║
║  │  }                                              │             ║
║  │  buf := bufPool.Get().(*bytes.Buffer)           │             ║
║  │  defer bufPool.Put(buf)                         │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

---

## §4. File System — Mọi Thứ Là File

### 4.1 "Everything is a File" — Triết Lý Unix

Trên Unix/Linux, gần như **mọi thứ** được trừu tượng hóa dưới dạng file: file thật, thư mục, thiết bị (disk, keyboard), network socket, pipe, thậm chí thông tin process (`/proc`). Đây là một trong những quyết định thiết kế quan trọng nhất của Unix, cho phép sử dụng cùng API (`open/read/write/close`) cho mọi loại I/O.

```
╔═══════════════════════════════════════════════════════════════╗
║   EVERYTHING IS A FILE                                       ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ┌──────────────────┬──────────────────────────┐             ║
║  │ "File" type       │ Ví dụ thực tế            │             ║
║  ├──────────────────┼──────────────────────────┤             ║
║  │ Regular file      │ /etc/nginx/nginx.conf     │             ║
║  │ Directory         │ /var/log/                  │             ║
║  │ Symlink           │ /usr/bin/python→python3   │             ║
║  │ Block device      │ /dev/sda (disk)            │             ║
║  │ Character device  │ /dev/tty (terminal)        │             ║
║  │ Named pipe (FIFO) │ /tmp/myfifo                │             ║
║  │ Unix socket       │ /var/run/docker.sock       │             ║
║  │ /proc file        │ /proc/1234/status          │             ║
║  │ /sys file         │ /sys/class/net/eth0/mtu    │             ║
║  └──────────────────┴──────────────────────────┘             ║
║                                                               ║
║  → TẤT CẢ đều dùng: open(), read(), write(), close()       ║
║  → Thống nhất interface = simple, composable, powerful!     ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 4.2 Inode — Danh Tính Thật Của File

**Inode** (index node) là cấu trúc dữ liệu trong filesystem chứa **metadata** của file. Tên file chỉ là "alias" — inode mới là danh tính thật.

```
╔═══════════════════════════════════════════════════════════════╗
║   INODE STRUCTURE                                            ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  $ ls -li /etc/passwd                                        ║
║  1234567 -rw-r--r-- 1 root root 2048 Jan 1 passwd            ║
║  ↑ inode number                                               ║
║                                                               ║
║  INODE chứa:                                                  ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  File type (regular/dir/symlink/...)          │             ║
║  │  Permissions (rwxr-xr-x)                      │             ║
║  │  Owner UID, Group GID                         │             ║
║  │  Size (bytes)                                  │             ║
║  │  Timestamps: atime, mtime, ctime              │             ║
║  │  Link count (hard links)                       │             ║
║  │  Data block pointers:                          │             ║
║  │  ├── 12 direct pointers → 48KB              │             ║
║  │  ├── 1 indirect → 4MB                        │             ║
║  │  ├── 1 double indirect → 4GB                 │             ║
║  │  └── 1 triple indirect → 4TB                 │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  INODE KHÔNG chứa: TÊN FILE!                                ║
║  → Tên nằm trong directory entry (dentry)                   ║
║  → Nhiều tên có thể chỉ tới cùng 1 inode = hard link      ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 4.3 File Descriptor — "Tay Cầm" Của Process

Khi process `open()` file, kernel trả về **file descriptor (fd)** — một số nguyên dùng cho mọi thao tác tiếp theo.

```
╔═══════════════════════════════════════════════════════════════╗
║   FILE DESCRIPTOR TABLE                                      ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Process (PID 1234):                                         ║
║  ┌─────┬───────────────────────────────────────┐             ║
║  │ fd  │ Trỏ tới (Open File Description)       │             ║
║  ├─────┼───────────────────────────────────────┤             ║
║  │  0  │ stdin  (terminal /dev/pts/0)           │             ║
║  │  1  │ stdout (terminal /dev/pts/0)           │             ║
║  │  2  │ stderr (terminal /dev/pts/0)           │             ║
║  │  3  │ /var/log/app.log (offset: 4096)        │             ║
║  │  4  │ TCP socket (port 8080)                  │             ║
║  │  5  │ PostgreSQL unix socket                  │             ║
║  └─────┴───────────────────────────────────────┘             ║
║                                                               ║
║  ⚠️ FILE DESCRIPTOR LIMIT:                                   ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  $ ulimit -n                                   │             ║
║  │  1024     ← default trên nhiều Linux!        │             ║
║  │                                                │             ║
║  │  → 1 TCP connection = 1 fd                    │             ║
║  │  → 1024 connections = HẾT fd!                │             ║
║  │  → "too many open files" error ← phổ biến!  │             ║
║  │                                                │             ║
║  │  Fix cho production:                           │             ║
║  │  $ ulimit -n 65535                             │             ║
║  │  /etc/security/limits.conf:                    │             ║
║  │  * soft nofile 65535                            │             ║
║  │  * hard nofile 65535                            │             ║
║  │                                                │             ║
║  │  → 10K connections Go server: cần ≥ 10K fds  │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 4.4 VFS — Virtual File System Layer

**VFS** là abstraction layer trong kernel, cho phép Linux hỗ trợ hàng chục filesystem types (ext4, XFS, Btrfs, NFS, procfs...) thông qua cùng một API.

```
╔═══════════════════════════════════════════════════════════════╗
║   VFS — VIRTUAL FILE SYSTEM                                 ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  User space: open("/data/myfile", O_RDONLY)                  ║
║  ──────────────────────────────────────────────              ║
║  Kernel:                                                      ║
║  ┌──────────────────────────────────────────────┐             ║
║  │              VFS Layer                         │             ║
║  │  (Chung cho TẤT CẢ filesystem types)         │             ║
║  │  → struct inode, struct dentry, struct file   │             ║
║  │  → Routing request tới đúng FS driver       │             ║
║  ├──────────┬──────────┬──────────┬─────────────┤             ║
║  │   ext4   │   XFS    │   NFS   │   procfs    │             ║
║  │  driver  │  driver  │  driver │   driver    │             ║
║  ├──────────┴──────────┴─────────┴──────────────┤             ║
║  │           Block I/O Layer                      │             ║
║  │    (I/O scheduling, request merging)           │             ║
║  ├──────────────────────────────────────────────┤             ║
║  │           Device Drivers                       │             ║
║  │    (SCSI, NVMe, virtio-blk)                    │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  → Go os.Open() → syscall.Open() → VFS → ext4 → disk     ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 4.5 ext4 & Journaling — Chống Corrupt Dữ Liệu

```
╔═══════════════════════════════════════════════════════════════╗
║   ext4 JOURNALING                                            ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  VẤN ĐỀ: Mất điện giữa lúc write file!                    ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  1. Update inode metadata        ✓ done       │             ║
║  │  2. Write data blocks            ✓ partial    │             ║
║  │  3. Update bitmap (free space)   ✗ chưa kịp! │             ║
║  │                                                │             ║
║  │  → Mất điện! → filesystem INCONSISTENT!      │             ║
║  │  → File corrupt, data mất, bitmap sai!       │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  JOURNALING — Write-Ahead Logging cho filesystem:           ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  1. Ghi JOURNAL ENTRY trước (what will change)│             ║
║  │  2. COMMIT journal entry                      │             ║
║  │  3. Apply changes to actual data/metadata     │             ║
║  │  4. Mark journal entry done                    │             ║
║  │                                                │             ║
║  │  Mất điện ở step 2? → Replay journal!        │             ║
║  │  Mất điện ở step 3? → Replay journal!        │             ║
║  │  → LUÔN consistent sau recovery!              │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  JOURNAL MODES (ext4):                                       ║
║  ┌──────────────────┬───────────────────────────┐             ║
║  │ Mode             │ Ghi gì vào journal?       │             ║
║  ├──────────────────┼───────────────────────────┤             ║
║  │ journal          │ metadata + data (chậm, safe)│             ║
║  │ ordered (default)│ metadata (data write first)│             ║
║  │ writeback        │ metadata only (nhanh, rủi ro)│             ║
║  └──────────────────┴───────────────────────────┘             ║
║                                                               ║
║  ☞ PostgreSQL + ext4:                                         ║
║  → DB có WAL riêng → ext4 ordered mode là đủ              ║
║  → full_page_writes=on (PG) bảo vệ thêm                   ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 4.6 Buffered I/O vs Direct I/O

```
╔═══════════════════════════════════════════════════════════════╗
║   BUFFERED vs DIRECT I/O                                     ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  BUFFERED I/O (default):                                     ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  App write() → Page Cache → (later) Disk    │             ║
║  │                                                │             ║
║  │  ┌─────┐    ┌────────────┐    ┌──────┐       │             ║
║  │  │ App │──►│ Page Cache │──►│ Disk │       │             ║
║  │  └─────┘    │ (RAM)      │    └──────┘       │             ║
║  │             │ kernel quản│                     │             ║
║  │             └────────────┘                     │             ║
║  │                                                │             ║
║  │  ✅ write() return NGAY (dữ liệu ở RAM)    │             ║
║  │  ✅ read() từ cache = nhanh!                 │             ║
║  │  ⚠️ Mất điện = mất data chưa flush!         │             ║
║  │  → fsync() để force flush cache → disk       │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  DIRECT I/O (O_DIRECT):                                      ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  App write() → BYPASS Page Cache → Disk     │             ║
║  │                                                │             ║
║  │  ┌─────┐              ┌──────┐                │             ║
║  │  │ App │────────────►│ Disk │                │             ║
║  │  └─────┘              └──────┘                │             ║
║  │                                                │             ║
║  │  ✅ Database tự manage cache hiệu quả hơn   │             ║
║  │  ✅ Tránh double caching (DB cache + page)   │             ║
║  │  ⚠️ Chậm hơn cho workload nhỏ              │             ║
║  │                                                │             ║
║  │  Ai dùng O_DIRECT?                            │             ║
║  │  → MySQL InnoDB (innodb_flush_method=O_DIRECT)│             ║
║  │  → PostgreSQL: effective_io_concurrency       │             ║
║  │  → RocksDB, ScyllaDB                          │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 4.7 Hard Links vs Soft Links

```
╔═══════════════════════════════════════════════════════════════╗
║   HARD LINKS vs SOFT LINKS (SYMLINKS)                       ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  HARD LINK:                                                   ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  $ ln original.txt hardlink.txt               │             ║
║  │                                                │             ║
║  │  "original.txt" ──→ inode 12345 ←── "hardlink"│             ║
║  │  (cùng inode = cùng data!)                    │             ║
║  │                                                │             ║
║  │  → Delete "original.txt": file VẪN TỒN TẠI! │             ║
║  │  → Vì inode link count = 1 (hardlink còn!)   │             ║
║  │  → Không thể hard link across filesystems    │             ║
║  │  → Không thể hard link directories           │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  SOFT LINK (Symlink):                                        ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  $ ln -s /usr/bin/go /usr/local/bin/go        │             ║
║  │                                                │             ║
║  │  "symlink" ──→ "/usr/bin/go" (path string)   │             ║
║  │  (inode KHÁC, chỉ chứa path!)               │             ║
║  │                                                │             ║
║  │  → Delete target: symlink BROKEN (dangling!) │             ║
║  │  → Có thể cross filesystem                   │             ║
║  │  → Có thể link directories                   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 4.8 Directory Structure — Cây Thư Mục Linux

```
╔═══════════════════════════════════════════════════════════════╗
║   LINUX DIRECTORY HIERARCHY (FHS)                           ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  /                           Root                             ║
║  ├── bin/                    Essential binaries               ║
║  ├── etc/                    Configuration files              ║
║  │   ├── nginx/              → Nginx config                  ║
║  │   └── postgresql/         → PG config                     ║
║  ├── var/                    Variable data                    ║
║  │   ├── log/                → Log files                     ║
║  │   ├── lib/postgresql/     → DB data files                 ║
║  │   └── run/                → Runtime data (PID, sockets)   ║
║  ├── proc/                   Process pseudo-filesystem       ║
║  │   ├── 1234/               → Process PID 1234 info        ║
║  │   │   ├── status          → Process state, memory        ║
║  │   │   ├── fd/             → Open file descriptors        ║
║  │   │   └── maps            → Memory mappings              ║
║  │   ├── cpuinfo             → CPU information               ║
║  │   └── meminfo             → Memory statistics             ║
║  ├── sys/                    Kernel/hardware tuning          ║
║  │   ├── class/net/          → Network interfaces            ║
║  │   └── block/              → Block devices                 ║
║  ├── dev/                    Device files                     ║
║  │   ├── sda                 → First disk                    ║
║  │   ├── null                → Discard output                ║
║  │   └── urandom             → Random bytes                  ║
║  └── tmp/                    Temporary files                  ║
║                                                               ║
║  ☞ Go debug production server:                                ║
║  → cat /proc/<pid>/status    (goroutine count, memory)      ║
║  → ls -la /proc/<pid>/fd/   (open file descriptors)         ║
║  → cat /proc/<pid>/maps     (memory regions)                ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

---

## §5. I/O & System Calls — Cổng Giao Tiếp

### 5.1 System Call — Cánh Cửa Vào Kernel

User-space programs **không thể** trực tiếp truy cập hardware. Muốn đọc file, gửi network packet, hay tạo process — phải nhờ kernel qua **system call** (syscall). Đây là ranh giới bảo mật giữa user mode và kernel mode.

```
╔═══════════════════════════════════════════════════════════════╗
║   SYSTEM CALL — USER MODE → KERNEL MODE                     ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Go code: os.ReadFile("/etc/hosts")                          ║
║                                                               ║
║  ┌─────────────────────────────────────────────┐             ║
║  │ USER MODE (Ring 3):                           │             ║
║  │ ┌───────────────────────────────────────────┐ │             ║
║  │ │ Go: os.ReadFile()                         │ │             ║
║  │ │   → os.Open() → syscall.Open()           │ │             ║
║  │ │   → os.Read() → syscall.Read()           │ │             ║
║  │ │   → os.Close() → syscall.Close()         │ │             ║
║  │ └──────────────────┬────────────────────────┘ │             ║
║  │                     │ SYSCALL instruction      │             ║
║  │                     │ (x86: syscall/int 0x80)  │             ║
║  ├─────────────────────┼───────────────────────┤ ║             ║
║  │ KERNEL MODE (Ring 0):                         │             ║
║  │ ┌──────────────────┴────────────────────────┐ │             ║
║  │ │ 1. Save user registers                    │ │             ║
║  │ │ 2. Lookup syscall table (RAX = syscall #) │ │             ║
║  │ │    → read = 0, write = 1, open = 2...     │ │             ║
║  │ │ 3. Execute kernel function                │ │             ║
║  │ │ 4. Return result to user space            │ │             ║
║  │ └───────────────────────────────────────────┘ │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
║  SYSCALL COST:                                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Normal function call:  ~1-2ns                │             ║
║  │  System call:           ~100-500ns            │             ║
║  │  Ratio:                 50-250× chậm hơn!   │             ║
║  │                                                │             ║
║  │  Tại sao chậm?                                │             ║
║  │  → Mode switch (user → kernel → user)        │             ║
║  │  → Save/restore registers                      │             ║
║  │  → TLB & cache pollution                       │             ║
║  │  → Security checks                             │             ║
║  │                                                │             ║
║  │  Go optimization:                              │             ║
║  │  → Batch syscalls khi có thể                 │             ║
║  │  → Netpoller: 1 epoll_wait thay vì N reads   │             ║
║  │  → Buffered I/O: giảm số write() calls       │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 5.2 Blocking vs Non-blocking I/O

```
╔═══════════════════════════════════════════════════════════════╗
║   BLOCKING vs NON-BLOCKING I/O                              ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  BLOCKING (default):                                         ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  conn.Read(buf)                                │             ║
║  │  → Thread BLOCKED cho đến khi có data!       │             ║
║  │  → Thread không làm gì = lãng phí!          │             ║
║  │                                                │             ║
║  │  Thread 1: |██read()██████████████|──data!──| │             ║
║  │             ↑ blocked, doing nothing ↑         │             ║
║  │                                                │             ║
║  │  1 thread per connection model:                │             ║
║  │  10K connections = 10K threads = 10GB RAM!    │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  NON-BLOCKING:                                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  fcntl(fd, F_SETFL, O_NONBLOCK)               │             ║
║  │  read(fd) → EAGAIN (no data yet!)            │             ║
║  │  → Return NGAY, không chờ!                   │             ║
║  │                                                │             ║
║  │  Vấn đề: phải poll liên tục (busy waiting!)  │             ║
║  │  while (true) {                                │             ║
║  │    if read(fd) != EAGAIN { process(data) }    │             ║
║  │    // CPU quay tít = lãng phí!               │             ║
║  │  }                                              │             ║
║  │                                                │             ║
║  │  → Giải pháp: I/O multiplexing! (5.3)        │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ Go hides complexity:                                       ║
║  → conn.Read() LOOKS blocking (đơn giản!)                   ║
║  → Nhưng INTERNALLY: non-blocking + epoll!                   ║
║  → Goroutine bị park, M chạy goroutine khác                │             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 5.3 select/poll/epoll — I/O Multiplexing

Thay vì 1 thread per connection, **multiplexing** cho phép 1 thread giám sát HÀNG NGÀN connections cùng lúc.

```
╔═══════════════════════════════════════════════════════════════╗
║   I/O MULTIPLEXING EVOLUTION                                 ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  1. select() — 1983 (BSD):                                   ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  fd_set readfds;                               │             ║
║  │  FD_SET(fd1, &readfds);                        │             ║
║  │  FD_SET(fd2, &readfds);                        │             ║
║  │  select(maxfd+1, &readfds, ...);               │             ║
║  │                                                │             ║
║  │  ⚠️ Giới hạn: FD_SETSIZE = 1024!            │             ║
║  │  ⚠️ O(n) scan toàn bộ fd_set mỗi lần call  │             ║
║  │  ⚠️ Copy fd_set user↔kernel mỗi lần         │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  2. poll() — 1986 (System V):                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Giống select nhưng KHÔNG giới hạn 1024      │             ║
║  │  → Nhưng vẫn O(n) scan!                     │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  3. epoll() — 2002 (Linux 2.6):                ★ QUAN TRỌNG ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  epfd = epoll_create()                         │             ║
║  │  epoll_ctl(epfd, ADD, fd, events)     ← O(1) │             ║
║  │  events = epoll_wait(epfd, ...)       ← O(k) │             ║
║  │  (k = số fd READY, không phải tổng fds!)    │             ║
║  │                                                │             ║
║  │  ✅ O(1) add/remove fd                        │             ║
║  │  ✅ O(k) return CHỈ ready fds                │             ║
║  │  ✅ Kernel quản lý state (không copy mỗi lần)│             ║
║  │  ✅ Scale tới 1M+ connections!               │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  SO SÁNH PERFORMANCE:                                        ║
║  ┌──────────┬───────────┬──────────┬──────────┐              ║
║  │ FDs      │ select    │ poll     │ epoll    │              ║
║  ├──────────┼───────────┼──────────┼──────────┤              ║
║  │ 10       │ Fast      │ Fast     │ Fast     │              ║
║  │ 1,000    │ Slow      │ Slow     │ Fast     │              ║
║  │ 10,000   │ N/A (1024)│ Very slow│ Fast     │              ║
║  │ 100,000  │ N/A       │ Unusable │ Fast!    │              ║
║  └──────────┴───────────┴──────────┴──────────┘              ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 5.4 Go Netpoller — Epoll Trong Go Runtime

```
╔═══════════════════════════════════════════════════════════════╗
║   GO NETPOLLER                                               ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Go MAGIC: code viết sync, chạy async!                      ║
║                                                               ║
║  Developer viết:                                              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  conn, _ := net.Dial("tcp", "db:5432")        │             ║
║  │  data := make([]byte, 4096)                    │             ║
║  │  n, _ := conn.Read(data)  // "blocking"       │             ║
║  │  // Nhưng KHÔNG block OS thread!              │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  BEHIND THE SCENES:                                          ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  1. conn.Read() → fd set to non-blocking     │             ║
║  │  2. read() returns EAGAIN (no data)           │             ║
║  │  3. Goroutine PARKED (not running, no thread!)│             ║
║  │  4. fd registered with epoll                   │             ║
║  │  5. Netpoller goroutine: epoll_wait()          │             ║
║  │  6. Data arrives! epoll reports fd ready       │             ║
║  │  7. Goroutine UNPARKED → back to run queue   │             ║
║  │  8. Goroutine scheduled → read() succeeds!   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  → 100K goroutines waiting on I/O:                           ║
║    100K connections, nhưng chỉ ~4 OS threads!               ║
║  → epoll_wait() handle TẤT CẢ pending I/O                  ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 5.5 io_uring — Tương Lai Của Linux I/O

```
╔═══════════════════════════════════════════════════════════════╗
║   io_uring — ASYNC I/O REVOLUTION (Linux 5.1+, 2019)       ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  VẤN ĐỀ với epoll:                                          ║
║  → Mỗi I/O vẫn cần syscall (read/write)!                  ║
║  → syscall = ~200ns overhead × millions = bottleneck!       ║
║                                                               ║
║  io_uring GIẢI QUYẾT — NO syscall per I/O!                 ║
║  ┌──────────────────────────────────────────────┐             ║
║  │                                                │             ║
║  │  Shared ring buffers between user & kernel:   │             ║
║  │                                                │             ║
║  │  Submission Queue (SQ):     Completion Q (CQ):│             ║
║  │  ┌──────────────────┐      ┌────────────────┐ │             ║
║  │  │ User puts I/O    │      │ Kernel puts    │ │             ║
║  │  │ requests here    │      │ results here   │ │             ║
║  │  │ (no syscall!)    │      │ (no syscall!)  │ │             ║
║  │  └──────────────────┘      └────────────────┘ │             ║
║  │                                                │             ║
║  │  → Batch submit: 1000 I/Os with 1 syscall!   │             ║
║  │  → Kernel polls SQ: 0 syscalls! (SQPOLL mode)│             ║
║  │  → Supports: read, write, accept, connect...  │             ║
║  │                                                │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ☞ Go + io_uring:                                             ║
║  → Chưa native support (Go 1.22 thảo luận)                 ║
║  → Library: github.com/iceber/iouring-go                     ║
║  → Tương lai: có thể thay thế netpoller                    ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 5.6 Signals — Thông Báo Bất Đồng Bộ

```
╔═══════════════════════════════════════════════════════════════╗
║   SIGNALS                                                    ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ┌─────────┬──────┬─────────────────────────────┐            ║
║  │ Signal   │ Num  │ Mô tả & Use Case           │            ║
║  ├─────────┼──────┼─────────────────────────────┤            ║
║  │ SIGTERM  │ 15   │ "Hãy tắt đi" (graceful)   │            ║
║  │ SIGKILL  │ 9    │ KILL ngay (không bắt được!)│            ║
║  │ SIGINT   │ 2    │ Ctrl+C                      │            ║
║  │ SIGHUP   │ 1    │ Reload config (nginx)       │            ║
║  │ SIGUSR1  │ 10   │ User-defined                │            ║
║  │ SIGCHLD  │ 17   │ Child process exited        │            ║
║  │ SIGSEGV  │ 11   │ Segmentation fault!         │            ║
║  │ SIGPIPE  │ 13   │ Write to broken pipe        │            ║
║  └─────────┴──────┴─────────────────────────────┘            ║
║                                                               ║
║  GO GRACEFUL SHUTDOWN:                                       ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  ctx, stop := signal.NotifyContext(            │             ║
║  │    context.Background(),                       │             ║
║  │    syscall.SIGTERM, syscall.SIGINT,            │             ║
║  │  )                                              │             ║
║  │  defer stop()                                   │             ║
║  │                                                │             ║
║  │  go func() {                                    │             ║
║  │    srv.ListenAndServe()                         │             ║
║  │  }()                                            │             ║
║  │                                                │             ║
║  │  <-ctx.Done() // chờ SIGTERM                   │             ║
║  │  srv.Shutdown(context.Background()) // graceful│             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ⚠️ SIGPIPE in Go:                                           ║
║  → Go ignores SIGPIPE by default (khác C!)                  ║
║  → Write to closed connection = error, not crash!           ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 5.7 /proc & /sys — Pseudo-Filesystems

```
╔═══════════════════════════════════════════════════════════════╗
║   /proc & /sys — KERNEL KNOBS                               ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  /proc — Process & kernel info:                              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  # Xem memory info                             │             ║
║  │  $ cat /proc/meminfo                           │             ║
║  │  MemTotal: 16384000 kB                         │             ║
║  │  MemFree:   2048000 kB                         │             ║
║  │                                                │             ║
║  │  # Xem TCP connections                         │             ║
║  │  $ cat /proc/net/tcp                           │             ║
║  │                                                │             ║
║  │  # Xem file descriptors của process           │             ║
║  │  $ ls /proc/$(pidof myserver)/fd | wc -l      │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  /sys — Kernel tuning:                                       ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  # Network tuning cho Go servers:              │             ║
║  │  echo 65535 > /proc/sys/net/core/somaxconn    │             ║
║  │  echo 1 > /proc/sys/net/ipv4/tcp_tw_reuse    │             ║
║  │                                                │             ║
║  │  # Memory tuning:                              │             ║
║  │  echo 1 > /proc/sys/vm/overcommit_memory      │             ║
║  │                                                │             ║
║  │  # File descriptors system-wide:               │             ║
║  │  echo 1000000 > /proc/sys/fs/file-max         │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 5.8 strace — Nhìn Thấy Syscalls

```
╔═══════════════════════════════════════════════════════════════╗
║   strace — DEBUGGING TOOL                                    ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  strace trace TẤT CẢ syscalls của 1 process:               ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  $ strace -f -c ./myserver                     │             ║
║  │                                                │             ║
║  │  % time   calls  syscall                       │             ║
║  │  ------  ------  --------                      │             ║
║  │  45.23    12456  futex        ← lock contention│             ║
║  │  23.11     8901  epoll_wait   ← I/O waiting   │             ║
║  │  15.67     3456  write        ← logging        │             ║
║  │   8.34     2345  read         ← network read   │             ║
║  │   4.12      890  nanosleep    ← time.Sleep     │             ║
║  │   3.53      234  openat       ← file opens     │             ║
║  │                                                │             ║
║  │  → futex 45%? Mutex contention problem!       │             ║
║  │  → Too many write? Logging overhead!          │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  COMMON USE CASES:                                            ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  # Xem file access patterns:                   │             ║
║  │  $ strace -e trace=file ./myserver              │             ║
║  │                                                │             ║
║  │  # Xem network operations:                     │             ║
║  │  $ strace -e trace=network ./myserver           │             ║
║  │                                                │             ║
║  │  # Attach to running process:                  │             ║
║  │  $ strace -p $(pidof myserver) -c               │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

---

## §6. Synchronization — Đồng Bộ Hóa

### 6.1 Race Condition — Khi Hai Goroutine Cùng Viết

**Race condition** xảy ra khi nhiều goroutine/thread cùng truy cập shared data mà không đồng bộ. Kết quả phụ thuộc vào thứ tự thực thi — **không deterministic**.

```
╔═══════════════════════════════════════════════════════════════╗
║   RACE CONDITION                                             ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  var counter = 0 // shared!                                  ║
║                                                               ║
║  // 2 goroutines cùng chạy: counter++                       ║
║  // counter++ = READ → ADD 1 → WRITE (3 bước!)            ║
║                                                               ║
║  Goroutine A:    Goroutine B:    counter:                    ║
║  READ  = 0                        0                           ║
║                  READ  = 0        0                           ║
║  ADD 1 = 1                        0                           ║
║                  ADD 1 = 1        0                           ║
║  WRITE = 1                        1                           ║
║                  WRITE = 1        1 ← EXPECTED 2!           ║
║                                                               ║
║  DETECT IN GO:                                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  $ go run -race main.go                       │             ║
║  │  WARNING: DATA RACE                            │             ║
║  │  Write by goroutine 7:                         │             ║
║  │    main.main.func1()                           │             ║
║  │        main.go:12 +0x38                        │             ║
║  │  Previous write by goroutine 6:                │             ║
║  │    main.main.func1()                           │             ║
║  │        main.go:12 +0x38                        │             ║
║  │                                                │             ║
║  │  → CI pipeline: go test -race ./...           │             ║
║  │  → LUÔN chạy race detector khi test!         │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 6.2 Mutex — Khóa Độc Quyền

```
╔═══════════════════════════════════════════════════════════════╗
║   MUTEX (Mutual Exclusion)                                   ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  var (                                                        ║
║    mu      sync.Mutex                                        ║
║    counter int                                                ║
║  )                                                            ║
║                                                               ║
║  func increment() {                                           ║
║    mu.Lock()           // Chỉ 1 goroutine vào!              ║
║    counter++           // Critical section                    ║
║    mu.Unlock()         // Cho goroutine khác vào             ║
║  }                                                            ║
║                                                               ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  G1: |──Lock──|██critical██|──Unlock──|       │             ║
║  │  G2:      |──wait──────────|──Lock──|██|──U── │             ║
║  │  G3:      |──wait──────────────────|──Lock──  │             ║
║  │           ↑ G2,G3 BLOCKED chờ G1 xong!       │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  GO MUTEX INTERNALS:                                         ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Normal mode: FIFO-ish (new goroutines        │             ║
║  │  can steal lock = better throughput)            │             ║
║  │                                                │             ║
║  │  Starvation mode: STRICT FIFO khi goroutine   │             ║
║  │  chờ > 1ms (Go 1.9+)                         │             ║
║  │  → Tránh goroutine starvation!               │             ║
║  │                                                │             ║
║  │  Spin: goroutine spin vài lần trước khi park  │             ║
║  │  → Nếu lock release nhanh → không cần sleep │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  ⚠️ GOLDEN RULES:                                            ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  1. LUÔN dùng defer mu.Unlock()               │             ║
║  │     → Tránh quên unlock khi panic/return!    │             ║
║  │  2. KHÔNG copy Mutex (embed trong struct)     │             ║
║  │  3. KHÔNG lock trong performance hot path     │             ║
║  │     → Xem atomic, sync.Map, channel thay thế │             ║
║  │  4. Giữ critical section NGẮN nhất có thể   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 6.3 RWMutex — Đọc Nhiều, Viết Ít

```
╔═══════════════════════════════════════════════════════════════╗
║   RWMutex — READ-WRITE LOCK                                 ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  var (                                                        ║
║    rwmu  sync.RWMutex                                        ║
║    cache map[string]string                                    ║
║  )                                                            ║
║                                                               ║
║  func get(key string) string {                                ║
║    rwmu.RLock()           // Nhiều readers CÙNG LÚC!        ║
║    defer rwmu.RUnlock()                                      ║
║    return cache[key]                                          ║
║  }                                                            ║
║                                                               ║
║  func set(key, val string) {                                  ║
║    rwmu.Lock()            // Writer: EXCLUSIVE!               ║
║    defer rwmu.Unlock()                                        ║
║    cache[key] = val                                           ║
║  }                                                            ║
║                                                               ║
║  RULES:                                                       ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  • N readers đồng thời: ✅ OK                │             ║
║  │  • 1 writer + 0 readers: ✅ OK                │             ║
║  │  • 1 writer + N readers: ❌ BLOCKED!          │             ║
║  │                                                │             ║
║  │  Tốt cho: read-heavy workloads               │             ║
║  │  (config cache, feature flags, routing table)  │             ║
║  │                                                │             ║
║  │  Xấu cho: write-heavy workloads              │             ║
║  │  → Writer phải chờ TẤT CẢ readers xong     │             ║
║  │  → Overhead lớn hơn Mutex!                   │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 6.4 Deadlock — Bế Tắc

```
╔═══════════════════════════════════════════════════════════════╗
║   DEADLOCK — 4 COFFIN CONDITIONS                            ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Deadlock = 2+ goroutines chờ nhau VĨNH VIỄN!              ║
║                                                               ║
║  func transfer(from, to *Account, amount int) {               ║
║    from.mu.Lock()     // Lock A                               ║
║    to.mu.Lock()       // Lock B                               ║
║    from.balance -= amount                                     ║
║    to.balance += amount                                       ║
║    to.mu.Unlock()                                             ║
║    from.mu.Unlock()                                           ║
║  }                                                            ║
║                                                               ║
║  // G1: transfer(A→B) locks A, waits B                      ║
║  // G2: transfer(B→A) locks B, waits A                      ║
║  // → DEADLOCK! Cả 2 chờ nhau mãi mãi!                    ║
║                                                               ║
║  4 ĐIỀU KIỆN CẦN (Coffin Conditions):                      ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  1. Mutual Exclusion (lock exclusive)         │             ║
║  │  2. Hold and Wait (giữ lock + chờ lock khác) │             ║
║  │  3. No Preemption (không ai cướp lock)       │             ║
║  │  4. Circular Wait (A→B→A)                    │             ║
║  │                                                │             ║
║  │  Phá BẤT KỲ 1 điều kiện → không deadlock!  │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  FIX — Lock Ordering:                                        ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  func transfer(a, b *Account, amount int) {    │             ║
║  │    // LUÔN lock theo thứ tự ID!               │             ║
║  │    first, second := a, b                       │             ║
║  │    if a.id > b.id {                             │             ║
║  │      first, second = b, a                      │             ║
║  │    }                                            │             ║
║  │    first.mu.Lock()                              │             ║
║  │    second.mu.Lock()                             │             ║
║  │    // ... transfer ...                         │             ║
║  │    second.mu.Unlock()                           │             ║
║  │    first.mu.Unlock()                            │             ║
║  │  }                                              │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  GO RUNTIME DEADLOCK DETECTION:                              ║
║  → "fatal error: all goroutines are asleep - deadlock!"     ║
║  → Chỉ detect khi TẤT CẢ goroutines blocked!             ║
║  → Partial deadlock (vài goroutines) = KHÔNG detect!       ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 6.5 Channels — CSP Model

```
╔═══════════════════════════════════════════════════════════════╗
║   GO CHANNELS — "Share Memory By Communicating"             ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  "Don't communicate by sharing memory;                       ║
║   share memory by communicating." — Go Proverbs              ║
║                                                               ║
║  UNBUFFERED CHANNEL (synchronous):                           ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  ch := make(chan int)                          │             ║
║  │                                                │             ║
║  │  G1: ch <- 42   // BLOCK cho đến G2 nhận!   │             ║
║  │  G2: v := <-ch  // BLOCK cho đến G1 gửi!    │             ║
║  │                                                │             ║
║  │  → Synchronization point!                     │             ║
║  │  → Cả 2 phải "gặp nhau" (rendezvous)         │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  BUFFERED CHANNEL (async up to capacity):                    ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  ch := make(chan int, 100) // buffer 100      │             ║
║  │                                                │             ║
║  │  ch <- 42  // Không block nếu buffer chưa đầy│             ║
║  │  ch <- 43  // ...                              │             ║
║  │  // Block khi buffer FULL!                    │             ║
║  │                                                │             ║
║  │  → Back-pressure tự nhiên!                   │             ║
║  │  → Producer nhanh hơn consumer → block!      │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  PATTERNS:                                                    ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Fan-out: 1 producer → N consumers (workers) │             ║
║  │  Fan-in:  N producers → 1 consumer (merge)   │             ║
║  │  Pipeline: stage1 → stage2 → stage3          │             ║
║  │  Semaphore: sem := make(chan struct{}, N)      │             ║
║  │  Done/Cancel: ctx.Done() channel              │             ║
║  │  Timeout: select + time.After()               │             ║
║  │  Rate limit: time.Ticker channel              │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 6.6 Atomic Operations — Lock-Free

```
╔═══════════════════════════════════════════════════════════════╗
║   ATOMIC OPERATIONS                                          ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  atomic: CPU instruction, KHÔNG cần lock!                   ║
║  → Nhanh hơn Mutex 5-10×!                                  ║
║                                                               ║
║  var counter atomic.Int64                                     ║
║  counter.Add(1)      // atomic increment                     ║
║  counter.Load()      // atomic read                           ║
║  counter.Store(42)   // atomic write                          ║
║  counter.CompareAndSwap(old, new) // CAS                     ║
║                                                               ║
║  PERFORMANCE:                                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  mutex lock+unlock:  ~25-50ns                  │             ║
║  │  atomic add:         ~5-10ns                   │             ║
║  │  (contended mutex:   ~500ns-5μs!)             │             ║
║  │                                                │             ║
║  │  Use atomic for:                               │             ║
║  │  → Simple counters (metrics, stats)            │             ║
║  │  → Flags (isRunning, isClosed)                 │             ║
║  │  → Single-value config updates                 │             ║
║  │                                                │             ║
║  │  Use mutex for:                                │             ║
║  │  → Complex data structures                     │             ║
║  │  → Multiple related fields                     │             ║
║  │  → Anything needing consistent reads           │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 6.7 sync.Pool — Object Reuse

```
╔═══════════════════════════════════════════════════════════════╗
║   sync.Pool — REDUCE GC PRESSURE                            ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  VẤN ĐỀ:                                                     ║
║  HTTP handler: mỗi request tạo buffer mới                  ║
║  → 10K req/s × buffer → 10K allocations/s → GC pressure!  ║
║                                                               ║
║  GIẢI PHÁP — sync.Pool:                                     ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  var bufPool = sync.Pool{                      │             ║
║  │    New: func() any {                            │             ║
║  │      return bytes.NewBuffer(make([]byte,0,4096))│             ║
║  │    },                                           │             ║
║  │  }                                              │             ║
║  │                                                │             ║
║  │  func handler(w http.ResponseWriter, r *req) { │             ║
║  │    buf := bufPool.Get().(*bytes.Buffer)         │             ║
║  │    buf.Reset()                                  │             ║
║  │    defer bufPool.Put(buf) // trả lại pool!    │             ║
║  │                                                │             ║
║  │    // dùng buf...                              │             ║
║  │  }                                              │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  HOW IT WORKS:                                                ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Per-P private + per-P shared + victim cache  │             ║
║  │  → Get(): private first → shared → New()    │             ║
║  │  → Put(): vào private cache                   │             ║
║  │  → GC sweep: clear pool (objects not permanent)│             ║
║  │                                                │             ║
║  │  ☞ stdlib uses: fmt, encoding/json, net/http  │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 6.8 Semaphore vs WaitGroup vs Once

```
╔═══════════════════════════════════════════════════════════════╗
║   SYNCHRONIZATION PRIMITIVES COMPARISON                     ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  sync.WaitGroup — Chờ N goroutines hoàn thành:              ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  var wg sync.WaitGroup                         │             ║
║  │  for i := 0; i < 10; i++ {                     │             ║
║  │    wg.Add(1)                                    │             ║
║  │    go func() {                                  │             ║
║  │      defer wg.Done()                            │             ║
║  │      doWork()                                   │             ║
║  │    }()                                          │             ║
║  │  }                                              │             ║
║  │  wg.Wait() // block cho đến 10 goroutines done│             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  sync.Once — Chạy ĐÚNG 1 LẦN (singleton, init):            ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  var (                                          │             ║
║  │    once sync.Once                               │             ║
║  │    db   *sql.DB                                 │             ║
║  │  )                                              │             ║
║  │  func getDB() *sql.DB {                         │             ║
║  │    once.Do(func() {                              │             ║
║  │      db, _ = sql.Open("postgres", dsn)          │             ║
║  │    })                                            │             ║
║  │    return db                                     │             ║
║  │  }                                              │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  Semaphore (golang.org/x/sync/semaphore):                    ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  sem := semaphore.NewWeighted(10) // max 10   │             ║
║  │  sem.Acquire(ctx, 1)  // giống Lock + counter │             ║
║  │  defer sem.Release(1)                          │             ║
║  │                                                │             ║
║  │  → Giới hạn concurrent DB connections         │             ║
║  │  → Rate limiting goroutines                    │             ║
║  │  → Connection pool                             │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  KHI NÀO DÙNG GÌ?                                           ║
║  ┌───────────────┬──────────────────────────────┐             ║
║  │ Primitive      │ Use Case                      │             ║
║  ├───────────────┼──────────────────────────────┤             ║
║  │ Mutex          │ Protect shared state          │             ║
║  │ RWMutex        │ Read-heavy shared state       │             ║
║  │ Channel        │ Communication + signaling     │             ║
║  │ WaitGroup      │ Wait for goroutines to finish │             ║
║  │ Once           │ One-time initialization       │             ║
║  │ Atomic         │ Simple counters/flags          │             ║
║  │ Semaphore      │ Limit concurrency              │             ║
║  │ sync.Pool      │ Reuse expensive objects        │             ║
║  │ sync.Map       │ Concurrent map (specific cases)│             ║
║  └───────────────┴──────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

---

## §7. Deep Analysis Patterns — 6 Tư Duy Phân Tích Áp Dụng Cho OS

### 7.1 Pattern 1: Đệ Quy "Tại Sao" (5 Whys)

```
╔═══════════════════════════════════════════════════════════════╗
║   5 WHYS — TẠI SAO GO DÙNG GOROUTINES THAY VÌ THREADS?    ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  WHY 1: Tại sao Go dùng goroutine?                          ║
║  → Vì thread quá nặng cho concurrent server.               ║
║                                                               ║
║  WHY 2: Tại sao thread nặng?                                ║
║  → Mỗi thread cần ~1-8MB stack, context switch ~1-5μs.     ║
║                                                               ║
║  WHY 3: Tại sao stack thread phải lớn?                      ║
║  → Vì OS KHÔNG BIẾT app cần bao nhiêu stack.               ║
║  → Phải allocate trước vì không thể grow giữa chừng!      ║
║                                                               ║
║  WHY 4: Tại sao OS không grow stack?                         ║
║  → Vì thread stack phải contiguous (performance, simplicity).║
║  → Grow = realloc + copy = quá chậm cho kernel!            ║
║                                                               ║
║  WHY 5: Goroutine giải quyết thế nào?                       ║
║  → Stack bắt đầu 2KB, grow/shrink dynamically!             ║
║  → Go runtime control → biết khi nào cần grow.            ║
║  → Copy stack khi grow (update pointers)                     ║
║  → GIỚI HẠN VẬT LÝ: memory bandwidth cho copy!           ║
║                                                               ║
║  → INSIGHT: Go trade CPU (copy stack) lấy Memory (nhỏ)    ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 7.2 Pattern 2: First Principles Thinking

```
╔═══════════════════════════════════════════════════════════════╗
║   FIRST PRINCIPLES — XÂY DỰNG OS TỪ SỐ 0                  ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Nếu bạn phải thiết kế OS từ đầu:                          ║
║                                                               ║
║  NGUYÊN LÝ GỐC:                                             ║
║  1. CPU chỉ chạy instructions tuần tự                      ║
║  2. RAM có giới hạn và volatile                              ║
║  3. I/O devices chậm hơn CPU 10^6 lần                      ║
║  4. Nhiều programs cần chạy "đồng thời"                    ║
║                                                               ║
║  TỪ NGUYÊN LÝ → XÂY DỰNG:                                 ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  1 CPU + N programs → cần SCHEDULING         │             ║
║  │  RAM giới hạn → cần VIRTUAL MEMORY           │             ║
║  │  I/O chậm → cần ASYNC / MULTIPLEXING        │             ║
║  │  Isolation → cần PROCESS / PROTECTION        │             ║
║  │  Persistence → cần FILE SYSTEM               │             ║
║  │  Shared data → cần SYNCHRONIZATION           │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  → Mọi OS concept đều là HỆ QUẢ TẤT YẾU                 ║
║    của hardware constraints!                                  ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 7.3 Pattern 3: Trade-off Analysis

```
╔═══════════════════════════════════════════════════════════════╗
║   TRADE-OFFS TRONG OS                                        ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  1. THREAD vs GOROUTINE vs PROCESS:                          ║
║  ┌──────────┬────────────┬──────────┬───────────┐            ║
║  │          │ Process     │ Thread   │ Goroutine │            ║
║  ├──────────┼────────────┼──────────┼───────────┤            ║
║  │ Isolation│ ★★★Full   │ ★ Shared │ ★ Shared  │            ║
║  │ Memory   │ ★ Heavy    │ ★★ 1-8MB│ ★★★ 2KB  │            ║
║  │ Create   │ ~ms        │ ~100μs   │ ~1μs      │            ║
║  │ Switch   │ ~5-10μs    │ ~1-5μs   │ ~200ns    │            ║
║  │ Scale    │ ~1K        │ ~10K     │ ~1M       │            ║
║  │ Debug    │ ★★★Easy   │ ★★ Hard │ ★ Harder  │            ║
║  └──────────┴────────────┴──────────┴───────────┘            ║
║                                                               ║
║  2. MUTEX vs CHANNEL vs ATOMIC:                              ║
║  ┌──────────┬──────────┬──────────┬──────────┐               ║
║  │          │ Mutex     │ Channel  │ Atomic   │               ║
║  ├──────────┼──────────┼──────────┼──────────┤               ║
║  │ Speed    │ ~50ns     │ ~200ns   │ ~5ns     │               ║
║  │ Flexibility│ ★★★    │ ★★★    │ ★ Limited│               ║
║  │ Deadlock │ Possible  │ Possible │ ★★★ No  │               ║
║  │ Pattern  │ Shared    │ Message  │ Counter  │               ║
║  │          │ state     │ passing  │ / flag   │               ║
║  └──────────┴──────────┴──────────┴──────────┘               ║
║                                                               ║
║  3. BUFFERED vs DIRECT I/O:                                  ║
║  ┌──────────┬──────────────┬──────────────┐                   ║
║  │          │ Buffered      │ Direct       │                   ║
║  ├──────────┼──────────────┼──────────────┤                   ║
║  │ Latency  │ Low (cache)   │ Higher       │                   ║
║  │ Throughput│ Good         │ Good (big IO)│                   ║
║  │ Control  │ OS decides    │ App decides  │                   ║
║  │ Use case │ General       │ Database     │                   ║
║  └──────────┴──────────────┴──────────────┘                   ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 7.4 Pattern 4: Mental Mapping

```
╔═══════════════════════════════════════════════════════════════╗
║   MENTAL MAP — GO HTTP REQUEST LIFECYCLE (OS View)          ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Client sends HTTP request...                                ║
║                                                               ║
║  ① NETWORK I/O (§5):                                        ║
║  → NIC receives packet → interrupt → kernel                ║
║  → TCP stack processes → data in socket buffer              ║
║  → epoll_wait() notifies Go netpoller                        ║
║  → Goroutine unparked → conn.Read() returns                 ║
║                                                               ║
║  ② SCHEDULING (§2):                                          ║
║  → Goroutine placed in P's runqueue                          ║
║  → M picks goroutine → runs handler                         ║
║  → If blocked (DB query): goroutine parks                   ║
║  → M picks another goroutine (no thread wasted!)             ║
║                                                               ║
║  ③ MEMORY (§3):                                              ║
║  → Request buffer from sync.Pool (avoid GC)                  ║
║  → Parse JSON: heap allocation (escape analysis)             ║
║  → Stack growth if deep call chain                           ║
║  → GC runs concurrently (tri-color mark)                     ║
║                                                               ║
║  ④ DB QUERY (§5 + §6):                                      ║
║  → Connection from pool (semaphore pattern)                  ║
║  → Write query → socket → kernel → network                 ║
║  → Goroutine parks (epoll waits for response)                ║
║  → Response arrives → goroutine resumes                     ║
║                                                               ║
║  ⑤ RESPONSE (§4 + §5):                                      ║
║  → Marshal JSON → buffered write                            ║
║  → If log: file write → page cache → async disk            ║
║  → TCP response → kernel socket → NIC → client             ║
║  → Goroutine: defer cleanup → back to pool                  ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 7.5 Pattern 5: Reverse Engineering & Implementation

```
╔═══════════════════════════════════════════════════════════════╗
║   REVERSE ENGINEERING — DOCKER = OS FEATURES!               ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Docker KHÔNG phải VM! Docker = Linux kernel features:      ║
║                                                               ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  Docker Feature    →  OS Feature              │             ║
║  │  ───────────────────────────────────────────  │             ║
║  │  Container          = namespaces + cgroups     │             ║
║  │  Image layers       = OverlayFS (union mount)  │             ║
║  │  Resource limits    = cgroups (CPU, memory)    │             ║
║  │  Network isolation  = network namespaces       │             ║
║  │  Process isolation  = PID namespace             │             ║
║  │  Filesystem isolation = mount namespace        │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  NAMESPACES (isolation):                                      ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  PID:     Container thấy PID 1 = app process │             ║
║  │  NET:     Container có network stack riêng    │             ║
║  │  MNT:     Container thấy filesystem riêng    │             ║
║  │  UTS:     Container có hostname riêng         │             ║
║  │  IPC:     Container isolated shared memory    │             ║
║  │  USER:    Container map UID (root trong = user)│             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
║  CGROUPS (resource limits):                                   ║
║  ┌──────────────────────────────────────────────┐             ║
║  │  docker run --memory=512m --cpus=2 myapp      │             ║
║  │  → cgroup limit: 512MB RAM, 2 CPU cores       │             ║
║  │                                                │             ║
║  │  Go + cgroups:                                  │             ║
║  │  → GOMAXPROCS tự detect cgroup CPU limit     │             ║
║  │    (Go 1.19+, automaxprocs trước đó)          │             ║
║  │  → GOMEMLIMIT match cgroup memory limit       │             ║
║  └──────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 7.6 Pattern 6: Historical Evolution

```
╔═══════════════════════════════════════════════════════════════╗
║   LỊCH SỬ TIẾN HÓA OS                                      ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  1960s: Batch processing (1 job at a time)                   ║
║  → VẤN ĐỀ: CPU idle khi chờ I/O!                          ║
║                                                               ║
║  1970s: Unix (AT&T Bell Labs, Thompson & Ritchie)            ║
║  → Process model, file abstraction, shell                    ║
║  → "Everything is a file" philosophy born                    ║
║                                                               ║
║  1980s: Networking + Multi-user                               ║
║  → BSD TCP/IP stack (foundation of internet!)                ║
║  → select() system call (I/O multiplexing)                   ║
║  → Thread concept (Mach microkernel)                          ║
║                                                               ║
║  1991: Linux (Linus Torvalds)                                ║
║  → Monolithic kernel, free & open source                     ║
║  → Explosive growth through community                         ║
║                                                               ║
║  2000s: Scalability                                           ║
║  → epoll (2002): handle 1M+ connections                      ║
║  → NPTL threads: lightweight user-space threads              ║
║  → Cgroups + namespaces (2007-2008): containerization        ║
║                                                               ║
║  2010s: Containers & Cloud Native                             ║
║  → Docker (2013): package + distribute apps                  ║
║  → Kubernetes (2014): orchestrate containers                 ║
║  → Go runtime evolves: GC < 1ms (Go 1.8, 2017)              ║
║                                                               ║
║  2019+: io_uring revolution                                   ║
║  → Zero-copy, async I/O without syscall overhead             ║
║  → Future: Go may adopt io_uring for netpoller               ║
║                                                               ║
║  INSIGHT: Mỗi OS feature ra đời để giải quyết             ║
║  bottleneck CỤ THỂ của thời đại trước!                    ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

---

## §8. Tổng Kết & Câu Hỏi Phỏng Vấn Senior

### 8.1 Bản Đồ Tổng Quan

```
╔═══════════════════════════════════════════════════════════════╗
║   OS FUNDAMENTALS — COMPLETE MAP                             ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  ┌─────────────────────────────────────────────┐             ║
║  │           APPLICATION (Go server)             │             ║
║  │  goroutine ─ channel ─ mutex ─ atomic        │             ║
║  └──────────────────┬──────────────────────────┘             ║
║                      │ syscall                                ║
║  ┌──────────────────┴──────────────────────────┐             ║
║  │              OS KERNEL                         │             ║
║  │  ┌──────┐ ┌────────┐ ┌──────┐ ┌──────────┐  │             ║
║  │  │Sched │ │ VMM    │ │ VFS  │ │ Net stack│  │             ║
║  │  │      │ │        │ │      │ │          │  │             ║
║  │  │ CFS  │ │ Page   │ │ ext4 │ │ TCP/IP   │  │             ║
║  │  │ MQMS │ │ Table  │ │ XFS  │ │ epoll    │  │             ║
║  │  │      │ │ TLB    │ │      │ │ io_uring │  │             ║
║  │  └──────┘ └────────┘ └──────┘ └──────────┘  │             ║
║  └──────────────────┬──────────────────────────┘             ║
║                      │ driver                                 ║
║  ┌──────────────────┴──────────────────────────┐             ║
║  │              HARDWARE                          │             ║
║  │  CPU ─ RAM ─ SSD/HDD ─ NIC ─ BUS            │             ║
║  └─────────────────────────────────────────────┘             ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 8.2 Câu Hỏi Phỏng Vấn Senior Backend

```
╔═══════════════════════════════════════════════════════════════╗
║   INTERVIEW QUESTIONS — OS FUNDAMENTALS                     ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  Process & Thread:                                            ║
║  Q1: Process vs Thread vs Goroutine — khác gì?              ║
║  Q2: Context switch là gì? Chi phí bao nhiêu?              ║
║  Q3: Goroutine có phải green thread không? Khác gì?        ║
║  Q4: fork() + exec() hoạt động thế nào?                    ║
║  Q5: Zombie process là gì? Cách phòng tránh?               ║
║                                                               ║
║  CPU Scheduling:                                              ║
║  Q6: CFS hoạt động thế nào? Tại sao dùng red-black tree?  ║
║  Q7: Go scheduler GMP model là gì?                          ║
║  Q8: Priority Inversion là gì? Mars Pathfinder example?     ║
║  Q9: GOMAXPROCS nên set bao nhiêu trong Docker container?   ║
║                                                               ║
║  Memory Management:                                           ║
║  Q10: Virtual Memory hoạt động thế nào?                     ║
║  Q11: Page fault là gì? Minor vs Major?                     ║
║  Q12: TLB là gì? Tại sao context switch flush TLB?         ║
║  Q13: Copy-on-Write dùng ở đâu? Redis RDB snapshot?       ║
║  Q14: Go GC tri-color algorithm giải thích chi tiết?       ║
║  Q15: GOGC vs GOMEMLIMIT — khi nào dùng cái nào?          ║
║  Q16: Go memory allocator: mcache → mcentral → mheap?     ║
║                                                               ║
║  File System:                                                ║
║  Q17: "Everything is a file" nghĩa là gì?                  ║
║  Q18: Inode là gì? Hard link vs Symlink?                    ║
║  Q19: File descriptor limit — ảnh hưởng server thế nào?   ║
║  Q20: Journaling (WAL) trong ext4 — tại sao cần?          ║
║  Q21: Buffered vs Direct I/O — database dùng cái nào?      ║
║                                                               ║
║  I/O & System Calls:                                         ║
║  Q22: System call hoạt động thế nào? Chi phí?              ║
║  Q23: select vs poll vs epoll — khác gì?                    ║
║  Q24: Go netpoller hoạt động thế nào?                       ║
║  Q25: io_uring là gì? Tại sao nhanh hơn epoll?            ║
║  Q26: Graceful shutdown trong Go — implement thế nào?       ║
║                                                               ║
║  Synchronization:                                             ║
║  Q27: Race condition là gì? Phát hiện bằng gì?             ║
║  Q28: Deadlock — 4 điều kiện cần? Cách phòng tránh?       ║
║  Q29: Mutex vs RWMutex vs Channel — khi nào dùng gì?      ║
║  Q30: Atomic operations — khi nào dùng thay Mutex?         ║
║  Q31: sync.Pool — giải quyết vấn đề gì?                   ║
║                                                               ║
║  System Design (OS-level):                                    ║
║  Q32: Docker container dùng OS features nào?                ║
║  Q33: "Too many open files" — debug + fix thế nào?         ║
║  Q34: OOM Killer — process nào bị kill? Config thế nào?    ║
║  Q35: strace cho output gì? Dùng debug vấn đề nào?        ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

### 8.3 Key Takeaways

```
╔═══════════════════════════════════════════════════════════════╗
║   KEY TAKEAWAYS                                               ║
╠═══════════════════════════════════════════════════════════════╣
║                                                               ║
║  1. OS KHÔNG chỉ là lý thuyết — nó ẢNH HƯỞNG            ║
║     trực tiếp đến performance backend của bạn!             ║
║                                                               ║
║  2. Go runtime LÀ một mini-OS:                               ║
║     → Scheduler (GMP) ≈ OS scheduler (CFS)                  ║
║     → Memory allocator ≈ TCMalloc                            ║
║     → Netpoller ≈ epoll                                      ║
║     → GC ≈ memory management                                 ║
║                                                               ║
║  3. Performance bottleneck thường ở OS layer:              ║
║     → fd limit, TLB miss, page fault, lock contention       ║
║     → Biết OS = debug nhanh hơn 10×!                       ║
║                                                               ║
║  4. Docker = Linux kernel features:                           ║
║     → namespaces + cgroups + overlayfs                       ║
║     → KHÔNG PHẢI virtualization!                             ║
║                                                               ║
║  5. Senior mindset:                                           ║
║     → Đừng chỉ biết API, hãy hiểu OS ĐẰNG SAU           ║
║     → conn.Read() = goroutine park + epoll + syscall        ║
║     → mutex.Lock() = futex + spin + park                     ║
║     → malloc() = demand paging + page fault + TLB           ║
║                                                               ║
╚═══════════════════════════════════════════════════════════════╝
```

---

> **📚 Tài liệu tham khảo:**
>
> - _Operating Systems: Three Easy Pieces_ — Remzi & Andrea Arpaci-Dusseau
> - _The Linux Programming Interface_ — Michael Kerrisk
> - _Understanding the Linux Kernel_ — Daniel P. Bovet
> - _Go runtime source code_ — github.com/golang/go/src/runtime
> - _Linux kernel source_ — kernel.org
> - _BPF Performance Tools_ — Brendan Gregg
